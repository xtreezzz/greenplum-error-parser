[
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "gpcontrib/orafce/sqlscan.l:875"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "gpcontrib/orafce/sqlscan.l:933"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "gpcontrib/gpcloud/include/s3log.h:15"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "gpcontrib/gp_sparse_vector/gp_sfv.c:178"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "contrib/cube/cubescan.l:94"
    },
    {
        "log": "ereport(ERROR, ...)",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "contrib/pgcrypto/px.c:93"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "contrib/seg/segscan.l:93"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/pl/plpgsql/src/pl_scanner.c:509"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/pl/plpgsql/src/pl_exec.c:271"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/pl/plpgsql/src/pl_exec.c:531"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/pl/plpgsql/src/pl_exec.c:810"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/pl/plpgsql/src/pl_exec.c:2875"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/pl/plpgsql/src/pl_comp.c:307"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/pl/plpgsql/src/pl_comp.c:869"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/pl/plpython/plpy_main.c:363"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/pl/plpython/plpy_main.c:448"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/pl/plperl/plperl.c:3910"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/pl/plperl/plperl.c:3913"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/pl/plperl/plperl.c:3914"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/pl/tcl/pltcl.c:1577"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/pl/tcl/pltcl.c:1625"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/pl/tcl/pltcl.c:1627"
    },
    {
        "log": "ereport (error message)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/test/regress/get_ereport.pl:78"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/test/isolation2/sql/segwalrep/die_commit_pending_replication.sql:54"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/test/isolation2/sql/segwalrep/die_commit_pending_replication.sql:55"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/bin/pg_dump/hashfn.c:145"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/miscadmin.h:74"
    },
    {
        "log": "ereport(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/miscadmin.h:74"
    },
    {
        "log": "ereport(PANIC)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/miscadmin.h:74"
    },
    {
        "log": "ereport(ERROR, ...)",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/include/cdb/cdbdisp.h:103"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/cdb/ml_ipc.h:59"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/cdb/ml_ipc.h:69"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/cdb/ml_ipc.h:129"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/relcache.h:71"
    },
    {
        "log": "elog(__VA_ARGS__)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/include/utils/resgroup.h:239"
    },
    {
        "log": "ereport(ERROR, (errcode(ERRCODE_INTERNAL_ERROR), ...))",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": "ERRCODE_INTERNAL_ERROR",
        "errcode_numeric": "XX000",
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/include/utils/elog.h:113"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/include/utils/elog.h:121"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:121"
    },
    {
        "log": "ereport(elevel, rest)",
        "severity_level": "elevel",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/include/utils/elog.h:190"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:277"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:278"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:319"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/include/utils/elog.h:320"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:357"
    },
    {
        "log": "elog(__VA_ARGS__)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/include/utils/elog.h:374"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:390"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:394"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:406"
    },
    {
        "log": "ereport(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:412"
    },
    {
        "log": "ereport(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:415"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:460"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:474"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:475"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/utils/elog.h:476"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/storage/ipc.h:25"
    },
    {
        "log": "ereport(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/storage/ipc.h:26"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/storage/ipc.h:31"
    },
    {
        "log": "ereport(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/include/storage/ipc.h:31"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/include/storage/smgr.h:91"
    },
    {
        "log": "elog(WARNING)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/include/storage/smgr.h:92"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/include/gpopt/utils/COptClient.h:88"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/include/executor/execdebug.h:6"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/cdb/cdbcopy.c:40"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/cdb/cdbcopy.c:404"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/cdb/cdbcopy.c:716"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/cdb/cdbfts.c:154"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/cdb/test/cdbbufferedread_test.c:68"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/cdb/dispatcher/cdbconn.c:538"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/cdb/dispatcher/cdbconn.c:538"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/cdb/dispatcher/cdbconn.c:690"
    },
    {
        "log": "ereport(ERROR,...)",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/cdb/dispatcher/cdbdisp_query.c:134"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/cdb/dispatcher/cdbdispatchresult.c:327"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/cdb/dispatcher/cdbdispatchresult.c:328"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/cdb/dispatcher/cdbdispatchresult.c:524"
    },
    {
        "log": "ereport(ERROR, ...)",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/cdb/dispatcher/cdbdisp.c:71"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/access/transam/xact.c:4724"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/transam/xact.c:4724"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/access/transam/xact.c:4857"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/transam/xact.c:4857"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/access/transam/xact.c:5024"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/transam/xact.c:5024"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/transam/slru.c:614"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/transam/slru.c:687"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/access/transam/slru.c:739"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/transam/xlog.c:7388"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/transam/xlog.c:11965"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/transam/xlog.c:12573"
    },
    {
        "log": "ereport(elevel, rest)",
        "severity_level": "elevel",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/access/transam/test/varsup_test.c:10"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/transam/test/varsup_test.c:49"
    },
    {
        "log": "ereport(ERROR, ...)",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/access/transam/test/varsup_test.c:74"
    },
    {
        "log": "ereport(WARNING)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/transam/test/varsup_test.c:94"
    },
    {
        "log": "ereport(WARNING, ...)",
        "severity_level": "WARNING",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/access/transam/test/varsup_test.c:141"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/bitmap/bitmapinsert.c:1355"
    },
    {
        "log": "elog(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/access/nbtree/nbtutils.c:1862"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/nbtree/nbtinsert.c:214"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/nbtree/nbtinsert.c:295"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/nbtree/nbtinsert.c:907"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/nbtree/nbtsearch.c:1472"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/nbtree/nbtpage.c:532"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/nbtree/nbtpage.c:810"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/nbtree/nbtpage.c:901"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/nbtree/nbtpage.c:1460"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/nbtree/nbtpage.c:1803"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/access/common/reloptions.c:552"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/heap/hio.c:211"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/heap/heapam.c:1834"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/heap/heapam.c:3461"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/heap/heapam.c:4577"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/external/url.c:31"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/external/url_curl.c:162"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/access/gist/gistbuildbuffers.c:755"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/optimizer/util/clauses.c:3135"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/optimizer/util/clauses.c:4742"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/optimizer/util/clauses.c:5268"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/postmaster.c:63"
    },
    {
        "log": "ereport(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/postmaster.c:2176"
    },
    {
        "log": "ereport(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/postmaster.c:2179"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/postmaster.c:2306"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/postmaster/postmaster.c:5234"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/postmaster/postmaster.c:6822"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/checkpointer.c:359"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/postmaster/checkpointer.c:439"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/bgwriter.c:242"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/postmaster/bgwriter.c:276"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/pgstat.c:3187"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/bgworker.c:734"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/walwriter.c:223"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/autovacuum.c:558"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/postmaster/autovacuum.c:1628"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/catalog/heap.c:460"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/catalog/pg_shdepend.c:1046"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/catalog/pg_proc.c:940"
    },
    {
        "log": "ereport(ERROR,...)",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/catalog/namespace.c:3901"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/tcop/postgres.c:5134"
    },
    {
        "log": "ereport(ERROR, ...)",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/tcop/fastpath.c:210"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/misc/tzparser.c:9"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/misc/guc.c:5577"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/misc/guc.c:5865"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/misc/guc.c:5869"
    },
    {
        "log": "elog(LOG)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/misc/guc.c:9921"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/misc/ps_status.c:130"
    },
    {
        "log": "ereport(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/init/miscinit.c:1262"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/adt/json.c:1208"
    },
    {
        "log": "ereport(INVALID_XML_DOCUMENT)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/adt/xml.c:851"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/adt/xml.c:1804"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/adt/xml.c:1805"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/adt/ascii.c:168"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/adt/pg_locale.c:395"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/adt/pg_locale.c:520"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/adt/pg_locale.c:1441"
    },
    {
        "log": "ereport(ERROR, ...)",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/utils/adt/varlena.c:225"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/adt/formatting.c:4444"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/adt/numeric.c:2612"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/adt/numutils.c:33"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/adt/gp_dump_oids.c:216"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/cache/lsyscache.c:1000"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/cache/typcache.c:151"
    },
    {
        "log": "elog(PANIC,\n#else\n\t\telog(ERROR,\n#endif\n\t\t\t \"Relation decrement reference count found relation %u/%u/%u with bad count (reference count %d)\",\n\t\t\t rel->rd_node.spcNode,\n\t\t\t rel->rd_node.dbNode,\n\t\t\t rel->rd_node.relNode,\n\t\t\t rel->rd_refcnt);\n\t}\n\t\n\trel->rd_refcnt -= 1;\n\tif (!IsBootstrapProcessingMode())\n\t\tResourceOwnerForgetRelationRef(CurrentResourceOwner, rel);\n}\n\n/*\n * RelationClose - close an open relation\n *\n *\tActually, we just decrement the refcount.\n *\n *\tNOTE: if compiled with -DRELCACHE_FORCE_RELEASE then relcache entries\n *\twill be freed as soon as their refcount goes to zero.  In combination\n *\twith aset.c's CLOBBER_FREED_MEMORY option, this provides a good test\n *\tto catch references to already-released relcache entries.  It slows\n *\tthings down quite a bit, however.\n */\nvoid\nRelationClose(Relation relation)\n{\n\t/* Note: no locking manipulations needed */\n\tRelationDecrementReferenceCount(relation);\n\n#ifdef RELCACHE_FORCE_RELEASE\n\tif (RelationHasReferenceCountZero(relation) &&\n\t\trelation->rd_createSubid == InvalidSubTransactionId &&\n\t\trelation->rd_newRelfilenodeSubid == InvalidSubTransactionId)\n\t\tRelationClearRelation(relation, false);\n#endif\n}\n\n/*\n * RelationReloadIndexInfo - reload minimal information for an open index\n *\n *\tThis function is used only for indexes.  A relcache inval on an index\n *\tcan mean that its pg_class or pg_index row changed.  There are only\n *\tvery limited changes that are allowed to an existing index's schema,\n *\tso we can update the relcache entry without a complete rebuild; which\n *\tis fortunate because we can't rebuild an index entry that is \"nailed\"\n *\tand/or in active use.  We support full replacement of the pg_class row,\n *\tas well as updates of a few simple fields of the pg_index row.\n *\n *\tWe can't necessarily reread the catalog rows right away; we might be\n *\tin a failed transaction when we receive the SI notification.  If so,\n *\tRelationClearRelation just marks the entry as invalid by setting\n *\trd_isvalid to false.  This routine is called to fix the entry when it\n *\tis next needed.\n *\n *\tWe assume that at the time we are called, we have at least AccessShareLock\n *\ton the target index.  (Note: in the calls from RelationClearRelation,\n *\tthis is legitimate because we know the rel has positive refcount.)\n *\n *\tIf the target index is an index on pg_class or pg_index, we'd better have\n *\tpreviously gotten at least AccessShareLock on its underlying catalog,\n *\telse we are at risk of deadlock against someone trying to exclusive-lock\n *\tthe heap and index in that order.  This is ensured in current usage by\n *\tonly applying this to indexes being opened or having positive refcount.\n */\nstatic void\nRelationReloadIndexInfo(Relation relation)\n{\n\tbool\t\tindexOK;\n\tHeapTuple\tpg_class_tuple;\n\tForm_pg_class relp;\n\n\t/* Should be called only for invalidated indexes */\n\tAssert(relation->rd_rel->relkind == RELKIND_INDEX &&\n\t\t   !relation->rd_isvalid);\n\n\t/* Ensure it's closed at smgr level */\n\tRelationCloseSmgr(relation);\n\n\t/* Must free any AM cached data upon relcache flush */\n\tif (relation->rd_amcache)\n\t\tpfree(relation->rd_amcache);\n\trelation->rd_amcache = NULL;\n\n\t/*\n\t * If it's a shared index, we might be called before backend startup has\n\t * finished selecting a database, in which case we have no way to read\n\t * pg_class yet.  However, a shared index can never have any significant\n\t * schema updates, so it's okay to ignore the invalidation signal.  Just\n\t * mark it valid and return without doing anything more.\n\t */\n\tif (relation->rd_rel->relisshared && !criticalRelcachesBuilt)\n\t{\n\t\trelation->rd_isvalid = true;\n\t\treturn;\n\t}\n\n\t/*\n\t * Read the pg_class row\n\t *\n\t * Don't try to use an indexscan of pg_class_oid_index to reload the info\n\t * for pg_class_oid_index ...\n\t */\n\tindexOK = (RelationGetRelid(relation) != ClassOidIndexId);\n\tpg_class_tuple = ScanPgRelation(RelationGetRelid(relation), indexOK, false);\n\tif (!HeapTupleIsValid(pg_class_tuple))\n\t\telog(ERROR, \"could not find pg_class tuple for index %u\",\n\t\t\t RelationGetRelid(relation));\n\trelp = (Form_pg_class) GETSTRUCT(pg_class_tuple);\n\tmemcpy(relation->rd_rel, relp, CLASS_TUPLE_SIZE);\n\t/* Reload reloptions in case they changed */\n\tif (relation->rd_options)\n\t\tpfree(relation->rd_options);\n\tRelationParseRelOptions(relation, pg_class_tuple);\n\t/* done with pg_class tuple */\n\theap_freetuple(pg_class_tuple);\n\t/* We must recalculate physical address in case it changed */\n\tRelationInitPhysicalAddr(relation);\n\n\t/*\n\t * For a non-system index, there are fields of the pg_index row that are\n\t * allowed to change, so re-read that row and update the relcache entry.\n\t * Most of the info derived from pg_index (such as support function lookup\n\t * info) cannot change, and indeed the whole point of this routine is to\n\t * update the relcache entry without clobbering that data; so wholesale\n\t * replacement is not appropriate.\n\t */\n\tif (!IsSystemRelation(relation))\n\t{\n\t\tHeapTuple\ttuple;\n\t\tForm_pg_index index;\n\n\t\ttuple = SearchSysCache1(INDEXRELID,\n\t\t\t\t\t\t\t\tObjectIdGetDatum(RelationGetRelid(relation)));\n\t\tif (!HeapTupleIsValid(tuple))\n\t\t\telog(ERROR, \"cache lookup failed for index %u\",\n\t\t\t\t RelationGetRelid(relation));\n\t\tindex = (Form_pg_index) GETSTRUCT(tuple);\n\n\t\t/*\n\t\t * Basically, let's just copy all the bool fields.  There are one or\n\t\t * two of these that can't actually change in the current code, but\n\t\t * it's not worth it to track exactly which ones they are.  None of\n\t\t * the array fields are allowed to change, though.\n\t\t */\n\t\trelation->rd_index->indisunique = index->indisunique;\n\t\trelation->rd_index->indisprimary = index->indisprimary;\n\t\trelation->rd_index->indisexclusion = index->indisexclusion;\n\t\trelation->rd_index->indimmediate = index->indimmediate;\n\t\trelation->rd_index->indisclustered = index->indisclustered;\n\t\trelation->rd_index->indisvalid = index->indisvalid;\n\t\trelation->rd_index->indcheckxmin = index->indcheckxmin;\n\t\trelation->rd_index->indisready = index->indisready;\n\t\trelation->rd_index->indislive = index->indislive;\n\n\t\t/* Copy xmin too, as that is needed to make sense of indcheckxmin */\n\t\tHeapTupleHeaderSetXmin(relation->rd_indextuple->t_data,\n\t\t\t\t\t\t\t   HeapTupleHeaderGetXmin(tuple->t_data));\n\n\t\tReleaseSysCache(tuple);\n\t}\n\n\t/* Okay, now it's valid again */\n\trelation->rd_isvalid = true;\n}\n\n/*\n * RelationReloadNailed - reload minimal information for nailed relations.\n *\n * The structure of a nailed relation can never change (which is good, because\n * we rely on knowing their structure to be able to read catalog content). But\n * some parts, e.g. pg_class.relfrozenxid, are still important to have\n * accurate content for. Therefore those need to be reloaded after the arrival\n * of invalidations.\n */\nstatic void\nRelationReloadNailed(Relation relation)\n{\n\tAssert(relation->rd_isnailed);\n\n\t/*\n\t * Redo RelationInitPhysicalAddr in case it is a mapped relation whose\n\t * mapping changed.\n\t */\n\tRelationInitPhysicalAddr(relation);\n\n\t/* flag as needing to be revalidated */\n\trelation->rd_isvalid = false;\n\n\t/*\n\t * Can only reread catalog contents if in a transaction.  If the relation\n\t * is currently open (not counting the nailed refcount), do so\n\t * immediately. Otherwise we've already marked the entry as possibly\n\t * invalid, and it'll be fixed when next opened.\n\t */\n\tif (!IsTransactionState() || relation->rd_refcnt <= 1)\n\t\treturn;\n\n\tif (relation->rd_rel->relkind == RELKIND_INDEX)\n\t{\n\t\t/*\n\t\t * If it's a nailed-but-not-mapped index, then we need to re-read the\n\t\t * pg_class row to see if its relfilenode changed.\n\t\t */\n\t\tRelationReloadIndexInfo(relation);\n\t}\n\telse\n\t{\n\t\t/*\n\t\t * Reload a non-index entry.  We can't easily do so if relcaches\n\t\t * aren't yet built, but that's fine because at that stage the\n\t\t * attributes that need to be current (like relfrozenxid) aren't yet\n\t\t * accessed.  To ensure the entry will later be revalidated, we leave\n\t\t * it in invalid state, but allow use (cf. RelationIdGetRelation()).\n\t\t */\n\t\tif (criticalRelcachesBuilt)\n\t\t{\n\t\t\tHeapTuple\tpg_class_tuple;\n\t\t\tForm_pg_class relp;\n\n\t\t\t/*\n\t\t\t * NB: Mark the entry as valid before starting to scan, to avoid\n\t\t\t * self-recursion when re-building pg_class.\n\t\t\t */\n\t\t\trelation->rd_isvalid = true;\n\n\t\t\tpg_class_tuple = ScanPgRelation(RelationGetRelid(relation),\n\t\t\t\t\t\t\t\t\t\t\ttrue, false);\n\t\t\trelp = (Form_pg_class) GETSTRUCT(pg_class_tuple);\n\t\t\tmemcpy(relation->rd_rel, relp, CLASS_TUPLE_SIZE);\n\t\t\theap_freetuple(pg_class_tuple);\n\n\t\t\t/*\n\t\t\t * Again mark as valid, to protect against concurrently arriving\n\t\t\t * invalidations.\n\t\t\t */\n\t\t\trelation->rd_isvalid = true;\n\t\t}\n\t}\n}\n\n/*\n * RelationDestroyRelation\n *\n *\tPhysically delete a relation cache entry and all subsidiary data.\n *\tCaller must already have unhooked the entry from the hash table.\n */\nstatic void\nRelationDestroyRelation(Relation relation, bool remember_tupdesc)\n{\n\tAssert(RelationHasReferenceCountZero(relation));\n\n\t/*\n\t * Make sure smgr and lower levels close the relation's files, if they\n\t * weren't closed already.  (This was probably done by caller, but let's\n\t * just be real sure.)\n\t */\n\tRelationCloseSmgr(relation);\n\n\t/*\n\t * Free all the subsidiary data structures of the relcache entry, then the\n\t * entry itself.\n\t */\n\tif (relation->rd_rel)\n\t\tpfree(relation->rd_rel);\n\t/* can't use DecrTupleDescRefCount here */\n\tAssert(relation->rd_att->tdrefcount > 0);\n\tif (--relation->rd_att->tdrefcount == 0)\n\t{\n\t\t/*\n\t\t * If we Rebuilt a relcache entry during a transaction then its\n\t\t * possible we did that because the TupDesc changed as the result of\n\t\t * an ALTER TABLE that ran at less than AccessExclusiveLock. It's\n\t\t * possible someone copied that TupDesc, in which case the copy would\n\t\t * point to free'd memory. So if we rebuild an entry we keep the\n\t\t * TupDesc around until end of transaction, to be safe.\n\t\t */\n\t\tif (remember_tupdesc)\n\t\t\tRememberToFreeTupleDescAtEOX(relation->rd_att);\n\t\telse\n\t\t\tFreeTupleDesc(relation->rd_att);\n\t}\n\tlist_free(relation->rd_indexlist);\n\tbms_free(relation->rd_indexattr);\n\tbms_free(relation->rd_keyattr);\n\tbms_free(relation->rd_idattr);\n\tFreeTriggerDesc(relation->trigdesc);\n\tif (relation->rd_options)\n\t\tpfree(relation->rd_options);\n\tif (relation->rd_indextuple)\n\t\tpfree(relation->rd_indextuple);\n\tif (relation->rd_aotuple)\n\t\tpfree(relation->rd_aotuple);\n\tif (relation->rd_am)\n\t\tpfree(relation->rd_am);\n\tif (relation->rd_indexcxt)\n\t\tMemoryContextDelete(relation->rd_indexcxt);\n\tif (relation->rd_rulescxt)\n\t\tMemoryContextDelete(relation->rd_rulescxt);\n\tif (relation->rd_fdwroutine)\n\t\tpfree(relation->rd_fdwroutine);\n\tif (relation->rd_cdbpolicy)\n\t\tpfree(relation->rd_cdbpolicy);\n\n\tpfree(relation);\n}\n\n/*\n * RelationClearRelation\n *\n *\t Physically blow away a relation cache entry, or reset it and rebuild\n *\t it from scratch (that is, from catalog entries).  The latter path is\n *\t used when we are notified of a change to an open relation (one with\n *\t refcount > 0).\n *\n *\t NB: when rebuilding, we'd better hold some lock on the relation,\n *\t else the catalog data we need to read could be changing under us.\n *\t Also, a rel to be rebuilt had better have refcnt > 0.  This is because\n *\t an sinval reset could happen while we're accessing the catalogs, and\n *\t the rel would get blown away underneath us by RelationCacheInvalidate\n *\t if it has zero refcnt.\n *\n *\t The \"rebuild\" parameter is redundant in current usage because it has\n *\t to match the relation's refcnt status, but we keep it as a crosscheck\n *\t that we're doing what the caller expects.\n */\nstatic void\nRelationClearRelation(Relation relation, bool rebuild)\n{\n\t/*\n\t * As per notes above, a rel to be rebuilt MUST have refcnt > 0; while of\n\t * course it would be an equally bad idea to blow away one with nonzero\n\t * refcnt, since that would leave someone somewhere with a dangling\n\t * pointer.  All callers are expected to have verified that this holds.\n\t */\n\tAssert(rebuild ?\n\t\t   !RelationHasReferenceCountZero(relation) :\n\t\t   RelationHasReferenceCountZero(relation));\n\n\t/*\n\t * Make sure smgr and lower levels close the relation's files, if they\n\t * weren't closed already.  If the relation is not getting deleted, the\n\t * next smgr access should reopen the files automatically.  This ensures\n\t * that the low-level file access state is updated after, say, a vacuum\n\t * truncation.\n\t */\n\tRelationCloseSmgr(relation);\n\n\t/*\n\t * Treat nailed-in system relations separately, they always need to be\n\t * accessible, so we can't blow them away.\n\t */\n\tif (relation->rd_isnailed)\n\t{\n\t\tRelationReloadNailed(relation);\n\t\treturn;\n\t}\n\n\t/*\n\t * Even non-system indexes should not be blown away if they are open and\n\t * have valid index support information.  This avoids problems with active\n\t * use of the index support information.  As with nailed indexes, we\n\t * re-read the pg_class row to handle possible physical relocation of the\n\t * index, and we check for pg_index updates too.\n\t */\n\tif (relation->rd_rel->relkind == RELKIND_INDEX &&\n\t\trelation->rd_refcnt > 0 &&\n\t\trelation->rd_indexcxt != NULL)\n\t{\n\t\trelation->rd_isvalid = false;\t/* needs to be revalidated */\n\t\tif (IsTransactionState())\n\t\t\tRelationReloadIndexInfo(relation);\n\t\treturn;\n\t}\n\n\t/* Mark it invalid until we've finished rebuild */\n\trelation->rd_isvalid = false;\n\n\t/*\n\t * If we're really done with the relcache entry, blow it away. But if\n\t * someone is still using it, reconstruct the whole deal without moving\n\t * the physical RelationData record (so that the someone's pointer is\n\t * still valid).\n\t */\n\tif (!rebuild)\n\t{\n\t\t/* Remove it from the hash table */\n\t\tRelationCacheDelete(relation);\n\n\t\t/* And release storage */\n\t\tRelationDestroyRelation(relation, false);\n\t}\n\telse if (!IsTransactionState())\n\t{\n\t\t/*\n\t\t * If we're not inside a valid transaction, we can't do any catalog\n\t\t * access so it's not possible to rebuild yet.  Just exit, leaving\n\t\t * rd_isvalid = false so that the rebuild will occur when the entry is\n\t\t * next opened.\n\t\t *\n\t\t * Note: it's possible that we come here during subtransaction abort,\n\t\t * and the reason for wanting to rebuild is that the rel is open in\n\t\t * the outer transaction.  In that case it might seem unsafe to not\n\t\t * rebuild immediately, since whatever code has the rel already open\n\t\t * will keep on using the relcache entry as-is.  However, in such a\n\t\t * case the outer transaction should be holding a lock that's\n\t\t * sufficient to prevent any significant change in the rel's schema,\n\t\t * so the existing entry contents should be good enough for its\n\t\t * purposes; at worst we might be behind on statistics updates or the\n\t\t * like.  (See also CheckTableNotInUse() and its callers.)\tThese same\n\t\t * remarks also apply to the cases above where we exit without having\n\t\t * done RelationReloadIndexInfo() yet.\n\t\t */\n\t\treturn;\n\t}\n\telse\n\t{\n\t\t/*\n\t\t * Our strategy for rebuilding an open relcache entry is to build a\n\t\t * new entry from scratch, swap its contents with the old entry, and\n\t\t * finally delete the new entry (along with any infrastructure swapped\n\t\t * over from the old entry).  This is to avoid trouble in case an\n\t\t * error causes us to lose control partway through.  The old entry\n\t\t * will still be marked !rd_isvalid, so we'll try to rebuild it again\n\t\t * on next access.  Meanwhile it's not any less valid than it was\n\t\t * before, so any code that might expect to continue accessing it\n\t\t * isn't hurt by the rebuild failure.  (Consider for example a\n\t\t * subtransaction that ALTERs a table and then gets canceled partway\n\t\t * through the cache entry rebuild.  The outer transaction should\n\t\t * still see the not-modified cache entry as valid.)  The worst\n\t\t * consequence of an error is leaking the necessarily-unreferenced new\n\t\t * entry, and this shouldn't happen often enough for that to be a big\n\t\t * problem.\n\t\t *\n\t\t * When rebuilding an open relcache entry, we must preserve ref count,\n\t\t * rd_createSubid/rd_newRelfilenodeSubid, and rd_toastoid state.  Also\n\t\t * attempt to preserve the pg_class entry (rd_rel), tupledesc, and\n\t\t * rewrite-rule substructures in place, because various places assume\n\t\t * that these structures won't move while they are working with an\n\t\t * open relcache entry.  (Note: the refcount mechanism for tupledescs\n\t\t * might someday allow us to remove this hack for the tupledesc.)\n\t\t *\n\t\t * When rebuilding an open relcache entry, we must preserve ref count\n\t\t * and rd_createSubid/rd_newRelfilenodeSubid state.  Also attempt to\n\t\t * preserve the pg_class entry (rd_rel), tupledesc, and rewrite-rule\n\t\t * substructures in place, because various places assume that these\n\t\t * structures won't move while they are working with an open relcache\n\t\t * entry.  (Note: the refcount mechanism for tupledescs might someday\n\t\t * allow us to remove this hack for the tupledesc.)\n\t\t *\n\t\t * Note that this process does not touch CurrentResourceOwner; which\n\t\t * is good because whatever ref counts the entry may have do not\n\t\t * necessarily belong to that resource owner.\n \t\t */\n\t\tRelation\tnewrel;\n\t\tOid\t\t\tsave_relid = RelationGetRelid(relation);\n\t\tbool\t\tkeep_tupdesc;\n\t\tbool\t\tkeep_rules;\n\t\tbool\t\tkeep_policy;\n\n\t\t/* Build temporary entry, but don't link it into hashtable */\n\t\tnewrel = RelationBuildDesc(save_relid, false);\n\t\tif (newrel == NULL)\n\t\t{\n\t\t\t/*\n\t\t\t * We can validly get here, if we're using a historic snapshot in\n\t\t\t * which a relation, accessed from outside logical decoding, is\n\t\t\t * still invisible. In that case it's fine to just mark the\n\t\t\t * relation as invalid and return - it'll fully get reloaded by\n\t\t\t * the cache reset at the end of logical decoding (or at the next\n\t\t\t * access).  During normal processing we don't want to ignore this\n\t\t\t * case as it shouldn't happen there, as explained below.\n\t\t\t */\n\t\t\tif (HistoricSnapshotActive())\n\t\t\t\treturn;\n\n\t\t\t/*\n\t\t\t * This shouldn't happen as dropping a relation is intended to be\n\t\t\t * impossible if still referenced (c.f. CheckTableNotInUse()). But\n\t\t\t * if we get here anyway, we can't just delete the relcache entry,\n\t\t\t * as it possibly could get accessed later (as e.g. the error\n\t\t\t * might get trapped and handled via a subtransaction rollback).\n\t\t\t */\n\t\t\telog(ERROR, \"relation %u deleted while still in use\", save_relid);\n\t\t}\n\n\t\tkeep_tupdesc = equalTupleDescs(relation->rd_att, newrel->rd_att, true);\n\t\tkeep_rules = equalRuleLocks(relation->rd_rules, newrel->rd_rules);\n\t\tkeep_policy = GpPolicyEqual(relation->rd_cdbpolicy, newrel->rd_cdbpolicy);\n\n\t\t/*\n\t\t * Perform swapping of the relcache entry contents.  Within this\n\t\t * process the old entry is momentarily invalid, so there *must*\n\t\t * be no possibility of CHECK_FOR_INTERRUPTS within this sequence.\n\t\t * Do it in all-in-line code for safety.\n\t\t *\n\t\t * Since the vast majority of fields should be swapped, our method\n\t\t * is to swap the whole structures and then re-swap those few fields\n\t\t * we didn't want swapped.\n\t\t */\n#define SWAPFIELD(fldtype, fldname) \\\n\t\tdo { \\\n\t\t\tfldtype _tmp = newrel->fldname; \\\n\t\t\tnewrel->fldname = relation->fldname; \\\n\t\t\trelation->fldname = _tmp; \\\n\t\t} while (0)\n\n\t\t/* swap all Relation struct fields */\n\t\t{\n\t\t\tRelationData tmpstruct;\n\n\t\t\tmemcpy(&tmpstruct, newrel, sizeof(RelationData));\n\t\t\tmemcpy(newrel, relation, sizeof(RelationData));\n\t\t\tmemcpy(relation, &tmpstruct, sizeof(RelationData));\n\t\t}\n\n\t\t/* rd_smgr must not be swapped, due to back-links from smgr level */\n\t\tSWAPFIELD(SMgrRelation, rd_smgr);\n\t\t/* rd_refcnt must be preserved */\n\t\tSWAPFIELD(int, rd_refcnt);\n\t\t/* isnailed shouldn't change */\n\t\tAssert(newrel->rd_isnailed == relation->rd_isnailed);\n\t\t/* creation sub-XIDs must be preserved */\n\t\tSWAPFIELD(SubTransactionId, rd_createSubid);\n\t\tSWAPFIELD(SubTransactionId, rd_newRelfilenodeSubid);\n\t\t/* un-swap rd_rel pointers, swap contents instead */\n\t\tSWAPFIELD(Form_pg_class, rd_rel);\n\t\t/* ... but actually, we don't have to update newrel->rd_rel */\n\t\tmemcpy(relation->rd_rel, newrel->rd_rel, CLASS_TUPLE_SIZE);\n\t\t/* preserve old tupledesc and rules if no logical change */\n\t\tif (keep_tupdesc)\n\t\t\tSWAPFIELD(TupleDesc, rd_att);\n\t\tif (keep_rules)\n\t\t{\n\t\t\tSWAPFIELD(RuleLock *, rd_rules);\n\t\t\tSWAPFIELD(MemoryContext, rd_rulescxt);\n\t\t}\n\t\t/* also preserve old policy if no logical change */\n\t\tif (keep_policy)\n\t\t\tSWAPFIELD(GpPolicy *, rd_cdbpolicy);\n\t\t/* pgstat_info must be preserved */\n\t\tSWAPFIELD(struct PgStat_TableStatus *, pgstat_info);\n\n#undef SWAPFIELD\n\n\t\t/* And now we can throw away the temporary entry */\n\t\tRelationDestroyRelation(newrel, !keep_tupdesc);\n\t}\n}\n\n/*\n * RelationFlushRelation\n *\n *\t Rebuild the relation if it is open (refcount > 0), else blow it away.\n *\t This is used when we receive a cache invalidation event for the rel.\n */\nstatic void\nRelationFlushRelation(Relation relation)\n{\n\tif (relation->rd_createSubid != InvalidSubTransactionId ||\n\t\trelation->rd_newRelfilenodeSubid != InvalidSubTransactionId)\n\t{\n\t\t/*\n\t\t * New relcache entries are always rebuilt, not flushed; else we'd\n\t\t * forget the \"new\" status of the relation, which is a useful\n\t\t * optimization to have.  Ditto for the new-relfilenode status.\n\t\t *\n\t\t * The rel could have zero refcnt here, so temporarily increment the\n\t\t * refcnt to ensure it's safe to rebuild it.  We can assume that the\n\t\t * current transaction has some lock on the rel already.\n\t\t */\n\t\tRelationIncrementReferenceCount(relation);\n\t\tRelationClearRelation(relation, true);\n\t\tRelationDecrementReferenceCount(relation);\n\t}\n\telse\n\t{\n\t\t/*\n\t\t * Pre-existing rels can be dropped from the relcache if not open.\n\t\t */\n\t\tbool\t\trebuild = !RelationHasReferenceCountZero(relation);\n\n\t\tRelationClearRelation(relation, rebuild);\n\t}\n}\n\n/*\n * RelationForgetRelation - unconditionally remove a relcache entry\n *\n *\t\t   External interface for destroying a relcache entry when we\n *\t\t   drop the relation.\n */\nvoid\nRelationForgetRelation(Oid rid)\n{\n\tRelation\trelation;\n\n\tRelationIdCacheLookup(rid, relation);\n\n\tif (!PointerIsValid(relation))\n\t\treturn;\t\t\t\t\t/* not in cache, nothing to do */\n\n\tif (!RelationHasReferenceCountZero(relation))\n\t\telog(ERROR, \"relation %u is still open\", rid);\n\n\t/* Unconditionally destroy the relcache entry */\n\tRelationClearRelation(relation, false);\n}\n\n/*\n *\t\tRelationCacheInvalidateEntry\n *\n *\t\tThis routine is invoked for SI cache flush messages.\n *\n * Any relcache entry matching the relid must be flushed.  (Note: caller has\n * already determined that the relid belongs to our database or is a shared\n * relation.)\n *\n * We used to skip local relations, on the grounds that they could\n * not be targets of cross-backend SI update messages; but it seems\n * safer to process them, so that our *own* SI update messages will\n * have the same effects during CommandCounterIncrement for both\n * local and nonlocal relations.\n */\nvoid\nRelationCacheInvalidateEntry(Oid relationId)\n{\n\tRelation\trelation;\n\n\tRelationIdCacheLookup(relationId, relation);\n\n\tif (PointerIsValid(relation))\n\t{\n\t\trelcacheInvalsReceived++;\n\t\tRelationFlushRelation(relation);\n\t}\n}\n\n/*\n * RelationCacheInvalidate\n *\t Blow away cached relation descriptors that have zero reference counts,\n *\t and rebuild those with positive reference counts.  Also reset the smgr\n *\t relation cache and re-read relation mapping data.\n *\n *\t This is currently used only to recover from SI message buffer overflow,\n *\t so we do not touch new-in-transaction relations; they cannot be targets\n *\t of cross-backend SI updates (and our own updates now go through a\n *\t separate linked list that isn't limited by the SI message buffer size).\n *\t Likewise, we need not discard new-relfilenode-in-transaction hints,\n *\t since any invalidation of those would be a local event.\n *\n *\t We do this in two phases: the first pass deletes deletable items, and\n *\t the second one rebuilds the rebuildable items.  This is essential for\n *\t safety, because hash_seq_search only copes with concurrent deletion of\n *\t the element it is currently visiting.  If a second SI overflow were to\n *\t occur while we are walking the table, resulting in recursive entry to\n *\t this routine, we could crash because the inner invocation blows away\n *\t the entry next to be visited by the outer scan.  But this way is OK,\n *\t because (a) during the first pass we won't process any more SI messages,\n *\t so hash_seq_search will complete safely; (b) during the second pass we\n *\t only hold onto pointers to nondeletable entries.\n *\n *\t The two-phase approach also makes it easy to update relfilenodes for\n *\t mapped relations before we do anything else, and to ensure that the\n *\t second pass processes nailed-in-cache items before other nondeletable\n *\t items.  This should ensure that system catalogs are up to date before\n *\t we attempt to use them to reload information about other open relations.\n */\nvoid\nRelationCacheInvalidate(void)\n{\n\tHASH_SEQ_STATUS status;\n\tRelIdCacheEnt *idhentry;\n\tRelation\trelation;\n\tList\t   *rebuildFirstList = NIL;\n\tList\t   *rebuildList = NIL;\n\tListCell   *l;\n\n\t/*\n\t * Reload relation mapping data before starting to reconstruct cache.\n\t */\n\tRelationMapInvalidateAll();\n\n\t/* Phase 1 */\n\thash_seq_init(&status, RelationIdCache);\n\n\twhile ((idhentry = (RelIdCacheEnt *) hash_seq_search(&status)) != NULL)\n\t{\n\t\trelation = idhentry->reldesc;\n\n\t\t/* Must close all smgr references to avoid leaving dangling ptrs */\n\t\tRelationCloseSmgr(relation);\n\n\t\t/*\n\t\t * Ignore new relations; no other backend will manipulate them before\n\t\t * we commit.  Likewise, before replacing a relation's relfilenode, we\n\t\t * shall have acquired AccessExclusiveLock and drained any applicable\n\t\t * pending invalidations.\n\t\t */\n\t\tif (relation->rd_createSubid != InvalidSubTransactionId ||\n\t\t\trelation->rd_newRelfilenodeSubid != InvalidSubTransactionId)\n\t\t\tcontinue;\n\n\t\trelcacheInvalsReceived++;\n\n\t\tif (RelationHasReferenceCountZero(relation))\n\t\t{\n\t\t\t/* Delete this entry immediately */\n\t\t\tAssert(!relation->rd_isnailed);\n\t\t\tRelationClearRelation(relation, false);\n\t\t}\n\t\telse\n\t\t{\n\t\t\t/*\n\t\t\t * If it's a mapped relation, immediately update its rd_node in\n\t\t\t * case its relfilenode changed.  We must do this during phase 1\n\t\t\t * in case the relation is consulted during rebuild of other\n\t\t\t * relcache entries in phase 2.  It's safe since consulting the\n\t\t\t * map doesn't involve any access to relcache entries.\n\t\t\t */\n\t\t\tif (RelationIsMapped(relation))\n\t\t\t\tRelationInitPhysicalAddr(relation);\n\n\t\t\t/*\n\t\t\t * Add this entry to list of stuff to rebuild in second pass.\n\t\t\t * pg_class goes to the front of rebuildFirstList while\n\t\t\t * pg_class_oid_index goes to the back of rebuildFirstList, so\n\t\t\t * they are done first and second respectively.  Other nailed\n\t\t\t * relations go to the front of rebuildList, so they'll be done\n\t\t\t * next in no particular order; and everything else goes to the\n\t\t\t * back of rebuildList.\n\t\t\t */\n\t\t\tif (RelationGetRelid(relation) == RelationRelationId)\n\t\t\t\trebuildFirstList = lcons(relation, rebuildFirstList);\n\t\t\telse if (RelationGetRelid(relation) == ClassOidIndexId)\n\t\t\t\trebuildFirstList = lappend(rebuildFirstList, relation);\n\t\t\telse if (relation->rd_isnailed)\n\t\t\t\trebuildList = lcons(relation, rebuildList);\n\t\t\telse\n\t\t\t\trebuildList = lappend(rebuildList, relation);\n\t\t}\n\t}\n\n\t/*\n\t * Now zap any remaining smgr cache entries.  This must happen before we\n\t * start to rebuild entries, since that may involve catalog fetches which\n\t * will re-open catalog files.\n\t */\n\tsmgrcloseall();\n\n\t/* Phase 2: rebuild the items found to need rebuild in phase 1 */\n\tforeach(l, rebuildFirstList)\n\t{\n\t\trelation = (Relation) lfirst(l);\n\t\tRelationClearRelation(relation, true);\n\t}\n\tlist_free(rebuildFirstList);\n\tforeach(l, rebuildList)\n\t{\n\t\trelation = (Relation) lfirst(l);\n\t\tRelationClearRelation(relation, true);\n\t}\n\tlist_free(rebuildList);\n}\n\n/*\n * RelationCloseSmgrByOid - close a relcache entry's smgr link\n *\n * Needed in some cases where we are changing a relation's physical mapping.\n * The link will be automatically reopened on next use.\n */\nvoid\nRelationCloseSmgrByOid(Oid relationId)\n{\n\tRelation\trelation;\n\n\tRelationIdCacheLookup(relationId, relation);\n\n\tif (!PointerIsValid(relation))\n\t\treturn;\t\t\t\t\t/* not in cache, nothing to do */\n\n\tRelationCloseSmgr(relation);\n}\n\nstatic void\nRememberToFreeTupleDescAtEOX(TupleDesc td)\n{\n\tif (EOXactTupleDescArray == NULL)\n\t{\n\t\tMemoryContext oldcxt;\n\n\t\toldcxt = MemoryContextSwitchTo(CacheMemoryContext);\n\n\t\tEOXactTupleDescArray = (TupleDesc *) palloc(16 * sizeof(TupleDesc));\n\t\tEOXactTupleDescArrayLen = 16;\n\t\tNextEOXactTupleDescNum = 0;\n\t\tMemoryContextSwitchTo(oldcxt);\n\t}\n\telse if (NextEOXactTupleDescNum >= EOXactTupleDescArrayLen)\n\t{\n\t\tint32\t\tnewlen = EOXactTupleDescArrayLen * 2;\n\n\t\tAssert(EOXactTupleDescArrayLen > 0);\n\n\t\tEOXactTupleDescArray = (TupleDesc *) repalloc(EOXactTupleDescArray,\n\t\t\t\t\t\t\t\t\t\t\t\t newlen * sizeof(TupleDesc));\n\t\tEOXactTupleDescArrayLen = newlen;\n\t}\n\n\tEOXactTupleDescArray[NextEOXactTupleDescNum++] = td;\n}\n\n/*\n * AtEOXact_RelationCache\n *\n *\tClean up the relcache at main-transaction commit or abort.\n *\n * Note: this must be called *before* processing invalidation messages.\n * In the case of abort, we don't want to try to rebuild any invalidated\n * cache entries (since we can't safely do database accesses).  Therefore\n * we must reset refcnts before handling pending invalidations.\n *\n * As of PostgreSQL 8.1, relcache refcnts should get released by the\n * ResourceOwner mechanism.  This routine just does a debugging\n * cross-check that no pins remain.  However, we also need to do special\n * cleanup when the current transaction created any relations or made use\n * of forced index lists.\n */\nvoid\nAtEOXact_RelationCache(bool isCommit)\n{\n\tHASH_SEQ_STATUS status;\n\tRelIdCacheEnt *idhentry;\n\tint\t\t\ti;\n\n\t/*\n\t * Unless the eoxact_list[] overflowed, we only need to examine the rels\n\t * listed in it.  Otherwise fall back on a hash_seq_search scan.\n\t *\n\t * For simplicity, eoxact_list[] entries are not deleted till end of\n\t * top-level transaction, even though we could remove them at\n\t * subtransaction end in some cases, or remove relations from the list if\n\t * they are cleared for other reasons.  Therefore we should expect the\n\t * case that list entries are not found in the hashtable; if not, there's\n\t * nothing to do for them.\n\t *\n\t * MPP-3333: READERS need to *always* scan, otherwise they will not be able\n\t * to maintain a coherent view of the storage layer.\n\t */\n\tif (eoxact_list_overflowed || DistributedTransactionContext == DTX_CONTEXT_QE_READER)\n\t{\n\t\thash_seq_init(&status, RelationIdCache);\n\t\twhile ((idhentry = (RelIdCacheEnt *) hash_seq_search(&status)) != NULL)\n\t\t{\n\t\t\tAtEOXact_cleanup(idhentry->reldesc, isCommit);\n\t\t}\n\t}\n\telse\n\t{\n\t\tfor (i = 0; i < eoxact_list_len; i++)\n\t\t{\n\t\t\tidhentry = (RelIdCacheEnt *) hash_search(RelationIdCache,\n\t\t\t\t\t\t\t\t\t\t\t\t\t (void *) &eoxact_list[i],\n\t\t\t\t\t\t\t\t\t\t\t\t\t HASH_FIND,\n\t\t\t\t\t\t\t\t\t\t\t\t\t NULL);\n\t\t\tif (idhentry != NULL)\n\t\t\t\tAtEOXact_cleanup(idhentry->reldesc, isCommit);\n\t\t}\n\t}\n\n\tif (EOXactTupleDescArrayLen > 0)\n\t{\n\t\tAssert(EOXactTupleDescArray != NULL);\n\t\tfor (i = 0; i < NextEOXactTupleDescNum; i++)\n\t\t\tFreeTupleDesc(EOXactTupleDescArray[i]);\n\t\tpfree(EOXactTupleDescArray);\n\t\tEOXactTupleDescArray = NULL;\n\t}\n\n\t/* Now we're out of the transaction and can clear the lists */\n\teoxact_list_len = 0;\n\teoxact_list_overflowed = false;\n\tNextEOXactTupleDescNum = 0;\n\tEOXactTupleDescArrayLen = 0;\n}\n\n/*\n * AtEOXact_cleanup\n *\n *\tClean up a single rel at main-transaction commit or abort\n *\n * NB: this processing must be idempotent, because EOXactListAdd() doesn't\n * bother to prevent duplicate entries in eoxact_list[].\n */\nstatic void\nAtEOXact_cleanup(Relation relation, bool isCommit)\n{\n\t/*\n\t * The relcache entry's ref count should be back to its normal\n\t * not-in-a-transaction state: 0 unless it's nailed in cache.\n\t *\n\t * In bootstrap mode, this is NOT true, so don't check it --- the\n\t * bootstrap code expects relations to stay open across start/commit\n\t * transaction calls.  (That seems bogus, but it's not worth fixing.)\n\t *\n\t * Note: ideally this check would be applied to every relcache entry, not\n\t * just those that have eoxact work to do.  But it's not worth forcing a\n\t * scan of the whole relcache just for this.  (Moreover, doing so would\n\t * mean that assert-enabled testing never tests the hash_search code path\n\t * above, which seems a bad idea.)\n\t */\n#ifdef USE_ASSERT_CHECKING\n\tif (!IsBootstrapProcessingMode())\n\t{\n\t\tint\t\t\texpected_refcnt;\n\n\t\texpected_refcnt = relation->rd_isnailed ? 1 : 0;\n\t\tAssert(relation->rd_refcnt == expected_refcnt);\n\t}\n#endif\n\n\t/*\n\t * QE-readers aren't properly enrolled in transactions, they\n\t * just get the snapshot which corresponds -- so here, where\n\t * we are maintaining their relcache, we want to just clean\n\t * up (almost as if we had aborted). (MPP-3338)\n\t */\n\tif (DistributedTransactionContext == DTX_CONTEXT_QE_ENTRY_DB_SINGLETON ||\n\t\tDistributedTransactionContext == DTX_CONTEXT_QE_READER)\n\t{\n\t\tRelationClearRelation(relation, relation->rd_isnailed ? true : false);\n\t\treturn;\n\t}\n\n\t/*\n\t * Is it a relation created in the current transaction?\n\t *\n\t * During commit, reset the flag to zero, since we are now out of the\n\t * creating transaction.  During abort, simply delete the relcache entry\n\t * --- it isn't interesting any longer.  (NOTE: if we have forgotten the\n\t * new-ness of a new relation due to a forced cache flush, the entry will\n\t * get deleted anyway by shared-cache-inval processing of the aborted\n\t * pg_class insertion.)\n\t */\n\tif (relation->rd_createSubid != InvalidSubTransactionId)\n\t{\n\t\tif (isCommit)\n\t\t\trelation->rd_createSubid = InvalidSubTransactionId;\n\t\telse if (RelationHasReferenceCountZero(relation))\n\t\t{\n\t\t\tRelationClearRelation(relation, false);\n\t\t\treturn;\n\t\t}\n\t\telse\n\t\t{\n\t\t\t/*\n\t\t\t * Hmm, somewhere there's a (leaked?) reference to the relation.\n\t\t\t * We daren't remove the entry for fear of dereferencing a\n\t\t\t * dangling pointer later.  Bleat, and mark it as not belonging to\n\t\t\t * the current transaction.  Hopefully it'll get cleaned up\n\t\t\t * eventually.  This must be just a WARNING to avoid\n\t\t\t * error-during-error-recovery loops.\n\t\t\t */\n\t\t\trelation->rd_createSubid = InvalidSubTransactionId;\n\t\t\telog(WARNING, \"cannot remove relcache entry for \\\"%s\\\" because it has nonzero refcount\",\n\t\t\t\t RelationGetRelationName(relation));\n\t\t}\n\t}\n\n\t/*\n\t * Likewise, reset the hint about the relfilenode being new.\n\t */\n\trelation->rd_newRelfilenodeSubid = InvalidSubTransactionId;\n\n\t/*\n\t * Flush any temporary index list.\n\t */\n\tif (relation->rd_indexvalid == 2)\n\t{\n\t\tlist_free(relation->rd_indexlist);\n\t\trelation->rd_indexlist = NIL;\n\t\trelation->rd_oidindex = InvalidOid;\n\t\trelation->rd_replidindex = InvalidOid;\n\t\trelation->rd_indexvalid = 0;\n\t}\n}\n\n/*\n * AtEOSubXact_RelationCache\n *\n *\tClean up the relcache at sub-transaction commit or abort.\n *\n * Note: this must be called *before* processing invalidation messages.\n */\nvoid\nAtEOSubXact_RelationCache(bool isCommit, SubTransactionId mySubid,\n\t\t\t\t\t\t  SubTransactionId parentSubid)\n{\n\tHASH_SEQ_STATUS status;\n\tRelIdCacheEnt *idhentry;\n\tint\t\t\ti;\n\n\t/*\n\t * Unless the eoxact_list[] overflowed, we only need to examine the rels\n\t * listed in it.  Otherwise fall back on a hash_seq_search scan.  Same\n\t * logic as in AtEOXact_RelationCache.\n\t */\n\tif (eoxact_list_overflowed || DistributedTransactionContext == DTX_CONTEXT_QE_READER)\n\t{\n\t\thash_seq_init(&status, RelationIdCache);\n\t\twhile ((idhentry = (RelIdCacheEnt *) hash_seq_search(&status)) != NULL)\n\t\t{\n\t\t\tAtEOSubXact_cleanup(idhentry->reldesc, isCommit,\n\t\t\t\t\t\t\t\tmySubid, parentSubid);\n\t\t}\n\t}\n\telse\n\t{\n\t\tfor (i = 0; i < eoxact_list_len; i++)\n\t\t{\n\t\t\tidhentry = (RelIdCacheEnt *) hash_search(RelationIdCache,\n\t\t\t\t\t\t\t\t\t\t\t\t\t (void *) &eoxact_list[i],\n\t\t\t\t\t\t\t\t\t\t\t\t\t HASH_FIND,\n\t\t\t\t\t\t\t\t\t\t\t\t\t NULL);\n\t\t\tif (idhentry != NULL)\n\t\t\t\tAtEOSubXact_cleanup(idhentry->reldesc, isCommit,\n\t\t\t\t\t\t\t\t\tmySubid, parentSubid);\n\t\t}\n\t}\n\n\t/* Don't reset the list; we still need more cleanup later */\n}\n\n/*\n * AtEOSubXact_cleanup\n *\n *\tClean up a single rel at subtransaction commit or abort\n *\n * NB: this processing must be idempotent, because EOXactListAdd() doesn't\n * bother to prevent duplicate entries in eoxact_list[].\n */\nstatic void\nAtEOSubXact_cleanup(Relation relation, bool isCommit,\n\t\t\t\t\tSubTransactionId mySubid, SubTransactionId parentSubid)\n{\n\t/*\n\t * As opposed to AtEOXact_RelationCache, subtransactions\n\t * in readers are only caused by internal commands, and\n\t * there shouldn't be interaction with global transactions,\n\t * (reader gangs commit their transaction independently)\n\t * we must not clear the relcache here.\n\t */\n\n\t/*\n\t * Is it a relation created in the current subtransaction?\n\t *\n\t * During subcommit, mark it as belonging to the parent, instead. During\n\t * subabort, simply delete the relcache entry.\n\t */\n\tif (relation->rd_createSubid == mySubid)\n\t{\n\t\tif (isCommit)\n\t\t\trelation->rd_createSubid = parentSubid;\n\t\telse if (RelationHasReferenceCountZero(relation))\n\t\t{\n\t\t\tRelationClearRelation(relation, false);\n\t\t\treturn;\n\t\t}\n\t\telse\n\t\t{\n\t\t\t/*\n\t\t\t * Hmm, somewhere there's a (leaked?) reference to the relation.\n\t\t\t * We daren't remove the entry for fear of dereferencing a\n\t\t\t * dangling pointer later.  Bleat, and transfer it to the parent\n\t\t\t * subtransaction so we can try again later.  This must be just a\n\t\t\t * WARNING to avoid error-during-error-recovery loops.\n\t\t\t */\n\t\t\trelation->rd_createSubid = parentSubid;\n\t\t\telog(WARNING, \"cannot remove relcache entry for \\\"%s\\\" because it has nonzero refcount\",\n\t\t\t\t RelationGetRelationName(relation));\n\t\t}\n\t}\n\n\t/*\n\t * Likewise, update or drop any new-relfilenode-in-subtransaction hint.\n\t */\n\tif (relation->rd_newRelfilenodeSubid == mySubid)\n\t{\n\t\tif (isCommit)\n\t\t\trelation->rd_newRelfilenodeSubid = parentSubid;\n\t\telse\n\t\t\trelation->rd_newRelfilenodeSubid = InvalidSubTransactionId;\n\t}\n\n\t/*\n\t * Flush any temporary index list.\n\t */\n\tif (relation->rd_indexvalid == 2)\n\t{\n\t\tlist_free(relation->rd_indexlist);\n\t\trelation->rd_indexlist = NIL;\n\t\trelation->rd_oidindex = InvalidOid;\n\t\trelation->rd_replidindex = InvalidOid;\n\t\trelation->rd_indexvalid = 0;\n\t}\n}\n\n\n/*\n *\t\tRelationBuildLocalRelation\n *\t\t\tBuild a relcache entry for an about-to-be-created relation,\n *\t\t\tand enter it into the relcache.\n */\nRelation\nRelationBuildLocalRelation(const char *relname,\n\t\t\t\t\t\t   Oid relnamespace,\n\t\t\t\t\t\t   TupleDesc tupDesc,\n\t\t\t\t\t\t   Oid relid,\n\t\t\t\t\t\t   Oid relfilenode,\n\t\t\t\t\t\t   Oid reltablespace,\n\t\t\t\t\t\t   bool shared_relation,\n\t\t\t\t\t\t   bool mapped_relation,\n\t\t\t\t\t\t   char relpersistence,\n\t\t\t\t\t\t   char relkind)\n{\n\tRelation\trel;\n\tMemoryContext oldcxt;\n\tint\t\t\tnatts = tupDesc->natts;\n\tint\t\t\ti;\n\tbool\t\thas_not_null;\n\tbool\t\tnailit;\n\n\tAssertArg(natts >= 0);\n\n\t/*\n\t * check for creation of a rel that must be nailed in cache.\n\t *\n\t * XXX this list had better match the relations specially handled in\n\t * RelationCacheInitializePhase2/3.\n\t */\n\tswitch (relid)\n\t{\n\t\tcase DatabaseRelationId:\n\t\tcase AuthIdRelationId:\n\t\tcase AuthMemRelationId:\n\t\tcase AuthTimeConstraintRelationId:\n\t\tcase RelationRelationId:\n\t\tcase AttributeRelationId:\n\t\tcase ProcedureRelationId:\n\t\tcase TypeRelationId:\n\t\t\tnailit = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tnailit = false;\n\t\t\tbreak;\n\t}\n\n\t/*\n\t * check that hardwired list of shared rels matches what's in the\n\t * bootstrap .bki file.  If you get a failure here during initdb, you\n\t * probably need to fix IsSharedRelation() to match whatever you've done\n\t * to the set of shared relations.\n\t */\n\tif (shared_relation != IsSharedRelation(relid))\n\t\telog(ERROR, \"shared_relation flag for \\\"%s\\\" does not match IsSharedRelation(%u)\",\n\t\t\t relname, relid);\n\n\t/* Shared relations had better be mapped, too */\n\tAssert(mapped_relation || !shared_relation);\n\n\t/*\n\t * switch to the cache context to create the relcache entry.\n\t */\n\tif (!CacheMemoryContext)\n\t\tCreateCacheMemoryContext();\n\n\toldcxt = MemoryContextSwitchTo(CacheMemoryContext);\n\n\t/*\n\t * allocate a new relation descriptor and fill in basic state fields.\n\t */\n\trel = (Relation) palloc0(sizeof(RelationData));\n\n\t/* make sure relation is marked as having no open file yet */\n\trel->rd_smgr = NULL;\n\n\t/* mark it nailed if appropriate */\n\trel->rd_isnailed = nailit;\n\n\trel->rd_refcnt = nailit ? 1 : 0;\n\n\t/* it's being created in this transaction */\n\trel->rd_createSubid = GetCurrentSubTransactionId();\n\trel->rd_newRelfilenodeSubid = InvalidSubTransactionId;\n\n\t/*\n\t * create a new tuple descriptor from the one passed in.  We do this\n\t * partly to copy it into the cache context, and partly because the new\n\t * relation can't have any defaults or constraints yet; they have to be\n\t * added in later steps, because they require additions to multiple system\n\t * catalogs.  We can copy attnotnull constraints here, however.\n\t */\n\trel->rd_att = CreateTupleDescCopy(tupDesc);\n\trel->rd_att->tdrefcount = 1;\t/* mark as refcounted */\n\thas_not_null = false;\n\tfor (i = 0; i < natts; i++)\n\t{\n\t\trel->rd_att->attrs[i]->attnotnull = tupDesc->attrs[i]->attnotnull;\n\t\thas_not_null |= tupDesc->attrs[i]->attnotnull;\n\t}\n\n\tif (has_not_null)\n\t{\n\t\tTupleConstr *constr = (TupleConstr *) palloc0(sizeof(TupleConstr));\n\n\t\tconstr->has_not_null = true;\n\t\trel->rd_att->constr = constr;\n\t}\n\n\t/*\n\t * initialize relation tuple form (caller may add/override data later)\n\t */\n\trel->rd_rel = (Form_pg_class) palloc0(CLASS_TUPLE_SIZE);\n\n\tnamestrcpy(&rel->rd_rel->relname, relname);\n\trel->rd_rel->relnamespace = relnamespace;\n\n\trel->rd_rel->relstorage = RELSTORAGE_HEAP;\n\trel->rd_rel->relkind = relkind;\n\trel->rd_rel->relhasoids = rel->rd_att->tdhasoid;\n\trel->rd_rel->relnatts = natts;\n\trel->rd_rel->reltype = InvalidOid;\n\t/* needed when bootstrapping: */\n\trel->rd_rel->relowner = BOOTSTRAP_SUPERUSERID;\n\n\t/* set up persistence and relcache fields dependent on it */\n\trel->rd_rel->relpersistence = relpersistence;\n\tswitch (relpersistence)\n\t{\n\t\tcase RELPERSISTENCE_UNLOGGED:\n\t\tcase RELPERSISTENCE_PERMANENT:\n\t\t\trel->rd_backend = InvalidBackendId;\n\t\t\trel->rd_islocaltemp = false;\n\t\t\tbreak;\n\t\tcase RELPERSISTENCE_TEMP:\n\t\t\tAssert(isTempOrToastNamespace(relnamespace));\n\t\t\trel->rd_backend = TempRelBackendId;\n\t\t\trel->rd_islocaltemp = true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\telog(ERROR, \"invalid relpersistence: %c\", relpersistence);\n\t\t\tbreak;\n\t}\n\n\t/* if it's a materialized view, it's not populated initially */\n\tif (relkind == RELKIND_MATVIEW)\n\t\trel->rd_rel->relispopulated = false;\n\telse\n\t\trel->rd_rel->relispopulated = true;\n\n\t/* system relations and non-table objects don't have one */\n\tif (!IsSystemNamespace(relnamespace) &&\n\t\t(relkind == RELKIND_RELATION || relkind == RELKIND_MATVIEW))\n\t\trel->rd_rel->relreplident = REPLICA_IDENTITY_DEFAULT;\n\telse\n\t\trel->rd_rel->relreplident = REPLICA_IDENTITY_NOTHING;\n\n\t/*\n\t * Insert relation physical and logical identifiers (OIDs) into the right\n\t * places.  For a mapped relation, we set relfilenode to zero and rely on\n\t * RelationInitPhysicalAddr to consult the map.\n\t *\n\t * In GPDB, the table's logical OID is allocated in the master, and might\n\t * already be in use as a relfilenode of an existing relation in a segment.\n\t */\n\trel->rd_rel->relisshared = shared_relation;\n\n\tRelationGetRelid(rel) = relid;\n\n\tfor (i = 0; i < natts; i++)\n\t\trel->rd_att->attrs[i]->attrelid = relid;\n\n\trel->rd_rel->reltablespace = reltablespace;\n\n\n\t/*\n\t * Further deviation in Greenplum: A new relfilenode must be generated even\n\t * for a mapped relation.  OIDs and relfilenodes are generated using two\n\t * separate counters.  If OID is reused as relfilenode, like in upstream,\n\t * without bumping the relfilenode counter, it may lead to a reuse of this\n\t * value as relfilenode in future.  E.g. if this is a non-temp relation and\n\t * the future relation happens to be a temp relation.  Shared buffer\n\t * manager in Greenplum breaks if this happens, see GPDB_91_MERGE_FIXME in\n\t * GetNewRelFileNode() for details.\n\t */\n\tif (relid < FirstNormalObjectId) /* bootstrap only */\n\t\trel->rd_rel->relfilenode = relid;\n\telse if (OidIsValid(relfilenode))\n\t\trel->rd_rel->relfilenode = relfilenode;\n\telse\n\t{\n\t\trel->rd_rel->relfilenode = GetNewRelFileNode(reltablespace, NULL, relpersistence);\n\t\tif (Gp_role == GP_ROLE_EXECUTE || IsBinaryUpgrade)\n\t\t\tAdvanceObjectId(relid);\n\t}\n\n\tif (mapped_relation)\n\t{\n\t\t/* Add it to the active mapping information */\n\t\tRelationMapUpdateMap(relid, rel->rd_rel->relfilenode, shared_relation, true);\n\t\trel->rd_rel->relfilenode = InvalidOid;\n\t}\n\n\tRelationInitLockInfo(rel);\t/* see lmgr.c */\n\n\tRelationInitPhysicalAddr(rel);\n\n\t/*\n\t * Okay to insert into the relcache hash table.\n\t *\n\t * Ordinarily, there should certainly not be an existing hash entry for\n\t * the same OID; but during bootstrap, when we create a \"real\" relcache\n\t * entry for one of the bootstrap relations, we'll be overwriting the\n\t * phony one created with formrdesc.  So allow that to happen for nailed\n\t * rels.\n\t */\n\tRelationCacheInsert(rel, nailit);\n\n\t/*\n\t * Flag relation as needing eoxact cleanup (to clear rd_createSubid). We\n\t * can't do this before storing relid in it.\n\t */\n\tEOXactListAdd(rel);\n\n\t/*\n\t * done building relcache entry.\n\t */\n\tMemoryContextSwitchTo(oldcxt);\n\n\t/* It's fully valid */\n\trel->rd_isvalid = true;\n\n\t/*\n\t * Caller expects us to pin the returned entry.\n\t */\n\tRelationIncrementReferenceCount(rel);\n\n\treturn rel;\n}\n\n\n/*\n * RelationSetNewRelfilenode\n *\n * Assign a new relfilenode (physical file name) to the relation.\n *\n * This allows a full rewrite of the relation to be done with transactional\n * safety (since the filenode assignment can be rolled back).  Note however\n * that there is no simple way to access the relation's old data for the\n * remainder of the current transaction.  This limits the usefulness to cases\n * such as TRUNCATE or rebuilding an index from scratch.\n *\n * Caller must already hold exclusive lock on the relation.\n *\n * The relation is marked with relfrozenxid = freezeXid (InvalidTransactionId\n * must be passed for indexes and sequences).  This should be a lower bound on\n * the XIDs that will be put into the new relation contents.\n */\nvoid\nRelationSetNewRelfilenode(Relation relation, TransactionId freezeXid,\n\t\t\t\t\t\t  MultiXactId minmulti)\n{\n\tOid\t\t\tnewrelfilenode;\n\tRelFileNodeBackend newrnode;\n\tRelation\tpg_class;\n\tHeapTuple\ttuple;\n\tForm_pg_class classform;\n\n\t/* Indexes, sequences must have Invalid frozenxid; other rels must not */\n\tAssert((relation->rd_rel->relkind == RELKIND_INDEX ||\n\t\t\trelation->rd_rel->relkind == RELKIND_SEQUENCE) ?\n\t\t   freezeXid == InvalidTransactionId :\n\t\t   TransactionIdIsNormal(freezeXid));\n\tAssert(TransactionIdIsNormal(freezeXid) == MultiXactIdIsValid(minmulti));\n\n\t/* Allocate a new relfilenode */\n\tnewrelfilenode = GetNewRelFileNode(relation->rd_rel->reltablespace, NULL,\n\t\t\t\t\t\t\t\t\t   relation->rd_rel->relpersistence);\n\n\t/*\n\t * Get a writable copy of the pg_class tuple for the given relation.\n\t */\n\tpg_class = heap_open(RelationRelationId, RowExclusiveLock);\n\n\ttuple = SearchSysCacheCopy1(RELOID,\n\t\t\t\t\t\t\t\tObjectIdGetDatum(RelationGetRelid(relation)));\n\tif (!HeapTupleIsValid(tuple))\n\t\telog(ERROR, \"could not find tuple for relation %u\",\n\t\t\t RelationGetRelid(relation));\n\tclassform = (Form_pg_class) GETSTRUCT(tuple);\n\n\t/*\n\t * Create storage for the main fork of the new relfilenode.\n\t *\n\t * NOTE: any conflict in relfilenode value will be caught here, if\n\t * GetNewRelFileNode messes up for any reason.\n\t */\n\tnewrnode.node = relation->rd_node;\n\tnewrnode.node.relNode = newrelfilenode;\n\tnewrnode.backend = relation->rd_backend;\n\tRelationCreateStorage(newrnode.node, relation->rd_rel->relpersistence,\n\t\t\t\t\t\t  relation->rd_rel->relstorage);\n\tsmgrclosenode(newrnode);\n\n\t/*\n\t * Schedule unlinking of the old storage at transaction commit.\n\t */\n\tRelationDropStorage(relation);\n\n\t/*\n\t * If we're dealing with a mapped index, pg_class.relfilenode doesn't\n\t * change; instead we have to send the update to the relation mapper.\n\t *\n\t * For mapped indexes, we don't actually change the pg_class entry at all;\n\t * this is essential when reindexing pg_class itself.  That leaves us with\n\t * possibly-inaccurate values of relpages etc, but those will be fixed up\n\t * later.\n\t */\n\tif (RelationIsMapped(relation))\n\t{\n\t\t/* This case is only supported for indexes */\n\t\tAssert(relation->rd_rel->relkind == RELKIND_INDEX);\n\n\t\t/* Since we're not updating pg_class, these had better not change */\n\t\tAssert(classform->relfrozenxid == freezeXid);\n\t\tAssert(classform->relminmxid == minmulti);\n\n\t\t/*\n\t\t * In some code paths it's possible that the tuple update we'd\n\t\t * otherwise do here is the only thing that would assign an XID for\n\t\t * the current transaction.  However, we must have an XID to delete\n\t\t * files, so make sure one is assigned.\n\t\t */\n\t\t(void) GetCurrentTransactionId();\n\n\t\t/* Do the deed */\n\t\tRelationMapUpdateMap(RelationGetRelid(relation),\n\t\t\t\t\t\t\t newrelfilenode,\n\t\t\t\t\t\t\t relation->rd_rel->relisshared,\n\t\t\t\t\t\t\t false);\n\n\t\t/* Since we're not updating pg_class, must trigger inval manually */\n\t\tCacheInvalidateRelcache(relation);\n\t}\n\telse\n\t{\n\t\t/* Normal case, update the pg_class entry */\n\t\tclassform->relfilenode = newrelfilenode;\n\n\t\t/* relpages etc. never change for sequences */\n\t\tif (relation->rd_rel->relkind != RELKIND_SEQUENCE)\n\t\t{\n\t\t\tclassform->relpages = 0;\t/* it's empty until further notice */\n\t\t\tclassform->reltuples = 0;\n\t\t\tclassform->relallvisible = 0;\n\t\t}\n\n\t\tif (TransactionIdIsValid(classform->relfrozenxid))\n\t\t{\n\t\t\tAssert(TransactionIdIsNormal(freezeXid));\n\t\t\tclassform->relfrozenxid = freezeXid;\n\t\t\t/*\n\t\t\t * Don't know partition parent or not here but passing false is\n\t\t\t * perfect for assertion, as valid relfrozenxid means it shouldn't\n\t\t\t * be parent.\n\t\t\t */\n\t\t\tAssert(should_have_valid_relfrozenxid(classform->relkind,\n\t\t\t\t\t\t\t\t\t\t\t\t  classform->relstorage, false));\n\t\t}\n\t\tclassform->relminmxid = minmulti;\n\n\t\tCatalogTupleUpdate(pg_class, &tuple->t_self, tuple);\n\t}\n\n\theap_freetuple(tuple);\n\n\theap_close(pg_class, RowExclusiveLock);\n\n\t/*\n\t * Make the pg_class row change or relation map change visible.  This will\n\t * cause the relcache entry to get updated, too.\n\t */\n\tCommandCounterIncrement();\n\n\t/*\n\t * Mark the rel as having been given a new relfilenode in the current\n\t * (sub) transaction.  This is a hint that can be used to optimize later\n\t * operations on the rel in the same transaction.\n\t */\n\trelation->rd_newRelfilenodeSubid = GetCurrentSubTransactionId();\n\n\t/* Flag relation as needing eoxact cleanup (to remove the hint) */\n\tEOXactListAdd(relation);\n}\n\n\n/*\n *\t\tRelationCacheInitialize\n *\n *\t\tThis initializes the relation descriptor cache.  At the time\n *\t\tthat this is invoked, we can't do database access yet (mainly\n *\t\tbecause the transaction subsystem is not up); all we are doing\n *\t\tis making an empty cache hashtable.  This must be done before\n *\t\tstarting the initialization transaction, because otherwise\n *\t\tAtEOXact_RelationCache would crash if that transaction aborts\n *\t\tbefore we can get the relcache set up.\n */\n\n#define INITRELCACHESIZE\t\t400\n\nvoid\nRelationCacheInitialize(void)\n{\n\tHASHCTL\t\tctl;\n\n\t/*\n\t * make sure cache memory context exists\n\t */\n\tif (!CacheMemoryContext)\n\t\tCreateCacheMemoryContext();\n\n\t/*\n\t * create hashtable that indexes the relcache\n\t */\n\tMemSet(&ctl, 0, sizeof(ctl));\n\tctl.keysize = sizeof(Oid);\n\tctl.entrysize = sizeof(RelIdCacheEnt);\n\tctl.hash = oid_hash;\n\tRelationIdCache = hash_create(\"Relcache by OID\", INITRELCACHESIZE,\n\t\t\t\t\t\t\t\t  &ctl, HASH_ELEM | HASH_FUNCTION);\n\n\t/*\n\t * relation mapper needs to be initialized too\n\t */\n\tRelationMapInitialize();\n}\n\n/*\n *\t\tRelationCacheInitializePhase2\n *\n *\t\tThis is called to prepare for access to shared catalogs during startup.\n *\t\tWe must at least set up nailed reldescs for pg_database, pg_authid,\n *\t\tand pg_auth_members.  Ideally we'd like to have reldescs for their\n *\t\tindexes, too.  We attempt to load this information from the shared\n *\t\trelcache init file.  If that's missing or broken, just make phony\n *\t\tentries for the catalogs themselves.  RelationCacheInitializePhase3\n *\t\twill clean up as needed.\n */\nvoid\nRelationCacheInitializePhase2(void)\n{\n\tMemoryContext oldcxt;\n\n\t/*\n\t * relation mapper needs initialized too\n\t */\n\tRelationMapInitializePhase2();\n\n\t/*\n\t * In bootstrap mode, the shared catalogs aren't there yet anyway, so do\n\t * nothing.\n\t */\n\tif (IsBootstrapProcessingMode())\n\t\treturn;\n\n\t/*\n\t * switch to cache memory context\n\t */\n\toldcxt = MemoryContextSwitchTo(CacheMemoryContext);\n\n\t/*\n\t * Try to load the shared relcache cache file.  If unsuccessful, bootstrap\n\t * the cache with pre-made descriptors for the critical shared catalogs.\n\t */\n\tif (!load_relcache_init_file(true))\n\t{\n\t\tformrdesc(\"pg_database\", DatabaseRelation_Rowtype_Id, true,\n\t\t\t\t  true, Natts_pg_database, Desc_pg_database);\n\t\tformrdesc(\"pg_authid\", AuthIdRelation_Rowtype_Id, true,\n\t\t\t\t  true, Natts_pg_authid, Desc_pg_authid);\n\t\tformrdesc(\"pg_auth_members\", AuthMemRelation_Rowtype_Id, true,\n\t\t\t\t  false, Natts_pg_auth_members, Desc_pg_auth_members);\n\t\tformrdesc(\"pg_auth_time_constraint\", AuthTimeConstraint_Rowtype_Id, true,\n\t\t\t\t  false, Natts_pg_auth_time_constraint, Desc_pg_auth_time_constraint_members);\n\n#define NUM_CRITICAL_SHARED_RELS\t4\t/* fix if you change list above */\n\t}\n\n\tMemoryContextSwitchTo(oldcxt);\n}\n\n/*\n *\t\tRelationCacheInitializePhase3\n *\n *\t\tThis is called as soon as the catcache and transaction system\n *\t\tare functional and we have determined MyDatabaseId.  At this point\n *\t\twe can actually read data from the database's system catalogs.\n *\t\tWe first try to read pre-computed relcache entries from the local\n *\t\trelcache init file.  If that's missing or broken, make phony entries\n *\t\tfor the minimum set of nailed-in-cache relations.  Then (unless\n *\t\tbootstrapping) make sure we have entries for the critical system\n *\t\tindexes.  Once we've done all this, we have enough infrastructure to\n *\t\topen any system catalog or use any catcache.  The last step is to\n *\t\trewrite the cache files if needed.\n */\nvoid\nRelationCacheInitializePhase3(void)\n{\n\tHASH_SEQ_STATUS status;\n\tRelIdCacheEnt *idhentry;\n\tMemoryContext oldcxt;\n\tbool\t\tneedNewCacheFile = !criticalSharedRelcachesBuilt;\n\n\t/*\n\t * Relation cache initialization or any sort of heap access is\n\t * dangerous before recovery is finished.\n\t */\n\tif (!EnableHotStandby && !IsBootstrapProcessingMode() && RecoveryInProgress())\n\t\telog(ERROR, \"relation cache initialization during recovery or non-bootstrap processes.\");\n\n\t/*\n\t * relation mapper needs initialized too\n\t */\n\tRelationMapInitializePhase3();\n\n\t/*\n\t * switch to cache memory context\n\t */\n\toldcxt = MemoryContextSwitchTo(CacheMemoryContext);\n\n\t/*\n\t * Try to load the local relcache cache file.  If unsuccessful, bootstrap\n\t * the cache with pre-made descriptors for the critical \"nailed-in\" system\n\t * catalogs.\n\t */\n\tif (IsBootstrapProcessingMode() ||\n\t\t!load_relcache_init_file(false))\n\t{\n\t\tneedNewCacheFile = true;\n\n\t\tformrdesc(\"pg_class\", RelationRelation_Rowtype_Id, false,\n\t\t\t\t  true, Natts_pg_class, Desc_pg_class);\n\t\tformrdesc(\"pg_attribute\", AttributeRelation_Rowtype_Id, false,\n\t\t\t\t  false, Natts_pg_attribute, Desc_pg_attribute);\n\t\tformrdesc(\"pg_proc\", ProcedureRelation_Rowtype_Id, false,\n\t\t\t\t  true, Natts_pg_proc, Desc_pg_proc);\n\t\tformrdesc(\"pg_type\", TypeRelation_Rowtype_Id, false,\n\t\t\t\t  true, Natts_pg_type, Desc_pg_type);\n\n#define NUM_CRITICAL_LOCAL_RELS 4\t\t/* fix if you change list above */\n\t}\n\n\tMemoryContextSwitchTo(oldcxt);\n\n\t/* In bootstrap mode, the faked-up formrdesc info is all we'll have */\n\tif (IsBootstrapProcessingMode())\n\t\treturn;\n\n\t/*\n\t * If we didn't get the critical system indexes loaded into relcache, do\n\t * so now.  These are critical because the catcache and/or opclass cache\n\t * depend on them for fetches done during relcache load.  Thus, we have an\n\t * infinite-recursion problem.  We can break the recursion by doing\n\t * heapscans instead of indexscans at certain key spots. To avoid hobbling\n\t * performance, we only want to do that until we have the critical indexes\n\t * loaded into relcache.  Thus, the flag criticalRelcachesBuilt is used to\n\t * decide whether to do heapscan or indexscan at the key spots, and we set\n\t * it true after we've loaded the critical indexes.\n\t *\n\t * The critical indexes are marked as \"nailed in cache\", partly to make it\n\t * easy for load_relcache_init_file to count them, but mainly because we\n\t * cannot flush and rebuild them once we've set criticalRelcachesBuilt to\n\t * true.  (NOTE: perhaps it would be possible to reload them by\n\t * temporarily setting criticalRelcachesBuilt to false again.  For now,\n\t * though, we just nail 'em in.)\n\t *\n\t * RewriteRelRulenameIndexId and TriggerRelidNameIndexId are not critical\n\t * in the same way as the others, because the critical catalogs don't\n\t * (currently) have any rules or triggers, and so these indexes can be\n\t * rebuilt without inducing recursion.  However they are used during\n\t * relcache load when a rel does have rules or triggers, so we choose to\n\t * nail them for performance reasons.\n\t */\n\tif (!criticalRelcachesBuilt)\n\t{\n\t\tload_critical_index(ClassOidIndexId,\n\t\t\t\t\t\t\tRelationRelationId);\n\t\tload_critical_index(AttributeRelidNumIndexId,\n\t\t\t\t\t\t\tAttributeRelationId);\n\t\tload_critical_index(IndexRelidIndexId,\n\t\t\t\t\t\t\tIndexRelationId);\n\t\tload_critical_index(OpclassOidIndexId,\n\t\t\t\t\t\t\tOperatorClassRelationId);\n\t\tload_critical_index(AccessMethodProcedureIndexId,\n\t\t\t\t\t\t\tAccessMethodProcedureRelationId);\n\t\tload_critical_index(RewriteRelRulenameIndexId,\n\t\t\t\t\t\t\tRewriteRelationId);\n\t\tload_critical_index(TriggerRelidNameIndexId,\n\t\t\t\t\t\t\tTriggerRelationId);\n\n#define NUM_CRITICAL_LOCAL_INDEXES\t7\t/* fix if you change list above */\n\n\t\tcriticalRelcachesBuilt = true;\n\t}\n\n\t/*\n\t * Process critical shared indexes too.\n\t *\n\t * DatabaseNameIndexId isn't critical for relcache loading, but rather for\n\t * initial lookup of MyDatabaseId, without which we'll never find any\n\t * non-shared catalogs at all.  Autovacuum calls InitPostgres with a\n\t * database OID, so it instead depends on DatabaseOidIndexId.  We also\n\t * need to nail up some indexes on pg_authid and pg_auth_members for use\n\t * during client authentication.\n\t *\n\t * GPDB: pg_auth_time_constraint is added to the above list.\n\t */\n\tif (!criticalSharedRelcachesBuilt)\n\t{\n\t\tload_critical_index(DatabaseNameIndexId,\n\t\t\t\t\t\t\tDatabaseRelationId);\n\t\tload_critical_index(DatabaseOidIndexId,\n\t\t\t\t\t\t\tDatabaseRelationId);\n\t\tload_critical_index(AuthIdRolnameIndexId,\n\t\t\t\t\t\t\tAuthIdRelationId);\n\t\tload_critical_index(AuthIdOidIndexId,\n\t\t\t\t\t\t\tAuthIdRelationId);\n\t\tload_critical_index(AuthMemMemRoleIndexId,\n\t\t\t\t\t\t\tAuthMemRelationId);\n\t\tload_critical_index(AuthTimeConstraintAuthIdIndexId,\n\t\t\t\t\t\t\tAuthTimeConstraintRelationId);\n\n#define NUM_CRITICAL_SHARED_INDEXES 6\t/* fix if you change list above */\n\n\t\tcriticalSharedRelcachesBuilt = true;\n\t}\n\n\t/*\n\t * Now, scan all the relcache entries and update anything that might be\n\t * wrong in the results from formrdesc or the relcache cache file. If we\n\t * faked up relcache entries using formrdesc, then read the real pg_class\n\t * rows and replace the fake entries with them. Also, if any of the\n\t * relcache entries have rules or triggers, load that info the hard way\n\t * since it isn't recorded in the cache file.\n\t *\n\t * Whenever we access the catalogs to read data, there is a possibility of\n\t * a shared-inval cache flush causing relcache entries to be removed.\n\t * Since hash_seq_search only guarantees to still work after the *current*\n\t * entry is removed, it's unsafe to continue the hashtable scan afterward.\n\t * We handle this by restarting the scan from scratch after each access.\n\t * This is theoretically O(N^2), but the number of entries that actually\n\t * need to be fixed is small enough that it doesn't matter.\n\t */\n\thash_seq_init(&status, RelationIdCache);\n\n\twhile ((idhentry = (RelIdCacheEnt *) hash_seq_search(&status)) != NULL)\n\t{\n\t\tRelation\trelation = idhentry->reldesc;\n\t\tbool\t\trestart = false;\n\n\t\t/*\n\t\t * Make sure *this* entry doesn't get flushed while we work with it.\n\t\t */\n\t\tRelationIncrementReferenceCount(relation);\n\n\t\t/*\n\t\t * If it's a faked-up entry, read the real pg_class tuple.\n\t\t */\n\t\tif (relation->rd_rel->relowner == InvalidOid)\n\t\t{\n\t\t\tHeapTuple\thtup;\n\t\t\tForm_pg_class relp;\n\n\t\t\thtup = SearchSysCache1(RELOID,\n\t\t\t\t\t\t\t   ObjectIdGetDatum(RelationGetRelid(relation)));\n\t\t\tif (!HeapTupleIsValid(htup))\n\t\t\t\telog(FATAL, \"cache lookup failed for relation %u\",\n\t\t\t\t\t RelationGetRelid(relation));\n\t\t\trelp = (Form_pg_class) GETSTRUCT(htup);\n\n\t\t\t/*\n\t\t\t * Copy tuple to relation->rd_rel. (See notes in\n\t\t\t * AllocateRelationDesc())\n\t\t\t */\n\t\t\tmemcpy((char *) relation->rd_rel, (char *) relp, CLASS_TUPLE_SIZE);\n\n\t\t\t/* Update rd_options while we have the tuple */\n\t\t\tif (relation->rd_options)\n\t\t\t\tpfree(relation->rd_options);\n\t\t\tRelationParseRelOptions(relation, htup);\n\n\t\t\t/*\n\t\t\t * Check the values in rd_att were set up correctly.  (We cannot\n\t\t\t * just copy them over now: formrdesc must have set up the rd_att\n\t\t\t * data correctly to start with, because it may already have been\n\t\t\t * copied into one or more catcache entries.)\n\t\t\t */\n\t\t\tAssert(relation->rd_att->tdtypeid == relp->reltype);\n\t\t\tAssert(relation->rd_att->tdtypmod == -1);\n\t\t\tAssert(relation->rd_att->tdhasoid == relp->relhasoids);\n\n\t\t\tReleaseSysCache(htup);\n\n\t\t\t/* relowner had better be OK now, else we'll loop forever */\n\t\t\tif (relation->rd_rel->relowner == InvalidOid)\n\t\t\t\telog(ERROR, \"invalid relowner in pg_class entry for \\\"%s\\\"\",\n\t\t\t\t\t RelationGetRelationName(relation));\n\n\t\t\trestart = true;\n\t\t}\n\n\t\t/*\n\t\t * Fix data that isn't saved in relcache cache file.\n\t\t *\n\t\t * relhasrules or relhastriggers could possibly be wrong or out of\n\t\t * date.  If we don't actually find any rules or triggers, clear the\n\t\t * local copy of the flag so that we don't get into an infinite loop\n\t\t * here.  We don't make any attempt to fix the pg_class entry, though.\n\t\t */\n\t\tif (relation->rd_rel->relhasrules && relation->rd_rules == NULL)\n\t\t{\n\t\t\tRelationBuildRuleLock(relation);\n\t\t\tif (relation->rd_rules == NULL)\n\t\t\t\trelation->rd_rel->relhasrules = false;\n\t\t\trestart = true;\n\t\t}\n\t\tif (relation->rd_rel->relhastriggers && relation->trigdesc == NULL)\n\t\t{\n\t\t\tRelationBuildTriggers(relation);\n\t\t\tif (relation->trigdesc == NULL)\n\t\t\t\trelation->rd_rel->relhastriggers = false;\n\t\t\trestart = true;\n\t\t}\n\n\t\t/* Release hold on the relation */\n\t\tRelationDecrementReferenceCount(relation);\n\n\t\t/* Now, restart the hashtable scan if needed */\n\t\tif (restart)\n\t\t{\n\t\t\thash_seq_term(&status);\n\t\t\thash_seq_init(&status, RelationIdCache);\n\t\t}\n\t}\n\n\t/*\n\t * Lastly, write out new relcache cache files if needed.  We don't bother\n\t * to distinguish cases where only one of the two needs an update.\n\t */\n\tif (needNewCacheFile)\n\t{\n\t\t/*\n\t\t * Force all the catcaches to finish initializing and thereby open the\n\t\t * catalogs and indexes they use.  This will preload the relcache with\n\t\t * entries for all the most important system catalogs and indexes, so\n\t\t * that the init files will be most useful for future backends.\n\t\t */\n\t\tInitCatalogCachePhase2();\n\n\t\t/* now write the files */\n\t\twrite_relcache_init_file(true);\n\t\twrite_relcache_init_file(false);\n\t}\n}\n\n/*\n * Load one critical system index into the relcache\n *\n * indexoid is the OID of the target index, heapoid is the OID of the catalog\n * it belongs to.\n */\nstatic void\nload_critical_index(Oid indexoid, Oid heapoid)\n{\n\tRelation\tird;\n\n\t/*\n\t * We must lock the underlying catalog before locking the index to avoid\n\t * deadlock, since RelationBuildDesc might well need to read the catalog,\n\t * and if anyone else is exclusive-locking this catalog and index they'll\n\t * be doing it in that order.\n\t */\n\tLockRelationOid(heapoid, AccessShareLock);\n\tLockRelationOid(indexoid, AccessShareLock);\n\tird = RelationBuildDesc(indexoid, true);\n\tif (ird == NULL)\n\t\telog(PANIC, \"could not open critical system index %u\", indexoid);\n\tird->rd_isnailed = true;\n\tird->rd_refcnt = 1;\n\tUnlockRelationOid(indexoid, AccessShareLock);\n\tUnlockRelationOid(heapoid, AccessShareLock);\n}\n\n/*\n * GetPgClassDescriptor -- get a predefined tuple descriptor for pg_class\n * GetPgIndexDescriptor -- get a predefined tuple descriptor for pg_index\n *\n * We need this kluge because we have to be able to access non-fixed-width\n * fields of pg_class and pg_index before we have the standard catalog caches\n * available.  We use predefined data that's set up in just the same way as\n * the bootstrapped reldescs used by formrdesc().  The resulting tupdesc is\n * not 100% kosher: it does not have the correct rowtype OID in tdtypeid, nor\n * does it have a TupleConstr field.  But it's good enough for the purpose of\n * extracting fields.\n */\nstatic TupleDesc\nBuildHardcodedDescriptor(int natts, const FormData_pg_attribute *attrs,\n\t\t\t\t\t\t bool hasoids)\n{\n\tTupleDesc\tresult;\n\tMemoryContext oldcxt;\n\tint\t\t\ti;\n\n\toldcxt = MemoryContextSwitchTo(CacheMemoryContext);\n\n\tresult = CreateTemplateTupleDesc(natts, hasoids);\n\tresult->tdtypeid = RECORDOID;\t\t/* not right, but we don't care */\n\tresult->tdtypmod = -1;\n\n\tfor (i = 0; i < natts; i++)\n\t{\n\t\tmemcpy(result->attrs[i], &attrs[i], ATTRIBUTE_FIXED_PART_SIZE);\n\t\t/* make sure attcacheoff is valid */\n\t\tresult->attrs[i]->attcacheoff = -1;\n\t}\n\n\t/* initialize first attribute's attcacheoff, cf RelationBuildTupleDesc */\n\tresult->attrs[0]->attcacheoff = 0;\n\n\t/* Note: we don't bother to set up a TupleConstr entry */\n\n\tMemoryContextSwitchTo(oldcxt);\n\n\treturn result;\n}\n\nstatic TupleDesc\nGetPgClassDescriptor(void)\n{\n\tstatic TupleDesc pgclassdesc = NULL;\n\n\t/* Already done? */\n\tif (pgclassdesc == NULL)\n\t\tpgclassdesc = BuildHardcodedDescriptor(Natts_pg_class,\n\t\t\t\t\t\t\t\t\t\t\t   Desc_pg_class,\n\t\t\t\t\t\t\t\t\t\t\t   true);\n\n\treturn pgclassdesc;\n}\n\nstatic TupleDesc\nGetPgIndexDescriptor(void)\n{\n\tstatic TupleDesc pgindexdesc = NULL;\n\n\t/* Already done? */\n\tif (pgindexdesc == NULL)\n\t\tpgindexdesc = BuildHardcodedDescriptor(Natts_pg_index,\n\t\t\t\t\t\t\t\t\t\t\t   Desc_pg_index,\n\t\t\t\t\t\t\t\t\t\t\t   false);\n\n\treturn pgindexdesc;\n}\n\n/*\n * Load any default attribute value definitions for the relation.\n */\nstatic void\nAttrDefaultFetch(Relation relation)\n{\n\tAttrDefault *attrdef = relation->rd_att->constr->defval;\n\tint\t\t\tndef = relation->rd_att->constr->num_defval;\n\tRelation\tadrel;\n\tSysScanDesc adscan;\n\tScanKeyData skey;\n\tHeapTuple\thtup;\n\tDatum\t\tval;\n\tbool\t\tisnull;\n\tint\t\t\tfound;\n\tint\t\t\ti;\n\n\tScanKeyInit(&skey,\n\t\t\t\tAnum_pg_attrdef_adrelid,\n\t\t\t\tBTEqualStrategyNumber, F_OIDEQ,\n\t\t\t\tObjectIdGetDatum(RelationGetRelid(relation)));\n\n\tadrel = heap_open(AttrDefaultRelationId, AccessShareLock);\n\tadscan = systable_beginscan(adrel, AttrDefaultIndexId, true,\n\t\t\t\t\t\t\t\tNULL, 1, &skey);\n\tfound = 0;\n\n\twhile (HeapTupleIsValid(htup = systable_getnext(adscan)))\n\t{\n\t\tForm_pg_attrdef adform = (Form_pg_attrdef) GETSTRUCT(htup);\n\n\t\tfor (i = 0; i < ndef; i++)\n\t\t{\n\t\t\tif (adform->adnum != attrdef[i].adnum)\n\t\t\t\tcontinue;\n\t\t\tif (attrdef[i].adbin != NULL)\n\t\t\t\telog(WARNING, \"multiple attrdef records found for attr %s of rel %s\",\n\t\t\t\tNameStr(relation->rd_att->attrs[adform->adnum - 1]->attname),\n\t\t\t\t\t RelationGetRelationName(relation));\n\t\t\telse\n\t\t\t\tfound++;\n\n\t\t\tval = fastgetattr(htup,\n\t\t\t\t\t\t\t  Anum_pg_attrdef_adbin,\n\t\t\t\t\t\t\t  adrel->rd_att, &isnull);\n\t\t\tif (isnull)\n\t\t\t\telog(WARNING, \"null adbin for attr %s of rel %s\",\n\t\t\t\tNameStr(relation->rd_att->attrs[adform->adnum - 1]->attname),\n\t\t\t\t\t RelationGetRelationName(relation));\n\t\t\telse\n\t\t\t{\n\t\t\t\t/* detoast and convert to cstring in caller's context */\n\t\t\t\tchar\t   *s = TextDatumGetCString(val);\n\n\t\t\t\tattrdef[i].adbin = MemoryContextStrdup(CacheMemoryContext, s);\n\t\t\t\tpfree(s);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tif (i >= ndef)\n\t\t\telog(WARNING, \"unexpected attrdef record found for attr %d of rel %s\",\n\t\t\t\t adform->adnum, RelationGetRelationName(relation));\n\t}\n\n\tsystable_endscan(adscan);\n\theap_close(adrel, AccessShareLock);\n\n\tif (found != ndef)\n\t\telog(WARNING, \"%d attrdef record(s) missing for rel %s\",\n\t\t\t ndef - found, RelationGetRelationName(relation));\n}\n\n/*\n * Load any check constraints for the relation.\n */\nstatic void\nCheckConstraintFetch(Relation relation)\n{\n\tConstrCheck *check = relation->rd_att->constr->check;\n\tint\t\t\tncheck = relation->rd_att->constr->num_check;\n\tRelation\tconrel;\n\tSysScanDesc conscan;\n\tScanKeyData skey[1];\n\tHeapTuple\thtup;\n\tDatum\t\tval;\n\tbool\t\tisnull;\n\tint\t\t\tfound = 0;\n\n\tScanKeyInit(&skey[0],\n\t\t\t\tAnum_pg_constraint_conrelid,\n\t\t\t\tBTEqualStrategyNumber, F_OIDEQ,\n\t\t\t\tObjectIdGetDatum(RelationGetRelid(relation)));\n\n\tconrel = heap_open(ConstraintRelationId, AccessShareLock);\n\tconscan = systable_beginscan(conrel, ConstraintRelidIndexId, true,\n\t\t\t\t\t\t\t\t NULL, 1, skey);\n\n\twhile (HeapTupleIsValid(htup = systable_getnext(conscan)))\n\t{\n\t\tForm_pg_constraint conform = (Form_pg_constraint) GETSTRUCT(htup);\n\n\t\t/* We want check constraints only */\n\t\tif (conform->contype != CONSTRAINT_CHECK)\n\t\t\tcontinue;\n\n\t\tif (found >= ncheck)\n\t\t\telog(ERROR,\n\t\t\t     \"pg_class reports %d constraint record(s) for rel %s, but found extra in pg_constraint\",\n\t\t\t     ncheck, RelationGetRelationName(relation));\n\n\t\tcheck[found].ccvalid = conform->convalidated;\n\t\tcheck[found].ccnoinherit = conform->connoinherit;\n\t\tcheck[found].ccname = MemoryContextStrdup(CacheMemoryContext,\n\t\t\t\t\t\t\t\t\t\t\t\t  NameStr(conform->conname));\n\n\t\t/* Grab and test conbin is actually set */\n\t\tval = fastgetattr(htup,\n\t\t\t\t\t\t  Anum_pg_constraint_conbin,\n\t\t\t\t\t\t  conrel->rd_att, &isnull);\n\t\tif (isnull)\n\t\t\telog(ERROR, \"null conbin for rel %s\",\n\t\t\t\t RelationGetRelationName(relation));\n\n\t\tcheck[found].ccbin = MemoryContextStrdup(CacheMemoryContext,\n\t\t\t\t\t\t\t\t\t\t\t\t TextDatumGetCString(val));\n\t\tfound++;\n\t}\n\n\tsystable_endscan(conscan);\n\theap_close(conrel, AccessShareLock);\n\n\tif (found != ncheck)\n\t\telog(ERROR,\n\t\t     \"found %d in pg_constraint, but pg_class reports %d constraint record(s) for rel %s\",\n\t\t     found, ncheck, RelationGetRelationName(relation));\n}\n\n\n/*\n * RelationGetPartitioningKey -- get GpPolicy struct for distributed relation\n *\n * Returns a copy of the relation's GpPolicy object, palloc'd in\n * the caller's context.  Caller should pfree() it.  If NULL is\n * returned, relation should be accessed locally.\n */\nGpPolicy*\nRelationGetPartitioningKey(Relation relation)\n{\n\treturn GpPolicyCopy(relation->rd_cdbpolicy);\n}                                       /* RelationGetPartitioningKey */\n\n\n/*\n * RelationGetIndexList -- get a list of OIDs of indexes on this relation\n *\n * The index list is created only if someone requests it.  We scan pg_index\n * to find relevant indexes, and add the list to the relcache entry so that\n * we won't have to compute it again.  Note that shared cache inval of a\n * relcache entry will delete the old list and set rd_indexvalid to 0,\n * so that we must recompute the index list on next request.  This handles\n * creation or deletion of an index.\n *\n * Indexes that are marked not IndexIsLive are omitted from the returned list.\n * Such indexes are expected to be dropped momentarily, and should not be\n * touched at all by any caller of this function.\n *\n * The returned list is guaranteed to be sorted in order by OID.  This is\n * needed by the executor, since for index types that we obtain exclusive\n * locks on when updating the index, all backends must lock the indexes in\n * the same order or we will get deadlocks (see ExecOpenIndices()).  Any\n * consistent ordering would do, but ordering by OID is easy.\n *\n * Since shared cache inval causes the relcache's copy of the list to go away,\n * we return a copy of the list palloc'd in the caller's context.  The caller\n * may list_free() the returned list after scanning it. This is necessary\n * since the caller will typically be doing syscache lookups on the relevant\n * indexes, and syscache lookup could cause SI messages to be processed!\n *\n * We also update rd_oidindex, which this module treats as effectively part\n * of the index list.  rd_oidindex is valid when rd_indexvalid isn't zero;\n * it is the pg_class OID of a unique index on OID when the relation has one,\n * and InvalidOid if there is no such index.\n *\n * In exactly the same way, we update rd_replidindex, which is the pg_class\n * OID of an index to be used as the relation's replication identity index,\n * or InvalidOid if there is no such index.\n */\nList *\nRelationGetIndexList(Relation relation)\n{\n\tRelation\tindrel;\n\tSysScanDesc indscan;\n\tScanKeyData skey;\n\tHeapTuple\thtup;\n\tList\t   *result;\n\tchar\t\treplident = relation->rd_rel->relreplident;\n\tOid\t\t\toidIndex = InvalidOid;\n\tOid\t\t\tpkeyIndex = InvalidOid;\n\tOid\t\t\tcandidateIndex = InvalidOid;\n\tMemoryContext oldcxt;\n\n\t/* Quick exit if we already computed the list. */\n\tif (relation->rd_indexvalid != 0)\n\t\treturn list_copy(relation->rd_indexlist);\n\n\t/*\n\t * We build the list we intend to return (in the caller's context) while\n\t * doing the scan.  After successfully completing the scan, we copy that\n\t * list into the relcache entry.  This avoids cache-context memory leakage\n\t * if we get some sort of error partway through.\n\t */\n\tresult = NIL;\n\toidIndex = InvalidOid;\n\n\t/* Prepare to scan pg_index for entries having indrelid = this rel. */\n\tScanKeyInit(&skey,\n\t\t\t\tAnum_pg_index_indrelid,\n\t\t\t\tBTEqualStrategyNumber, F_OIDEQ,\n\t\t\t\tObjectIdGetDatum(RelationGetRelid(relation)));\n\n\tindrel = heap_open(IndexRelationId, AccessShareLock);\n\tindscan = systable_beginscan(indrel, IndexIndrelidIndexId, true,\n\t\t\t\t\t\t\t\t NULL, 1, &skey);\n\n\twhile (HeapTupleIsValid(htup = systable_getnext(indscan)))\n\t{\n\t\tForm_pg_index index = (Form_pg_index) GETSTRUCT(htup);\n\t\tDatum\t\tindclassDatum;\n\t\toidvector  *indclass;\n\t\tbool\t\tisnull;\n\n\t\t/*\n\t\t * Ignore any indexes that are currently being dropped.  This will\n\t\t * prevent them from being searched, inserted into, or considered in\n\t\t * HOT-safety decisions.  It's unsafe to touch such an index at all\n\t\t * since its catalog entries could disappear at any instant.\n\t\t */\n\t\tif (!IndexIsLive(index))\n\t\t\tcontinue;\n\n\t\t/* Add index's OID to result list in the proper order */\n\t\tresult = insert_ordered_oid(result, index->indexrelid);\n\n\t\t/*\n\t\t * indclass cannot be referenced directly through the C struct,\n\t\t * because it comes after the variable-width indkey field.  Must\n\t\t * extract the datum the hard way...\n\t\t */\n\t\tindclassDatum = heap_getattr(htup,\n\t\t\t\t\t\t\t\t\t Anum_pg_index_indclass,\n\t\t\t\t\t\t\t\t\t GetPgIndexDescriptor(),\n\t\t\t\t\t\t\t\t\t &isnull);\n\t\tAssert(!isnull);\n\t\tindclass = (oidvector *) DatumGetPointer(indclassDatum);\n\n\t\t/*\n\t\t * Invalid, non-unique, non-immediate or predicate indexes aren't\n\t\t * interesting for either oid indexes or replication identity indexes,\n\t\t * so don't check them.\n\t\t */\n\t\tif (!IndexIsValid(index) || !index->indisunique ||\n\t\t\t!index->indimmediate ||\n\t\t\t!heap_attisnull(htup, Anum_pg_index_indpred))\n\t\t\tcontinue;\n\n\t\t/* Check to see if is a usable btree index on OID */\n\t\tif (index->indnatts == 1 &&\n\t\t\tindex->indkey.values[0] == ObjectIdAttributeNumber &&\n\t\t\tindclass->values[0] == OID_BTREE_OPS_OID)\n\t\t\toidIndex = index->indexrelid;\n\n\t\t/* remember primary key index if any */\n\t\tif (index->indisprimary)\n\t\t\tpkeyIndex = index->indexrelid;\n\n\t\t/* remember explicitly chosen replica index */\n\t\tif (index->indisreplident)\n\t\t\tcandidateIndex = index->indexrelid;\n\t}\n\n\tsystable_endscan(indscan);\n\n\theap_close(indrel, AccessShareLock);\n\n\t/* Now save a copy of the completed list in the relcache entry. */\n\toldcxt = MemoryContextSwitchTo(CacheMemoryContext);\n\trelation->rd_indexlist = list_copy(result);\n\trelation->rd_oidindex = oidIndex;\n\tif (replident == REPLICA_IDENTITY_DEFAULT && OidIsValid(pkeyIndex))\n\t\trelation->rd_replidindex = pkeyIndex;\n\telse if (replident == REPLICA_IDENTITY_INDEX && OidIsValid(candidateIndex))\n\t\trelation->rd_replidindex = candidateIndex;\n\telse\n\t\trelation->rd_replidindex = InvalidOid;\n\trelation->rd_indexvalid = 1;\n\tMemoryContextSwitchTo(oldcxt);\n\n\treturn result;\n}\n\n/*\n * insert_ordered_oid\n *\t\tInsert a new Oid into a sorted list of Oids, preserving ordering\n *\n * Building the ordered list this way is O(N^2), but with a pretty small\n * constant, so for the number of entries we expect it will probably be\n * faster than trying to apply qsort().  Most tables don't have very many\n * indexes...\n */\nstatic List *\ninsert_ordered_oid(List *list, Oid datum)\n{\n\tListCell   *prev;\n\n\t/* Does the datum belong at the front? */\n\tif (list == NIL || datum < linitial_oid(list))\n\t\treturn lcons_oid(datum, list);\n\t/* No, so find the entry it belongs after */\n\tprev = list_head(list);\n\tfor (;;)\n\t{\n\t\tListCell   *curr = lnext(prev);\n\n\t\tif (curr == NULL || datum < lfirst_oid(curr))\n\t\t\tbreak;\t\t\t\t/* it belongs after 'prev', before 'curr' */\n\n\t\tprev = curr;\n\t}\n\t/* Insert datum into list after 'prev' */\n\tlappend_cell_oid(list, prev, datum);\n\treturn list;\n}\n\n/*\n * RelationSetIndexList -- externally force the index list contents\n *\n * This is used to temporarily override what we think the set of valid\n * indexes is (including the presence or absence of an OID index).\n * The forcing will be valid only until transaction commit or abort.\n *\n * This should only be applied to nailed relations, because in a non-nailed\n * relation the hacked index list could be lost at any time due to SI\n * messages.  In practice it is only used on pg_class (see REINDEX).\n *\n * It is up to the caller to make sure the given list is correctly ordered.\n *\n * We deliberately do not change rd_indexattr here: even when operating\n * with a temporary partial index list, HOT-update decisions must be made\n * correctly with respect to the full index set.  It is up to the caller\n * to ensure that a correct rd_indexattr set has been cached before first\n * calling RelationSetIndexList; else a subsequent inquiry might cause a\n * wrong rd_indexattr set to get computed and cached.  Likewise, we do not\n * touch rd_keyattr or rd_idattr.\n */\nvoid\nRelationSetIndexList(Relation relation, List *indexIds, Oid oidIndex)\n{\n\tMemoryContext oldcxt;\n\n\tAssert(relation->rd_isnailed);\n\t/* Copy the list into the cache context (could fail for lack of mem) */\n\toldcxt = MemoryContextSwitchTo(CacheMemoryContext);\n\tindexIds = list_copy(indexIds);\n\tMemoryContextSwitchTo(oldcxt);\n\t/* Okay to replace old list */\n\tlist_free(relation->rd_indexlist);\n\trelation->rd_indexlist = indexIds;\n\trelation->rd_oidindex = oidIndex;\n\t/* For the moment, assume the target rel hasn't got a replica index */\n\trelation->rd_replidindex = InvalidOid;\n\trelation->rd_indexvalid = 2;\t/* mark list as forced */\n\t/* Flag relation as needing eoxact cleanup (to reset the list) */\n\tEOXactListAdd(relation);\n}\n\n/*\n * RelationGetOidIndex -- get the pg_class OID of the relation's OID index\n *\n * Returns InvalidOid if there is no such index.\n */\nOid\nRelationGetOidIndex(Relation relation)\n{\n\tList\t   *ilist;\n\n\t/*\n\t * If relation doesn't have OIDs at all, caller is probably confused. (We\n\t * could just silently return InvalidOid, but it seems better to throw an\n\t * assertion.)\n\t */\n\tAssert(relation->rd_rel->relhasoids);\n\n\tif (relation->rd_indexvalid == 0)\n\t{\n\t\t/* RelationGetIndexList does the heavy lifting. */\n\t\tilist = RelationGetIndexList(relation);\n\t\tlist_free(ilist);\n\t\tAssert(relation->rd_indexvalid != 0);\n\t}\n\n\treturn relation->rd_oidindex;\n}\n\n/*\n * RelationGetReplicaIndex -- get OID of the relation's replica identity index\n *\n * Returns InvalidOid if there is no such index.\n */\nOid\nRelationGetReplicaIndex(Relation relation)\n{\n\tList\t   *ilist;\n\n\tif (relation->rd_indexvalid == 0)\n\t{\n\t\t/* RelationGetIndexList does the heavy lifting. */\n\t\tilist = RelationGetIndexList(relation);\n\t\tlist_free(ilist);\n\t\tAssert(relation->rd_indexvalid != 0);\n\t}\n\n\treturn relation->rd_replidindex;\n}\n\n/*\n * RelationGetIndexExpressions -- get the index expressions for an index\n *\n * We cache the result of transforming pg_index.indexprs into a node tree.\n * If the rel is not an index or has no expressional columns, we return NIL.\n * Otherwise, the returned tree is copied into the caller's memory context.\n * (We don't want to return a pointer to the relcache copy, since it could\n * disappear due to relcache invalidation.)\n */\nList *\nRelationGetIndexExpressions(Relation relation)\n{\n\tList\t   *result;\n\tDatum\t\texprsDatum;\n\tbool\t\tisnull;\n\tchar\t   *exprsString;\n\tMemoryContext oldcxt;\n\n\t/* Quick exit if we already computed the result. */\n\tif (relation->rd_indexprs)\n\t\treturn (List *) copyObject(relation->rd_indexprs);\n\n\t/* Quick exit if there is nothing to do. */\n\tif (relation->rd_indextuple == NULL ||\n\t\theap_attisnull(relation->rd_indextuple, Anum_pg_index_indexprs))\n\t\treturn NIL;\n\n\t/*\n\t * We build the tree we intend to return in the caller's context. After\n\t * successfully completing the work, we copy it into the relcache entry.\n\t * This avoids problems if we get some sort of error partway through.\n\t */\n\texprsDatum = heap_getattr(relation->rd_indextuple,\n\t\t\t\t\t\t\t  Anum_pg_index_indexprs,\n\t\t\t\t\t\t\t  GetPgIndexDescriptor(),\n\t\t\t\t\t\t\t  &isnull);\n\tAssert(!isnull);\n\texprsString = TextDatumGetCString(exprsDatum);\n\tresult = (List *) stringToNode(exprsString);\n\tpfree(exprsString);\n\n\t/*\n\t * Run the expressions through eval_const_expressions. This is not just an\n\t * optimization, but is necessary, because the planner will be comparing\n\t * them to similarly-processed qual clauses, and may fail to detect valid\n\t * matches without this.  We must not use canonicalize_qual, however,\n\t * since these aren't qual expressions.\n\t */\n\tresult = (List *) eval_const_expressions(NULL, (Node *) result);\n\n\t/* May as well fix opfuncids too */\n\tfix_opfuncids((Node *) result);\n\n\t/* Now save a copy of the completed tree in the relcache entry. */\n\toldcxt = MemoryContextSwitchTo(relation->rd_indexcxt);\n\trelation->rd_indexprs = (List *) copyObject(result);\n\tMemoryContextSwitchTo(oldcxt);\n\n\treturn result;\n}\n\n/*\n * RelationGetDummyIndexExpressions -- get dummy expressions for an index\n *\n * Return a list of dummy expressions (just Const nodes) with the same\n * types/typmods/collations as the index's real expressions.  This is\n * useful in situations where we don't want to run any user-defined code.\n */\nList *\nRelationGetDummyIndexExpressions(Relation relation)\n{\n\tList\t   *result;\n\tDatum\t\texprsDatum;\n\tbool\t\tisnull;\n\tchar\t   *exprsString;\n\tList\t   *rawExprs;\n\tListCell   *lc;\n\n\t/* Quick exit if there is nothing to do. */\n\tif (relation->rd_indextuple == NULL ||\n\t\theap_attisnull(relation->rd_indextuple, Anum_pg_index_indexprs))\n\t\treturn NIL;\n\n\t/* Extract raw node tree(s) from index tuple. */\n\texprsDatum = heap_getattr(relation->rd_indextuple,\n\t\t\t\t\t\t\t  Anum_pg_index_indexprs,\n\t\t\t\t\t\t\t  GetPgIndexDescriptor(),\n\t\t\t\t\t\t\t  &isnull);\n\tAssert(!isnull);\n\texprsString = TextDatumGetCString(exprsDatum);\n\trawExprs = (List *) stringToNode(exprsString);\n\tpfree(exprsString);\n\n\t/* Construct null Consts; the typlen and typbyval are arbitrary. */\n\tresult = NIL;\n\tforeach(lc, rawExprs)\n\t{\n\t\tNode\t   *rawExpr = (Node *) lfirst(lc);\n\n\t\tresult = lappend(result,\n\t\t\t\t\t\t makeConst(exprType(rawExpr),\n\t\t\t\t\t\t\t\t   exprTypmod(rawExpr),\n\t\t\t\t\t\t\t\t   exprCollation(rawExpr),\n\t\t\t\t\t\t\t\t   1,\n\t\t\t\t\t\t\t\t   (Datum) 0,\n\t\t\t\t\t\t\t\t   true,\n\t\t\t\t\t\t\t\t   true));\n\t}\n\n\treturn result;\n}\n\n/*\n * RelationGetIndexPredicate -- get the index predicate for an index\n *\n * We cache the result of transforming pg_index.indpred into an implicit-AND\n * node tree (suitable for ExecQual).\n * If the rel is not an index or has no predicate, we return NIL.\n * Otherwise, the returned tree is copied into the caller's memory context.\n * (We don't want to return a pointer to the relcache copy, since it could\n * disappear due to relcache invalidation.)\n */\nList *\nRelationGetIndexPredicate(Relation relation)\n{\n\tList\t   *result;\n\tDatum\t\tpredDatum;\n\tbool\t\tisnull;\n\tchar\t   *predString;\n\tMemoryContext oldcxt;\n\n\t/* Quick exit if we already computed the result. */\n\tif (relation->rd_indpred)\n\t\treturn (List *) copyObject(relation->rd_indpred);\n\n\t/* Quick exit if there is nothing to do. */\n\tif (relation->rd_indextuple == NULL ||\n\t\theap_attisnull(relation->rd_indextuple, Anum_pg_index_indpred))\n\t\treturn NIL;\n\n\t/*\n\t * We build the tree we intend to return in the caller's context. After\n\t * successfully completing the work, we copy it into the relcache entry.\n\t * This avoids problems if we get some sort of error partway through.\n\t */\n\tpredDatum = heap_getattr(relation->rd_indextuple,\n\t\t\t\t\t\t\t Anum_pg_index_indpred,\n\t\t\t\t\t\t\t GetPgIndexDescriptor(),\n\t\t\t\t\t\t\t &isnull);\n\tAssert(!isnull);\n\tpredString = TextDatumGetCString(predDatum);\n\tresult = (List *) stringToNode(predString);\n\tpfree(predString);\n\n\t/*\n\t * Run the expression through const-simplification and canonicalization.\n\t * This is not just an optimization, but is necessary, because the planner\n\t * will be comparing it to similarly-processed qual clauses, and may fail\n\t * to detect valid matches without this.  This must match the processing\n\t * done to qual clauses in preprocess_expression()!  (We can skip the\n\t * stuff involving subqueries, however, since we don't allow any in index\n\t * predicates.)\n\t */\n\tresult = (List *) eval_const_expressions(NULL, (Node *) result);\n\n\tresult = (List *) canonicalize_qual_ext((Expr *) result, false);\n\n\t/* Also convert to implicit-AND format */\n\tresult = make_ands_implicit((Expr *) result);\n\n\t/* May as well fix opfuncids too */\n\tfix_opfuncids((Node *) result);\n\n\t/* Now save a copy of the completed tree in the relcache entry. */\n\toldcxt = MemoryContextSwitchTo(relation->rd_indexcxt);\n\trelation->rd_indpred = (List *) copyObject(result);\n\tMemoryContextSwitchTo(oldcxt);\n\n\treturn result;\n}\n\n/*\n * RelationGetIndexAttrBitmap -- get a bitmap of index attribute numbers\n *\n * The result has a bit set for each attribute used anywhere in the index\n * definitions of all the indexes on this relation.  (This includes not only\n * simple index keys, but attributes used in expressions and partial-index\n * predicates.)\n *\n * Depending on attrKind, a bitmap covering the attnums for all index columns,\n * for all potential foreign key columns, or for all columns in the configured\n * replica identity index is returned.\n *\n * Attribute numbers are offset by FirstLowInvalidHeapAttributeNumber so that\n * we can include system attributes (e.g., OID) in the bitmap representation.\n *\n * Caller had better hold at least RowExclusiveLock on the target relation\n * to ensure it is safe (deadlock-free) for us to take locks on the relation's\n * indexes.  Note that since the introduction of CREATE INDEX CONCURRENTLY,\n * that lock level doesn't guarantee a stable set of indexes, so we have to\n * be prepared to retry here in case of a change in the set of indexes.\n *\n * The returned result is palloc'd in the caller's memory context and should\n * be bms_free'd when not needed anymore.\n */\nBitmapset *\nRelationGetIndexAttrBitmap(Relation relation, IndexAttrBitmapKind attrKind)\n{\n\tBitmapset  *indexattrs;\t\t/* indexed columns */\n\tBitmapset  *uindexattrs;\t/* columns in unique indexes */\n\tBitmapset  *idindexattrs;\t/* columns in the replica identity */\n\tList\t   *indexoidlist;\n\tList\t   *newindexoidlist;\n\tOid\t\t\trelreplindex;\n\tListCell   *l;\n\tMemoryContext oldcxt;\n\n\t/* Quick exit if we already computed the result. */\n\tif (relation->rd_indexattr != NULL)\n\t{\n\t\tswitch (attrKind)\n\t\t{\n\t\t\tcase INDEX_ATTR_BITMAP_ALL:\n\t\t\t\treturn bms_copy(relation->rd_indexattr);\n\t\t\tcase INDEX_ATTR_BITMAP_KEY:\n\t\t\t\treturn bms_copy(relation->rd_keyattr);\n\t\t\tcase INDEX_ATTR_BITMAP_IDENTITY_KEY:\n\t\t\t\treturn bms_copy(relation->rd_idattr);\n\t\t\tdefault:\n\t\t\t\telog(ERROR, \"unknown attrKind %u\", attrKind);\n\t\t}\n\t}\n\n\t/* Fast path if definitely no indexes */\n\tif (!RelationGetForm(relation)->relhasindex)\n\t\treturn NULL;\n\n\t/*\n\t * Get cached list of index OIDs. If we have to start over, we do so here.\n\t */\nrestart:\n\tindexoidlist = RelationGetIndexList(relation);\n\n\t/* Fall out if no indexes (but relhasindex was set) */\n\tif (indexoidlist == NIL)\n\t\treturn NULL;\n\n\t/*\n\t * Copy the rd_replidindex value computed by RelationGetIndexList before\n\t * proceeding.  This is needed because a relcache flush could occur inside\n\t * index_open below, resetting the fields managed by RelationGetIndexList.\n\t * We need to do the work with a stable value for relreplindex.\n\t */\n\trelreplindex = relation->rd_replidindex;\n\n\t/*\n\t * For each index, add referenced attributes to indexattrs.\n\t *\n\t * Note: we consider all indexes returned by RelationGetIndexList, even if\n\t * they are not indisready or indisvalid.  This is important because an\n\t * index for which CREATE INDEX CONCURRENTLY has just started must be\n\t * included in HOT-safety decisions (see README.HOT).  If a DROP INDEX\n\t * CONCURRENTLY is far enough along that we should ignore the index, it\n\t * won't be returned at all by RelationGetIndexList.\n\t */\n\tindexattrs = NULL;\n\tuindexattrs = NULL;\n\tidindexattrs = NULL;\n\tforeach(l, indexoidlist)\n\t{\n\t\tOid\t\t\tindexOid = lfirst_oid(l);\n\t\tRelation\tindexDesc;\n\t\tDatum\t\tdatum;\n\t\tbool\t\tisnull;\n\t\tNode\t   *indexExpressions;\n\t\tNode\t   *indexPredicate;\n\t\tint\t\t\ti;\n\t\tbool\t\tisKey;\t\t/* candidate key */\n\t\tbool\t\tisIDKey;\t/* replica identity index */\n\n\t\tindexDesc = index_open(indexOid, AccessShareLock);\n\n\t\t/*\n\t\t * Extract index expressions and index predicate.  Note: Don't use\n\t\t * RelationGetIndexExpressions()/RelationGetIndexPredicate(), because\n\t\t * those might run constant expressions evaluation, which needs a\n\t\t * snapshot, which we might not have here.  (Also, it's probably more\n\t\t * sound to collect the bitmaps before any transformations that might\n\t\t * eliminate columns, but the practical impact of this is limited.)\n\t\t */\n\n\t\tdatum = heap_getattr(indexDesc->rd_indextuple, Anum_pg_index_indexprs,\n\t\t\t\t\t\t\t GetPgIndexDescriptor(), &isnull);\n\t\tif (!isnull)\n\t\t\tindexExpressions = stringToNode(TextDatumGetCString(datum));\n\t\telse\n\t\t\tindexExpressions = NULL;\n\n\t\tdatum = heap_getattr(indexDesc->rd_indextuple, Anum_pg_index_indpred,\n\t\t\t\t\t\t\t GetPgIndexDescriptor(), &isnull);\n\t\tif (!isnull)\n\t\t\tindexPredicate = stringToNode(TextDatumGetCString(datum));\n\t\telse\n\t\t\tindexPredicate = NULL;\n\n\t\t/* Can this index be referenced by a foreign key? */\n\t\tisKey = indexDesc->rd_index->indisunique &&\n\t\t\tindexExpressions == NULL &&\n\t\t\tindexPredicate == NULL;\n\n\t\t/* Is this index the configured (or default) replica identity? */\n\t\tisIDKey = (indexOid == relreplindex);\n\n\t\t/* Collect simple attribute references */\n\t\tfor (i = 0; i < indexDesc->rd_index->indnatts; i++)\n\t\t{\n\t\t\tint\t\t\tattrnum = indexDesc->rd_index->indkey.values[i];\n\n\t\t\tif (attrnum != 0)\n\t\t\t{\n\t\t\t\tindexattrs = bms_add_member(indexattrs,\n\t\t\t\t\t\t\t   attrnum - FirstLowInvalidHeapAttributeNumber);\n\n\t\t\t\tif (isKey)\n\t\t\t\t\tuindexattrs = bms_add_member(uindexattrs,\n\t\t\t\t\t\t\t   attrnum - FirstLowInvalidHeapAttributeNumber);\n\n\t\t\t\tif (isIDKey)\n\t\t\t\t\tidindexattrs = bms_add_member(idindexattrs,\n\t\t\t\t\t\t\t   attrnum - FirstLowInvalidHeapAttributeNumber);\n\t\t\t}\n\t\t}\n\n\t\t/* Collect all attributes used in expressions, too */\n\t\tpull_varattnos(indexExpressions, 1, &indexattrs);\n\n\t\t/* Collect all attributes in the index predicate, too */\n\t\tpull_varattnos(indexPredicate, 1, &indexattrs);\n\n\t\tindex_close(indexDesc, AccessShareLock);\n\t}\n\n\t/*\n\t * During one of the index_opens in the above loop, we might have received\n\t * a relcache flush event on this relcache entry, which might have been\n\t * signaling a change in the rel's index list.  If so, we'd better start\n\t * over to ensure we deliver up-to-date attribute bitmaps.\n\t */\n\tnewindexoidlist = RelationGetIndexList(relation);\n\tif (equal(indexoidlist, newindexoidlist) &&\n\t\trelreplindex == relation->rd_replidindex)\n\t{\n\t\t/* Still the same index set, so proceed */\n\t\tlist_free(newindexoidlist);\n\t\tlist_free(indexoidlist);\n\t}\n\telse\n\t{\n\t\t/* Gotta do it over ... might as well not leak memory */\n\t\tlist_free(newindexoidlist);\n\t\tlist_free(indexoidlist);\n\t\tbms_free(uindexattrs);\n\t\tbms_free(idindexattrs);\n\t\tbms_free(indexattrs);\n\n\t\tgoto restart;\n\t}\n\n\t/*\n\t * Now save copies of the bitmaps in the relcache entry.  We intentionally\n\t * set rd_indexattr last, because that's the one that signals validity of\n\t * the values; if we run out of memory before making that copy, we won't\n\t * leave the relcache entry looking like the other ones are valid but\n\t * empty.\n\t */\n\toldcxt = MemoryContextSwitchTo(CacheMemoryContext);\n\trelation->rd_keyattr = bms_copy(uindexattrs);\n\trelation->rd_idattr = bms_copy(idindexattrs);\n\trelation->rd_indexattr = bms_copy(indexattrs);\n\tMemoryContextSwitchTo(oldcxt);\n\n\t/* We return our original working copy for caller to play with */\n\tswitch (attrKind)\n\t{\n\t\tcase INDEX_ATTR_BITMAP_ALL:\n\t\t\treturn indexattrs;\n\t\tcase INDEX_ATTR_BITMAP_KEY:\n\t\t\treturn uindexattrs;\n\t\tcase INDEX_ATTR_BITMAP_IDENTITY_KEY:\n\t\t\treturn idindexattrs;\n\t\tdefault:\n\t\t\telog(ERROR, \"unknown attrKind %u\", attrKind);\n\t\t\treturn NULL;\n\t}\n}\n\n/*\n * RelationGetExclusionInfo -- get info about index's exclusion constraint\n *\n * This should be called only for an index that is known to have an\n * associated exclusion constraint.  It returns arrays (palloc'd in caller's\n * context) of the exclusion operator OIDs, their underlying functions'\n * OIDs, and their strategy numbers in the index's opclasses.  We cache\n * all this information since it requires a fair amount of work to get.\n */\nvoid\nRelationGetExclusionInfo(Relation indexRelation,\n\t\t\t\t\t\t Oid **operators,\n\t\t\t\t\t\t Oid **procs,\n\t\t\t\t\t\t uint16 **strategies)\n{\n\tint\t\t\tncols = indexRelation->rd_rel->relnatts;\n\tOid\t\t   *ops;\n\tOid\t\t   *funcs;\n\tuint16\t   *strats;\n\tRelation\tconrel;\n\tSysScanDesc conscan;\n\tScanKeyData skey[1];\n\tHeapTuple\thtup;\n\tbool\t\tfound;\n\tMemoryContext oldcxt;\n\tint\t\t\ti;\n\n\t/* Allocate result space in caller context */\n\t*operators = ops = (Oid *) palloc(sizeof(Oid) * ncols);\n\t*procs = funcs = (Oid *) palloc(sizeof(Oid) * ncols);\n\t*strategies = strats = (uint16 *) palloc(sizeof(uint16) * ncols);\n\n\t/* Quick exit if we have the data cached already */\n\tif (indexRelation->rd_exclstrats != NULL)\n\t{\n\t\tmemcpy(ops, indexRelation->rd_exclops, sizeof(Oid) * ncols);\n\t\tmemcpy(funcs, indexRelation->rd_exclprocs, sizeof(Oid) * ncols);\n\t\tmemcpy(strats, indexRelation->rd_exclstrats, sizeof(uint16) * ncols);\n\t\treturn;\n\t}\n\n\t/*\n\t * Search pg_constraint for the constraint associated with the index. To\n\t * make this not too painfully slow, we use the index on conrelid; that\n\t * will hold the parent relation's OID not the index's own OID.\n\t */\n\tScanKeyInit(&skey[0],\n\t\t\t\tAnum_pg_constraint_conrelid,\n\t\t\t\tBTEqualStrategyNumber, F_OIDEQ,\n\t\t\t\tObjectIdGetDatum(indexRelation->rd_index->indrelid));\n\n\tconrel = heap_open(ConstraintRelationId, AccessShareLock);\n\tconscan = systable_beginscan(conrel, ConstraintRelidIndexId, true,\n\t\t\t\t\t\t\t\t NULL, 1, skey);\n\tfound = false;\n\n\twhile (HeapTupleIsValid(htup = systable_getnext(conscan)))\n\t{\n\t\tForm_pg_constraint conform = (Form_pg_constraint) GETSTRUCT(htup);\n\t\tDatum\t\tval;\n\t\tbool\t\tisnull;\n\t\tArrayType  *arr;\n\t\tint\t\t\tnelem;\n\n\t\t/* We want the exclusion constraint owning the index */\n\t\tif (conform->contype != CONSTRAINT_EXCLUSION ||\n\t\t\tconform->conindid != RelationGetRelid(indexRelation))\n\t\t\tcontinue;\n\n\t\t/* There should be only one */\n\t\tif (found)\n\t\t\telog(ERROR, \"unexpected exclusion constraint record found for rel %s\",\n\t\t\t\t RelationGetRelationName(indexRelation));\n\t\tfound = true;\n\n\t\t/* Extract the operator OIDS from conexclop */\n\t\tval = fastgetattr(htup,\n\t\t\t\t\t\t  Anum_pg_constraint_conexclop,\n\t\t\t\t\t\t  conrel->rd_att, &isnull);\n\t\tif (isnull)\n\t\t\telog(ERROR, \"null conexclop for rel %s\",\n\t\t\t\t RelationGetRelationName(indexRelation));\n\n\t\tarr = DatumGetArrayTypeP(val);\t/* ensure not toasted */\n\t\tnelem = ARR_DIMS(arr)[0];\n\t\tif (ARR_NDIM(arr) != 1 ||\n\t\t\tnelem != ncols ||\n\t\t\tARR_HASNULL(arr) ||\n\t\t\tARR_ELEMTYPE(arr) != OIDOID)\n\t\t\telog(ERROR, \"conexclop is not a 1-D Oid array\");\n\n\t\tmemcpy(ops, ARR_DATA_PTR(arr), sizeof(Oid) * ncols);\n\t}\n\n\tsystable_endscan(conscan);\n\theap_close(conrel, AccessShareLock);\n\n\tif (!found)\n\t\telog(ERROR, \"exclusion constraint record missing for rel %s\",\n\t\t\t RelationGetRelationName(indexRelation));\n\n\t/* We need the func OIDs and strategy numbers too */\n\tfor (i = 0; i < ncols; i++)\n\t{\n\t\tfuncs[i] = get_opcode(ops[i]);\n\t\tstrats[i] = get_op_opfamily_strategy(ops[i],\n\t\t\t\t\t\t\t\t\t\t\t indexRelation->rd_opfamily[i]);\n\t\t/* shouldn't fail, since it was checked at index creation */\n\t\tif (strats[i] == InvalidStrategy)\n\t\t\telog(ERROR, \"could not find strategy for operator %u in family %u\",\n\t\t\t\t ops[i], indexRelation->rd_opfamily[i]);\n\t}\n\n\t/* Save a copy of the results in the relcache entry. */\n\toldcxt = MemoryContextSwitchTo(indexRelation->rd_indexcxt);\n\tindexRelation->rd_exclops = (Oid *) palloc(sizeof(Oid) * ncols);\n\tindexRelation->rd_exclprocs = (Oid *) palloc(sizeof(Oid) * ncols);\n\tindexRelation->rd_exclstrats = (uint16 *) palloc(sizeof(uint16) * ncols);\n\tmemcpy(indexRelation->rd_exclops, ops, sizeof(Oid) * ncols);\n\tmemcpy(indexRelation->rd_exclprocs, funcs, sizeof(Oid) * ncols);\n\tmemcpy(indexRelation->rd_exclstrats, strats, sizeof(uint16) * ncols);\n\tMemoryContextSwitchTo(oldcxt);\n}\n\n\n/*\n * Routines to support ereport() reports of relation-related errors\n *\n * These could have been put into elog.c, but it seems like a module layering\n * violation to have elog.c calling relcache or syscache stuff --- and we\n * definitely don't want elog.h including rel.h.  So we put them here.\n */\n\n/*\n * errtable --- stores schema_name and table_name of a table\n * within the current errordata.\n */\nint\nerrtable(Relation rel)\n{\n\terr_generic_string(PG_DIAG_SCHEMA_NAME,\n\t\t\t\t\t   get_namespace_name(RelationGetNamespace(rel)));\n\terr_generic_string(PG_DIAG_TABLE_NAME, RelationGetRelationName(rel));\n\n\treturn 0;\t\t\t\t\t/* return value does not matter */\n}\n\n/*\n * errtablecol --- stores schema_name, table_name and column_name\n * of a table column within the current errordata.\n *\n * The column is specified by attribute number --- for most callers, this is\n * easier and less error-prone than getting the column name for themselves.\n */\nint\nerrtablecol(Relation rel, int attnum)\n{\n\tTupleDesc\treldesc = RelationGetDescr(rel);\n\tconst char *colname;\n\n\t/* Use reldesc if it's a user attribute, else consult the catalogs */\n\tif (attnum > 0 && attnum <= reldesc->natts)\n\t\tcolname = NameStr(reldesc->attrs[attnum - 1]->attname);\n\telse\n\t\tcolname = get_relid_attribute_name(RelationGetRelid(rel), attnum);\n\n\treturn errtablecolname(rel, colname);\n}\n\n/*\n * errtablecolname --- stores schema_name, table_name and column_name\n * of a table column within the current errordata, where the column name is\n * given directly rather than extracted from the relation's catalog data.\n *\n * Don't use this directly unless errtablecol() is inconvenient for some\n * reason.  This might possibly be needed during intermediate states in ALTER\n * TABLE, for instance.\n */\nint\nerrtablecolname(Relation rel, const char *colname)\n{\n\terrtable(rel);\n\terr_generic_string(PG_DIAG_COLUMN_NAME, colname);\n\n\treturn 0;\t\t\t\t\t/* return value does not matter */\n}\n\n/*\n * errtableconstraint --- stores schema_name, table_name and constraint_name\n * of a table-related constraint within the current errordata.\n */\nint\nerrtableconstraint(Relation rel, const char *conname)\n{\n\terrtable(rel);\n\terr_generic_string(PG_DIAG_CONSTRAINT_NAME, conname);\n\n\treturn 0;\t\t\t\t\t/* return value does not matter */\n}\n\n\n/*\n *\tload_relcache_init_file, write_relcache_init_file\n *\n *\t\tIn late 1992, we started regularly having databases with more than\n *\t\ta thousand classes in them.  With this number of classes, it became\n *\t\tcritical to do indexed lookups on the system catalogs.\n *\n *\t\tBootstrapping these lookups is very hard.  We want to be able to\n *\t\tuse an index on pg_attribute, for example, but in order to do so,\n *\t\twe must have read pg_attribute for the attributes in the index,\n *\t\twhich implies that we need to use the index.\n *\n *\t\tIn order to get around the problem, we do the following:\n *\n *\t\t   +  When the database system is initialized (at initdb time), we\n *\t\t\t  don't use indexes.  We do sequential scans.\n *\n *\t\t   +  When the backend is started up in normal mode, we load an image\n *\t\t\t  of the appropriate relation descriptors, in internal format,\n *\t\t\t  from an initialization file in the data/base/... directory.\n *\n *\t\t   +  If the initialization file isn't there, then we create the\n *\t\t\t  relation descriptors using sequential scans and write 'em to\n *\t\t\t  the initialization file for use by subsequent backends.\n *\n *\t\tAs of Postgres 9.0, there is one local initialization file in each\n *\t\tdatabase, plus one shared initialization file for shared catalogs.\n *\n *\t\tWe could dispense with the initialization files and just build the\n *\t\tcritical reldescs the hard way on every backend startup, but that\n *\t\tslows down backend startup noticeably.\n *\n *\t\tWe can in fact go further, and save more relcache entries than\n *\t\tjust the ones that are absolutely critical; this allows us to speed\n *\t\tup backend startup by not having to build such entries the hard way.\n *\t\tPresently, all the catalog and index entries that are referred to\n *\t\tby catcaches are stored in the initialization files.\n *\n *\t\tThe same mechanism that detects when catcache and relcache entries\n *\t\tneed to be invalidated (due to catalog updates) also arranges to\n *\t\tunlink the initialization files when the contents may be out of date.\n *\t\tThe files will then be rebuilt during the next backend startup.\n */\n\n/*\n * load_relcache_init_file -- attempt to load cache from the shared\n * or local cache init file\n *\n * If successful, return TRUE and set criticalRelcachesBuilt or\n * criticalSharedRelcachesBuilt to true.\n * If not successful, return FALSE.\n *\n * NOTE: we assume we are already switched into CacheMemoryContext.\n */\nstatic bool\nload_relcache_init_file(bool shared)\n{\n\tFILE\t   *fp;\n\tchar\t\tinitfilename[MAXPGPATH];\n\tRelation   *rels;\n\tint\t\t\trelno,\n\t\t\t\tnum_rels,\n\t\t\t\tmax_rels,\n\t\t\t\tnailed_rels,\n\t\t\t\tnailed_indexes,\n\t\t\t\tmagic;\n\tint\t\t\ti;\n\n\tif (shared)\n\t\tsnprintf(initfilename, sizeof(initfilename), \"global/%s\",\n\t\t\t\t RELCACHE_INIT_FILENAME);\n\telse\n\t\tsnprintf(initfilename, sizeof(initfilename), \"%s/%s\",\n\t\t\t\t DatabasePath, RELCACHE_INIT_FILENAME);\n\n\tfp = AllocateFile(initfilename, PG_BINARY_R);\n\tif (fp == NULL)\n\t\treturn false;\n\n\t/*\n\t * Read the index relcache entries from the file.  Note we will not enter\n\t * any of them into the cache if the read fails partway through; this\n\t * helps to guard against broken init files.\n\t */\n\tmax_rels = 100;\n\trels = (Relation *) palloc(max_rels * sizeof(Relation));\n\tnum_rels = 0;\n\tnailed_rels = nailed_indexes = 0;\n\n\t/* check for correct magic number (compatible version) */\n\tif (fread(&magic, 1, sizeof(magic), fp) != sizeof(magic))\n\t\tgoto read_failed;\n\tif (magic != RELCACHE_INIT_FILEMAGIC)\n\t\tgoto read_failed;\n\n\tfor (relno = 0;; relno++)\n\t{\n\t\tSize\t\tlen;\n\t\tsize_t\t\tnread;\n\t\tRelation\trel;\n\t\tForm_pg_class relform;\n\t\tbool\t\thas_not_null;\n\n\t\t/* first read the relation descriptor length */\n\t\tnread = fread(&len, 1, sizeof(len), fp);\n\t\tif (nread != sizeof(len))\n\t\t{\n\t\t\tif (nread == 0)\n\t\t\t\tbreak;\t\t\t/* end of file */\n\t\t\tgoto read_failed;\n\t\t}\n\n\t\t/* safety check for incompatible relcache layout */\n\t\tif (len != sizeof(RelationData))\n\t\t\tgoto read_failed;\n\n\t\t/* allocate another relcache header */\n\t\tif (num_rels >= max_rels)\n\t\t{\n\t\t\tmax_rels *= 2;\n\t\t\trels = (Relation *) repalloc(rels, max_rels * sizeof(Relation));\n\t\t}\n\n\t\trel = rels[num_rels++] = (Relation) palloc(len);\n\n\t\t/* then, read the Relation structure */\n\t\tif (fread(rel, 1, len, fp) != len)\n\t\t\tgoto read_failed;\n\n\t\t/* next read the relation tuple form */\n\t\tif (fread(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\t\tgoto read_failed;\n\n\t\trelform = (Form_pg_class) palloc(len);\n\t\tif (fread(relform, 1, len, fp) != len)\n\t\t\tgoto read_failed;\n\n\t\trel->rd_rel = relform;\n\n\t\t/* initialize attribute tuple forms */\n\t\trel->rd_att = CreateTemplateTupleDesc(relform->relnatts,\n\t\t\t\t\t\t\t\t\t\t\t  relform->relhasoids);\n\t\trel->rd_att->tdrefcount = 1;\t/* mark as refcounted */\n\n\t\trel->rd_att->tdtypeid = relform->reltype;\n\t\trel->rd_att->tdtypmod = -1;\t\t/* unnecessary, but... */\n\n\t\t/* next read all the attribute tuple form data entries */\n\t\thas_not_null = false;\n\t\tfor (i = 0; i < relform->relnatts; i++)\n\t\t{\n\t\t\tif (fread(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\t\t\tgoto read_failed;\n\t\t\tif (len != ATTRIBUTE_FIXED_PART_SIZE)\n\t\t\t\tgoto read_failed;\n\t\t\tif (fread(rel->rd_att->attrs[i], 1, len, fp) != len)\n\t\t\t\tgoto read_failed;\n\n\t\t\thas_not_null |= rel->rd_att->attrs[i]->attnotnull;\n\t\t}\n\n\t\t/* next read the access method specific field */\n\t\tif (fread(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\t\tgoto read_failed;\n\t\tif (len > 0)\n\t\t{\n\t\t\trel->rd_options = palloc(len);\n\t\t\tif (fread(rel->rd_options, 1, len, fp) != len)\n\t\t\t\tgoto read_failed;\n\t\t\tif (len != VARSIZE(rel->rd_options))\n\t\t\t\tgoto read_failed;\t\t/* sanity check */\n\t\t}\n\t\telse\n\t\t{\n\t\t\trel->rd_options = NULL;\n\t\t}\n\n\t\t/* mark not-null status */\n\t\tif (has_not_null)\n\t\t{\n\t\t\tTupleConstr *constr = (TupleConstr *) palloc0(sizeof(TupleConstr));\n\n\t\t\tconstr->has_not_null = true;\n\t\t\trel->rd_att->constr = constr;\n\t\t}\n\n\t\t/* If it's an index, there's more to do */\n\t\tif (rel->rd_rel->relkind == RELKIND_INDEX)\n\t\t{\n\t\t\tForm_pg_am\tam;\n\t\t\tMemoryContext indexcxt;\n\t\t\tOid\t\t   *opfamily;\n\t\t\tOid\t\t   *opcintype;\n\t\t\tRegProcedure *support;\n\t\t\tint\t\t\tnsupport;\n\t\t\tint16\t   *indoption;\n\t\t\tOid\t\t   *indcollation;\n\n\t\t\t/* Count nailed indexes to ensure we have 'em all */\n\t\t\tif (rel->rd_isnailed)\n\t\t\t\tnailed_indexes++;\n\n\t\t\t/* next, read the pg_index tuple */\n\t\t\tif (fread(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\t\t\tgoto read_failed;\n\n\t\t\trel->rd_indextuple = (HeapTuple) palloc(len);\n\t\t\tif (fread(rel->rd_indextuple, 1, len, fp) != len)\n\t\t\t\tgoto read_failed;\n\n\t\t\t/* Fix up internal pointers in the tuple -- see heap_copytuple */\n\t\t\trel->rd_indextuple->t_data = (HeapTupleHeader) ((char *) rel->rd_indextuple + HEAPTUPLESIZE);\n\t\t\trel->rd_index = (Form_pg_index) GETSTRUCT(rel->rd_indextuple);\n\n\t\t\t/* next, read the access method tuple form */\n\t\t\tif (fread(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\t\t\tgoto read_failed;\n\n\t\t\tam = (Form_pg_am) palloc(len);\n\t\t\tif (fread(am, 1, len, fp) != len)\n\t\t\t\tgoto read_failed;\n\t\t\trel->rd_am = am;\n\n\t\t\t/*\n\t\t\t * prepare index info context --- parameters should match\n\t\t\t * RelationInitIndexAccessInfo\n\t\t\t */\n\t\t\tindexcxt = AllocSetContextCreate(CacheMemoryContext,\n\t\t\t\t\t\t\t\t\t\t\t RelationGetRelationName(rel),\n\t\t\t\t\t\t\t\t\t\t\t ALLOCSET_SMALL_MINSIZE,\n\t\t\t\t\t\t\t\t\t\t\t ALLOCSET_SMALL_INITSIZE,\n\t\t\t\t\t\t\t\t\t\t\t ALLOCSET_SMALL_MAXSIZE);\n\t\t\trel->rd_indexcxt = indexcxt;\n\n\t\t\t/* next, read the vector of opfamily OIDs */\n\t\t\tif (fread(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\t\t\tgoto read_failed;\n\n\t\t\topfamily = (Oid *) MemoryContextAlloc(indexcxt, len);\n\t\t\tif (fread(opfamily, 1, len, fp) != len)\n\t\t\t\tgoto read_failed;\n\n\t\t\trel->rd_opfamily = opfamily;\n\n\t\t\t/* next, read the vector of opcintype OIDs */\n\t\t\tif (fread(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\t\t\tgoto read_failed;\n\n\t\t\topcintype = (Oid *) MemoryContextAlloc(indexcxt, len);\n\t\t\tif (fread(opcintype, 1, len, fp) != len)\n\t\t\t\tgoto read_failed;\n\n\t\t\trel->rd_opcintype = opcintype;\n\n\t\t\t/* next, read the vector of support procedure OIDs */\n\t\t\tif (fread(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\t\t\tgoto read_failed;\n\t\t\tsupport = (RegProcedure *) MemoryContextAlloc(indexcxt, len);\n\t\t\tif (fread(support, 1, len, fp) != len)\n\t\t\t\tgoto read_failed;\n\n\t\t\trel->rd_support = support;\n\n\t\t\t/* next, read the vector of collation OIDs */\n\t\t\tif (fread(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\t\t\tgoto read_failed;\n\n\t\t\tindcollation = (Oid *) MemoryContextAlloc(indexcxt, len);\n\t\t\tif (fread(indcollation, 1, len, fp) != len)\n\t\t\t\tgoto read_failed;\n\n\t\t\trel->rd_indcollation = indcollation;\n\n\t\t\t/* finally, read the vector of indoption values */\n\t\t\tif (fread(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\t\t\tgoto read_failed;\n\n\t\t\tindoption = (int16 *) MemoryContextAlloc(indexcxt, len);\n\t\t\tif (fread(indoption, 1, len, fp) != len)\n\t\t\t\tgoto read_failed;\n\n\t\t\trel->rd_indoption = indoption;\n\n\t\t\t/* set up zeroed fmgr-info vectors */\n\t\t\trel->rd_aminfo = (RelationAmInfo *)\n\t\t\t\tMemoryContextAllocZero(indexcxt, sizeof(RelationAmInfo));\n\t\t\tnsupport = relform->relnatts * am->amsupport;\n\t\t\trel->rd_supportinfo = (FmgrInfo *)\n\t\t\t\tMemoryContextAllocZero(indexcxt, nsupport * sizeof(FmgrInfo));\n\t\t}\n\t\telse\n\t\t{\n\t\t\t/* Count nailed rels to ensure we have 'em all */\n\t\t\tif (rel->rd_isnailed)\n\t\t\t\tnailed_rels++;\n\n\t\t\tAssert(rel->rd_index == NULL);\n\t\t\tAssert(rel->rd_indextuple == NULL);\n\t\t\tAssert(rel->rd_am == NULL);\n\t\t\tAssert(rel->rd_indexcxt == NULL);\n\t\t\tAssert(rel->rd_aminfo == NULL);\n\t\t\tAssert(rel->rd_opfamily == NULL);\n\t\t\tAssert(rel->rd_opcintype == NULL);\n\t\t\tAssert(rel->rd_support == NULL);\n\t\t\tAssert(rel->rd_supportinfo == NULL);\n\t\t\tAssert(rel->rd_indoption == NULL);\n\t\t\tAssert(rel->rd_indcollation == NULL);\n\t\t}\n\n\t\t/*\n\t\t * Rules and triggers are not saved (mainly because the internal\n\t\t * format is complex and subject to change).  They must be rebuilt if\n\t\t * needed by RelationCacheInitializePhase3.  This is not expected to\n\t\t * be a big performance hit since few system catalogs have such. Ditto\n\t\t * for index expressions, predicates, exclusion info, and FDW info.\n\t\t */\n\t\trel->rd_rules = NULL;\n\t\trel->rd_rulescxt = NULL;\n\t\trel->trigdesc = NULL;\n\t\trel->rd_indexprs = NIL;\n\t\trel->rd_indpred = NIL;\n\t\trel->rd_exclops = NULL;\n\t\trel->rd_exclprocs = NULL;\n\t\trel->rd_exclstrats = NULL;\n\t\trel->rd_fdwroutine = NULL;\n\n\t\t/*\n\t\t * Reset transient-state fields in the relcache entry\n\t\t */\n\t\trel->rd_smgr = NULL;\n\t\tif (rel->rd_isnailed)\n\t\t\trel->rd_refcnt = 1;\n\t\telse\n\t\t\trel->rd_refcnt = 0;\n\t\trel->rd_indexvalid = 0;\n\t\trel->rd_indexlist = NIL;\n\t\trel->rd_oidindex = InvalidOid;\n\t\trel->rd_replidindex = InvalidOid;\n\t\trel->rd_indexattr = NULL;\n\t\trel->rd_keyattr = NULL;\n\t\trel->rd_idattr = NULL;\n\t\trel->rd_createSubid = InvalidSubTransactionId;\n\t\trel->rd_newRelfilenodeSubid = InvalidSubTransactionId;\n\t\trel->rd_amcache = NULL;\n\t\tMemSet(&rel->pgstat_info, 0, sizeof(rel->pgstat_info));\n        rel->rd_cdbpolicy = NULL;\n        rel->rd_cdbDefaultStatsWarningIssued = false;\n\n\t\t/*\n\t\t * Recompute lock and physical addressing info.  This is needed in\n\t\t * case the pg_internal.init file was copied from some other database\n\t\t * by CREATE DATABASE.\n\t\t */\n\t\tRelationInitLockInfo(rel);\n\t\tRelationInitPhysicalAddr(rel);\n\t}\n\n\t/*\n\t * We reached the end of the init file without apparent problem.  Did we\n\t * get the right number of nailed items?  This is a useful crosscheck in\n\t * case the set of critical rels or indexes changes.  However, that should\n\t * not happen in a normally-running system, so let's bleat if it does.\n\t */\n\tif (shared)\n\t{\n\t\tif (nailed_rels != NUM_CRITICAL_SHARED_RELS ||\n\t\t\tnailed_indexes != NUM_CRITICAL_SHARED_INDEXES)\n\t\t{\n\t\t\telog(WARNING, \"found %d nailed shared rels and %d nailed shared indexes in init file, but expected %d and %d respectively\",\n\t\t\t\t nailed_rels, nailed_indexes,\n\t\t\t\t NUM_CRITICAL_SHARED_RELS, NUM_CRITICAL_SHARED_INDEXES);\n\t\t\tgoto read_failed;\n\t\t}\n\t}\n\telse\n\t{\n\t\tif (nailed_rels != NUM_CRITICAL_LOCAL_RELS ||\n\t\t\tnailed_indexes != NUM_CRITICAL_LOCAL_INDEXES)\n\t\t{\n\t\t\telog(WARNING, \"found %d nailed rels and %d nailed indexes in init file, but expected %d and %d respectively\",\n\t\t\t\t nailed_rels, nailed_indexes,\n\t\t\t\t NUM_CRITICAL_LOCAL_RELS, NUM_CRITICAL_LOCAL_INDEXES);\n\t\t\tgoto read_failed;\n\t\t}\n\t}\n\n\t/*\n\t * OK, all appears well.\n\t *\n\t * Now insert all the new relcache entries into the cache.\n\t */\n\tfor (relno = 0; relno < num_rels; relno++)\n\t{\n\t\tRelationCacheInsert(rels[relno], false);\n\t}\n\n\tpfree(rels);\n\tFreeFile(fp);\n\n\tif (shared)\n\t\tcriticalSharedRelcachesBuilt = true;\n\telse\n\t\tcriticalRelcachesBuilt = true;\n\treturn true;\n\n\t/*\n\t * init file is broken, so do it the hard way.  We don't bother trying to\n\t * free the clutter we just allocated; it's not in the relcache so it\n\t * won't hurt.\n\t */\nread_failed:\n\tpfree(rels);\n\tFreeFile(fp);\n\n\treturn false;\n}\n\n/*\n * Write out a new initialization file with the current contents\n * of the relcache (either shared rels or local rels, as indicated).\n */\nstatic void\nwrite_relcache_init_file(bool shared)\n{\n\tFILE\t   *fp;\n\tchar\t\ttempfilename[MAXPGPATH];\n\tchar\t\tfinalfilename[MAXPGPATH];\n\tint\t\t\tmagic;\n\tHASH_SEQ_STATUS status;\n\tRelIdCacheEnt *idhentry;\n\tint\t\t\ti;\n\n\t/*\n\t * If we have already received any relcache inval events, there's no\n\t * chance of succeeding so we may as well skip the whole thing.\n\t */\n\tif (relcacheInvalsReceived != 0L)\n\t\treturn;\n\n\t/*\n\t * We must write a temporary file and rename it into place. Otherwise,\n\t * another backend starting at about the same time might crash trying to\n\t * read the partially-complete file.\n\t */\n\tif (shared)\n\t{\n\t\tsnprintf(tempfilename, sizeof(tempfilename), \"global/%s.%d\",\n\t\t\t\t RELCACHE_INIT_FILENAME, MyProcPid);\n\t\tsnprintf(finalfilename, sizeof(finalfilename), \"global/%s\",\n\t\t\t\t RELCACHE_INIT_FILENAME);\n\t}\n\telse\n\t{\n\t\tsnprintf(tempfilename, sizeof(tempfilename), \"%s/%s.%d\",\n\t\t\t\t DatabasePath, RELCACHE_INIT_FILENAME, MyProcPid);\n\t\tsnprintf(finalfilename, sizeof(finalfilename), \"%s/%s\",\n\t\t\t\t DatabasePath, RELCACHE_INIT_FILENAME);\n\t}\n\n\tunlink(tempfilename);\t\t/* in case it exists w/wrong permissions */\n\n\tfp = AllocateFile(tempfilename, PG_BINARY_W);\n\tif (fp == NULL)\n\t{\n\t\t/*\n\t\t * We used to consider this a fatal error, but we might as well\n\t\t * continue with backend startup ...\n\t\t */\n\t\tereport(WARNING,\n\t\t\t\t(errcode_for_file_access(),\n\t\t\t\t errmsg(\"could not create relation-cache initialization file \\\"%s\\\": %m\",\n\t\t\t\t\t\ttempfilename),\n\t\t\t  errdetail(\"Continuing anyway, but there's something wrong.\")));\n\t\treturn;\n\t}\n\n\t/*\n\t * Write a magic number to serve as a file version identifier.  We can\n\t * change the magic number whenever the relcache layout changes.\n\t */\n\tmagic = RELCACHE_INIT_FILEMAGIC;\n\tif (fwrite(&magic, 1, sizeof(magic), fp) != sizeof(magic))\n\t\telog(FATAL, \"could not write init file\");\n\n\t/*\n\t * Write all the appropriate reldescs (in no particular order).\n\t */\n\thash_seq_init(&status, RelationIdCache);\n\n\twhile ((idhentry = (RelIdCacheEnt *) hash_seq_search(&status)) != NULL)\n\t{\n\t\tRelation\trel = idhentry->reldesc;\n\t\tForm_pg_class relform = rel->rd_rel;\n\n\t\t/* ignore if not correct group */\n\t\tif (relform->relisshared != shared)\n\t\t\tcontinue;\n\n\t\t/*\n\t\t * Ignore if not supposed to be in init file.  We can allow any shared\n\t\t * relation that's been loaded so far to be in the shared init file,\n\t\t * but unshared relations must be ones that should be in the local\n\t\t * file per RelationIdIsInInitFile.  (Note: if you want to change the\n\t\t * criterion for rels to be kept in the init file, see also inval.c.\n\t\t * The reason for filtering here is to be sure that we don't put\n\t\t * anything into the local init file for which a relcache inval would\n\t\t * not cause invalidation of that init file.)\n\t\t */\n\t\tif (!shared && !RelationIdIsInInitFile(RelationGetRelid(rel)))\n\t\t{\n\t\t\t/* Nailed rels had better get stored. */\n\t\t\tAssert(!rel->rd_isnailed);\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* first write the relcache entry proper */\n\t\twrite_item(rel, sizeof(RelationData), fp);\n\n\t\t/* next write the relation tuple form */\n\t\twrite_item(relform, CLASS_TUPLE_SIZE, fp);\n\n\t\t/* next, do all the attribute tuple form data entries */\n\t\tfor (i = 0; i < relform->relnatts; i++)\n\t\t{\n\t\t\twrite_item(rel->rd_att->attrs[i], ATTRIBUTE_FIXED_PART_SIZE, fp);\n\t\t}\n\n\t\t/* next, do the access method specific field */\n\t\twrite_item(rel->rd_options,\n\t\t\t\t   (rel->rd_options ? VARSIZE(rel->rd_options) : 0),\n\t\t\t\t   fp);\n\n\t\t/* If it's an index, there's more to do */\n\t\tif (rel->rd_rel->relkind == RELKIND_INDEX)\n\t\t{\n\t\t\tForm_pg_am\tam = rel->rd_am;\n\n\t\t\t/* write the pg_index tuple */\n\t\t\t/* we assume this was created by heap_copytuple! */\n\t\t\twrite_item(rel->rd_indextuple,\n\t\t\t\t\t   HEAPTUPLESIZE + rel->rd_indextuple->t_len,\n\t\t\t\t\t   fp);\n\n\t\t\t/* next, write the access method tuple form */\n\t\t\twrite_item(am, sizeof(FormData_pg_am), fp);\n\n\t\t\t/* next, write the vector of opfamily OIDs */\n\t\t\twrite_item(rel->rd_opfamily,\n\t\t\t\t\t   relform->relnatts * sizeof(Oid),\n\t\t\t\t\t   fp);\n\n\t\t\t/* next, write the vector of opcintype OIDs */\n\t\t\twrite_item(rel->rd_opcintype,\n\t\t\t\t\t   relform->relnatts * sizeof(Oid),\n\t\t\t\t\t   fp);\n\n\t\t\t/* next, write the vector of support procedure OIDs */\n\t\t\twrite_item(rel->rd_support,\n\t\t\t\t  relform->relnatts * (am->amsupport * sizeof(RegProcedure)),\n\t\t\t\t\t   fp);\n\n\t\t\t/* next, write the vector of collation OIDs */\n\t\t\twrite_item(rel->rd_indcollation,\n\t\t\t\t\t   relform->relnatts * sizeof(Oid),\n\t\t\t\t\t   fp);\n\n\t\t\t/* finally, write the vector of indoption values */\n\t\t\twrite_item(rel->rd_indoption,\n\t\t\t\t\t   relform->relnatts * sizeof(int16),\n\t\t\t\t\t   fp);\n\t\t}\n\t}\n\n\tif (FreeFile(fp))\n\t\telog(FATAL, \"could not write init file\");\n\n\t/*\n\t * Now we have to check whether the data we've so painstakingly\n\t * accumulated is already obsolete due to someone else's just-committed\n\t * catalog changes.  If so, we just delete the temp file and leave it to\n\t * the next backend to try again.  (Our own relcache entries will be\n\t * updated by SI message processing, but we can't be sure whether what we\n\t * wrote out was up-to-date.)\n\t *\n\t * This mustn't run concurrently with the code that unlinks an init file\n\t * and sends SI messages, so grab a serialization lock for the duration.\n\t */\n\tLWLockAcquire(RelCacheInitLock, LW_EXCLUSIVE);\n\n\t/* Make sure we have seen all incoming SI messages */\n\tAcceptInvalidationMessages();\n\n\t/*\n\t * If we have received any SI relcache invals since backend start, assume\n\t * we may have written out-of-date data.\n\t */\n\tif (relcacheInvalsReceived == 0L)\n\t{\n\t\t/*\n\t\t * OK, rename the temp file to its final name, deleting any\n\t\t * previously-existing init file.\n\t\t *\n\t\t * Note: a failure here is possible under Cygwin, if some other\n\t\t * backend is holding open an unlinked-but-not-yet-gone init file. So\n\t\t * treat this as a noncritical failure; just remove the useless temp\n\t\t * file on failure.\n\t\t */\n\t\tif (rename(tempfilename, finalfilename) < 0)\n\t\t\tunlink(tempfilename);\n\t}\n\telse\n\t{\n\t\t/* Delete the already-obsolete temp file */\n\t\tunlink(tempfilename);\n\t}\n\n\tLWLockRelease(RelCacheInitLock);\n}\n\n/* write a chunk of data preceded by its length */\nstatic void\nwrite_item(const void *data, Size len, FILE *fp)\n{\n\tif (fwrite(&len, 1, sizeof(len), fp) != sizeof(len))\n\t\telog(FATAL, \"could not write init file\");\n\tif (fwrite(data, 1, len, fp) != len)\n\t\telog(FATAL, \"could not write init file\");\n}\n\n/*\n * Determine whether a given relation (identified by OID) is one of the ones\n * we should store in a relcache init file.\n *\n * We must cache all nailed rels, and for efficiency we should cache every rel\n * that supports a syscache.  The former set is almost but not quite a subset\n * of the latter. The special cases are relations where\n * RelationCacheInitializePhase2/3 chooses to nail for efficiency reasons, but\n * which do not support any syscache.\n */\nbool\nRelationIdIsInInitFile(Oid relationId)\n{\n\tif (relationId == TriggerRelidNameIndexId ||\n\t\trelationId == DatabaseNameIndexId)\n\t{\n\t\t/*\n\t\t * If this Assert fails, we don't need the applicable special case\n\t\t * anymore.\n\t\t */\n\t\tAssert(!RelationSupportsSysCache(relationId));\n\t\treturn true;\n\t}\n\treturn RelationSupportsSysCache(relationId);\n}\n\n/*\n * Invalidate (remove) the init file during commit of a transaction that\n * changed one or more of the relation cache entries that are kept in the\n * local init file.\n *\n * To be safe against concurrent inspection or rewriting of the init file,\n * we must take RelCacheInitLock, then remove the old init file, then send\n * the SI messages that include relcache inval for such relations, and then\n * release RelCacheInitLock.  This serializes the whole affair against\n * write_relcache_init_file, so that we can be sure that any other process\n * that's concurrently trying to create a new init file won't move an\n * already-stale version into place after we unlink.  Also, because we unlink\n * before sending the SI messages, a backend that's currently starting cannot\n * read the now-obsolete init file and then miss the SI messages that will\n * force it to update its relcache entries.  (This works because the backend\n * startup sequence gets into the sinval array before trying to load the init\n * file.)\n *\n * We take the lock and do the unlink in RelationCacheInitFilePreInvalidate,\n * then release the lock in RelationCacheInitFilePostInvalidate.  Caller must\n * send any pending SI messages between those calls.\n */\nvoid\nRelationCacheInitFilePreInvalidate(void)\n{\n\tchar\t\tlocalinitfname[MAXPGPATH];\n\tchar\t\tsharedinitfname[MAXPGPATH];\n\n\tif (DatabasePath)\n\t\tsnprintf(localinitfname, sizeof(localinitfname), \"%s/%s\",\n\t\t\t\t DatabasePath, RELCACHE_INIT_FILENAME);\n\tsnprintf(sharedinitfname, sizeof(sharedinitfname), \"global/%s\",\n\t\t\t RELCACHE_INIT_FILENAME);\n\n\tLWLockAcquire(RelCacheInitLock, LW_EXCLUSIVE);\n\n\t/*\n\t * The files might not be there if no backend has been started since the\n\t * last removal.  But complain about failures other than ENOENT with\n\t * ERROR.  Fortunately, it's not too late to abort the transaction if we\n\t * can't get rid of the would-be-obsolete init file.\n\t */\n\tif (DatabasePath)\n\t\tunlink_initfile(localinitfname, ERROR);\n\tunlink_initfile(sharedinitfname, ERROR);\n}\n\nvoid\nRelationCacheInitFilePostInvalidate(void)\n{\n\tLWLockRelease(RelCacheInitLock);\n}\n\n/*\n * Remove the init files during postmaster startup.\n *\n * We used to keep the init files across restarts, but that is unsafe in PITR\n * scenarios, and even in simple crash-recovery cases there are windows for\n * the init files to become out-of-sync with the database.  So now we just\n * remove them during startup and expect the first backend launch to rebuild\n * them.  Of course, this has to happen in each database of the cluster.\n */\nvoid\nRelationCacheInitFileRemove(void)\n{\n\tconst char *tblspcdir = \"pg_tblspc\";\n\tDIR\t\t   *dir;\n\tstruct dirent *de;\n\tchar\t\tpath[MAXPGPATH + 11 + get_dbid_string_length() + 1 + sizeof(GP_TABLESPACE_VERSION_DIRECTORY)];\n\n\tsnprintf(path, sizeof(path), \"global/%s\",\n\t\t\t RELCACHE_INIT_FILENAME);\n\tunlink_initfile(path, LOG);\n\n\t/* Scan everything in the default tablespace */\n\tRelationCacheInitFileRemoveInDir(\"base\");\n\n\t/* Scan the tablespace link directory to find non-default tablespaces */\n\tdir = AllocateDir(tblspcdir);\n\tif (dir == NULL)\n\t{\n\t\telog(LOG, \"could not open tablespace link directory \\\"%s\\\": %m\",\n\t\t\t tblspcdir);\n\t\treturn;\n\t}\n\n\twhile ((de = ReadDir(dir, tblspcdir)) != NULL)\n\t{\n\t\tif (strspn(de->d_name, \"0123456789\") == strlen(de->d_name))\n\t\t{\n\t\t\t/* Scan the tablespace dir for per-database dirs */\n\t\t\tsnprintf(path, sizeof(path), \"%s/%s/%s\",\n\t\t\t\t\t tblspcdir, de->d_name, GP_TABLESPACE_VERSION_DIRECTORY);\n\t\t\tRelationCacheInitFileRemoveInDir(path);\n\t\t}\n\t}\n\n\tFreeDir(dir);\n}\n\n/* Process one per-tablespace directory for RelationCacheInitFileRemove */\nstatic void\nRelationCacheInitFileRemoveInDir(const char *tblspcpath)\n{\n\tDIR\t\t   *dir;\n\tstruct dirent *de;\n\tchar\t\tinitfilename[MAXPGPATH * 2];\n\n\t/* Scan the tablespace directory to find per-database directories */\n\tdir = AllocateDir(tblspcpath);\n\tif (dir == NULL)\n\t{\n\t\telog(LOG, \"could not open tablespace directory \\\"%s\\\": %m\",\n\t\t\t tblspcpath);\n\t\treturn;\n\t}\n\n\twhile ((de = ReadDir(dir, tblspcpath)) != NULL)\n\t{\n\t\tif (strspn(de->d_name, \"0123456789\") == strlen(de->d_name))\n\t\t{\n\t\t\t/* Try to remove the init file in each database */\n\t\t\tsnprintf(initfilename, sizeof(initfilename), \"%s/%s/%s\",\n\t\t\t\t\t tblspcpath, de->d_name, RELCACHE_INIT_FILENAME);\n\t\t\tunlink_initfile(initfilename, LOG);\n\t\t}\n\t}\n\n\tFreeDir(dir);\n}\n\nstatic void\nunlink_initfile(const char *initfilename, int elevel)\n{\n\tif (unlink(initfilename) < 0)\n\t{\n\t\t/* It might not be there, but log any error other than ENOENT */\n\t\tif (errno != ENOENT)\n\t\t\tereport(elevel,\n\t\t\t\t\t(errcode_for_file_access(),\n\t\t\t\t\t errmsg(\"could not remove cache file \\\"%s\\\": %m\",\n\t\t\t\t\t\t\tinitfilename)));\n\t}\n}\n",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/cache/relcache.c:5617"
    },
    {
        "log": "ereport(FATAL, ...)",
        "severity_level": "FATAL",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/utils/test/session_state_test.c:377"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/sort/logtape.c:70"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/sort/logtape.c:239"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/sort/logtape.c:258"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/sort/logtape.c:466"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/sort/tuplestore.c:235"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/sort/tuplesort_mk.c:461"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/sort/tuplesort.c:499"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/mmgr/psprintf.c:44"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/mmgr/psprintf.c:92"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/mmgr/portalmem.c:320"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/mmgr/runaway_cleaner.c:107"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/mmgr/mcxt.c:373"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/mmgr/mcxt.c:1138"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/fmgr/dfmgr.c:91"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/mb/wchar.c:1885"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/mb/mbutils.c:1100"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/mb/mbutils.c:1156"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/error/elog.c:1339"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/error/elog.c:1736"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/utils/error/elog.c:1792"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/error/elog.c:2093"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/utils/error/assert.c:16"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/parser/scan.l:1014"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/parser/parse_type.c:753"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/parser/parse_relation.c:79"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/parser/parse_clause.c:1955"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/parser/parse_node.c:136"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/parser/parse_node.c:183"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/lmgr/lmgr.c:334"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/storage/lmgr/predicate.c:1686"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/lmgr/predicate.c:4601"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/lmgr/lwlock.c:1316"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/lmgr/proc.c:800"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/lmgr/lock.c:618"
    },
    {
        "log": "elog(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/storage/lmgr/lock.c:1859"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/lmgr/lock.c:2023"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/file/buffile.c:28"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/file/fd.c:777"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/file/fd.c:2146"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/ipc/ipc.c:39"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/ipc/ipc.c:40"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/ipc/ipc.c:167"
    },
    {
        "log": "elog(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/storage/ipc/ipc.c:192"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/ipc/ipc.c:223"
    },
    {
        "log": "ereport(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/ipc/ipc.c:223"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/buffer/bufmgr.c:2108"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/storage/buffer/bufmgr.c:2610"
    },
    {
        "log": "elog(ERROR,\n#else\n\t\t\telog(LOG,\n#endif\n\t\t\t\t\"invalid in-recovery message %s \"\n\t\t\t\t\"(content=%d, dbid=%d) state=%d\",\n\t\t\t\tPQerrorMessage(ftsInfo->conn),\n\t\t\t\tftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\tftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\tftsInfo->state);\n\t\t\treturn;\n\t\t}\n\t\ttmpptr = ((uint64) tmp_xlogid) << 32 | (uint64) tmp_xrecoff;\n\n\t\t/*\n\t\t * If the xlog record returned from the primary is less than or\n\t\t * equal to the xlog record we had saved from the last probe\n\t\t * then we assume that recovery is not making progress. In the\n\t\t * case of rolling panics on the primary the returned xlog\n\t\t * location can be less than the recorded xlog location. In\n\t\t * these cases of rolling panic or recovery hung we want to\n\t\t * mark the primary as down.\n\t\t */\n\t\tif (tmpptr <= ftsInfo->xlogrecptr)\n\t\t{\n\t\t\tftsInfo->restart_state = PM_IN_RECOVERY_NOT_MAKING_PROGRESS;\n\t\t\telog(LOG, \"FTS: detected segment is in recovery mode and not making progress (content=%d) \"\n\t\t\t\t \"primary dbid=%d, mirror dbid=%d\",\n\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t ftsInfo->mirror_cdbinfo->config->dbid);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tftsInfo->restart_state = PM_IN_RECOVERY_MAKING_PROGRESS;\n\t\t\tftsInfo->xlogrecptr = tmpptr;\n\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t   \"FTS: detected segment is in recovery mode replayed (%X/%X) (content=%d) \"\n\t\t\t\t   \"primary dbid=%d, mirror dbid=%d\",\n\t\t\t\t   (uint32) (tmpptr >> 32),\n\t\t\t\t   (uint32) tmpptr,\n\t\t\t\t   ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t   ftsInfo->mirror_cdbinfo->config->dbid,\n\t\t\t\t   ftsInfo->mirror_cdbinfo->config->dbid);\n\t\t}\n\t}\n\telse if (strstr(PQerrorMessage(ftsInfo->conn), _(POSTMASTER_IN_RESET_MSG)))\n\t{\n\t\tftsInfo->restart_state = PM_IN_RESETTING;\n\t\telog(LOG, \"FTS: detected segment is in RESET state (content=%d) \"\n\t\t\t\t   \"primary dbid=%d, mirror dbid=%d\",\n\t\t\t\t   ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t   ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t   ftsInfo->mirror_cdbinfo->config->dbid);\n\t}\n}\n\n/*\n * Start a libpq connection for each \"per segment\" object in context.  If the\n * connection is already started for an object, advance libpq state machine for\n * that object by calling PQconnectPoll().  An established libpq connection\n * (authentication complete and ready-for-query received) is identified by: (1)\n * state of the \"per segment\" object is any of FTS_PROBE_SEGMENT,\n * FTS_SYNCREP_OFF_SEGMENT, FTS_PROMOTE_SEGMENT and (2) PQconnectPoll() returns\n * PGRES_POLLING_OK for the connection.\n *\n * Upon failure, transition that object to a failed state.\n */\nstatic void\nftsConnect(fts_context *context)\n{\n\tint i;\n\tfor (i = 0; i < context->num_pairs; i++)\n\t{\n\t\tfts_segment_info *ftsInfo = &context->perSegInfos[i];\n\t\telogif(gp_log_fts == GPVARS_VERBOSITY_DEBUG, LOG,\n\t\t\t   \"FTS: ftsConnect (content=%d, dbid=%d) state=%d, \"\n\t\t\t   \"retry_count=%d, conn->status=%d\",\n\t\t\t   ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t   ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t   ftsInfo->state, ftsInfo->retry_count,\n\t\t\t   ftsInfo->conn ? ftsInfo->conn->status : -1);\n\t\tif (ftsInfo->conn && PQstatus(ftsInfo->conn) == CONNECTION_OK)\n\t\t\tcontinue;\n\t\tswitch(ftsInfo->state)\n\t\t{\n\t\t\tcase FTS_PROBE_SEGMENT:\n\t\t\tcase FTS_SYNCREP_OFF_SEGMENT:\n\t\t\tcase FTS_PROMOTE_SEGMENT:\n\t\t\t\t/*\n\t\t\t\t * We always default to PM_NOT_IN_RESTART.  If connect fails, we then check\n\t\t\t\t * the primary's restarting state, so we can skip promoting mirror if it's in\n\t\t\t\t * PM_IN_RESETTING or PM_IN_RECOVERY_MAKING_PROGRESS.\n\t\t\t\t */\n\t\t\t\tftsInfo->restart_state = PM_NOT_IN_RESTART;\n\t\t\t\tif (ftsInfo->conn == NULL)\n\t\t\t\t{\n\t\t\t\t\tAssertImply(ftsInfo->retry_count > 0,\n\t\t\t\t\t\t\t\tftsInfo->retry_count <= gp_fts_probe_retries);\n\t\t\t\t\tif (!ftsConnectStart(ftsInfo))\n\t\t\t\t\t\tftsInfo->state = nextFailedState(ftsInfo->state);\n\t\t\t\t}\n\t\t\t\telse if (ftsInfo->poll_revents & (POLLOUT | POLLIN))\n\t\t\t\t{\n\t\t\t\t\tswitch(PQconnectPoll(ftsInfo->conn))\n\t\t\t\t\t{\n\t\t\t\t\t\tcase PGRES_POLLING_OK:\n\t\t\t\t\t\t\t/*\n\t\t\t\t\t\t\t * Response-state is already set and now the\n\t\t\t\t\t\t\t * connection is also ready with authentication\n\t\t\t\t\t\t\t * completed.  Next step should now be able to send\n\t\t\t\t\t\t\t * the appropriate FTS message.\n\t\t\t\t\t\t\t */\n\t\t\t\t\t\t\telogif(gp_log_fts == GPVARS_VERBOSITY_DEBUG, LOG,\n\t\t\t\t\t\t\t\t   \"FTS: established libpq connection \"\n\t\t\t\t\t\t\t\t   \"(content=%d, dbid=%d) state=%d, \"\n\t\t\t\t\t\t\t\t   \"retry_count=%d, conn->status=%d\",\n\t\t\t\t\t\t\t\t   ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t\t\t\t   ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t\t\t\t\t   ftsInfo->state, ftsInfo->retry_count,\n\t\t\t\t\t\t\t\t   ftsInfo->conn->status);\n\t\t\t\t\t\t\tftsInfo->poll_events = POLLOUT;\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase PGRES_POLLING_READING:\n\t\t\t\t\t\t\t/*\n\t\t\t\t\t\t\t * The connection can now be polled for reading and\n\t\t\t\t\t\t\t * if the poll() returns POLLIN in revents, data\n\t\t\t\t\t\t\t * has arrived.\n\t\t\t\t\t\t\t */\n\t\t\t\t\t\t\tftsInfo->poll_events |= POLLIN;\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase PGRES_POLLING_WRITING:\n\t\t\t\t\t\t\t/*\n\t\t\t\t\t\t\t * The connection can now be polled for writing and\n\t\t\t\t\t\t\t * may be written to, if ready.\n\t\t\t\t\t\t\t */\n\t\t\t\t\t\t\tftsInfo->poll_events |= POLLOUT;\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tcase PGRES_POLLING_FAILED:\n\t\t\t\t\t\t\tftsInfo->state = nextFailedState(ftsInfo->state);\n\t\t\t\t\t\t\tcheckIfFailedDueToNormalRestart(ftsInfo);\n\t\t\t\t\t\t\telog(LOG, \"FTS: cannot establish libpq connection \"\n\t\t\t\t\t\t\t\t \"(content=%d, dbid=%d): %s, retry_count=%d\",\n\t\t\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t\t\t\t\t PQerrorMessage(ftsInfo->conn),\n\t\t\t\t\t\t\t\t ftsInfo->retry_count);\n\t\t\t\t\t\t\tbreak;\n\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\telog(ERROR, \"FTS: invalid response to PQconnectPoll\"\n\t\t\t\t\t\t\t\t \" (content=%d, dbid=%d): %s\",\n\t\t\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t\t\t\t\t PQerrorMessage(ftsInfo->conn));\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t\t   \"FTS: ftsConnect (content=%d, dbid=%d) state=%d, \"\n\t\t\t\t\t\t   \"retry_count=%d, conn->status=%d pollfd.revents unset\",\n\t\t\t\t\t\t   ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t\t   ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t\t\t   ftsInfo->state, ftsInfo->retry_count,\n\t\t\t\t\t\t   ftsInfo->conn ? ftsInfo->conn->status : -1);\n\t\t\t\tbreak;\n\t\t\tcase FTS_PROBE_SUCCESS:\n\t\t\tcase FTS_SYNCREP_OFF_SUCCESS:\n\t\t\tcase FTS_PROMOTE_SUCCESS:\n\t\t\tcase FTS_PROBE_FAILED:\n\t\t\tcase FTS_SYNCREP_OFF_FAILED:\n\t\t\tcase FTS_PROMOTE_FAILED:\n\t\t\tcase FTS_PROBE_RETRY_WAIT:\n\t\t\tcase FTS_SYNCREP_OFF_RETRY_WAIT:\n\t\t\tcase FTS_PROMOTE_RETRY_WAIT:\n\t\t\tcase FTS_RESPONSE_PROCESSED:\n\t\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * Timeout is said to have occurred if greater than gp_fts_probe_timeout\n * seconds have elapsed since connection start and a response is not received.\n * Segments for which a response is received already are exempted from timeout\n * evaluation.\n */\nstatic void\nftsCheckTimeout(fts_segment_info *ftsInfo, pg_time_t now)\n{\n\tif (!IsFtsMessageStateSuccess(ftsInfo->state) &&\n\t\t(int) (now - ftsInfo->startTime) > gp_fts_probe_timeout)\n\t{\n\t\telog(LOG,\n\t\t\t \"FTS timeout detected for (content=%d, dbid=%d) \"\n\t\t\t \"state=%d, retry_count=%d,\",\n\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t ftsInfo->primary_cdbinfo->config->dbid, ftsInfo->state,\n\t\t\t ftsInfo->retry_count);\n\t\tftsInfo->state = nextFailedState(ftsInfo->state);\n\t}\n}\n\nstatic void\nftsPoll(fts_context *context)\n{\n\tint i;\n\tint nfds=0;\n\tint nready;\n\tfor (i = 0; i < context->num_pairs; i++)\n\t{\n\t\tfts_segment_info *ftsInfo = &context->perSegInfos[i];\n\t\tif (ftsInfo->poll_events & (POLLIN|POLLOUT))\n\t\t{\n\t\t\tPollFds[nfds].fd = PQsocket(ftsInfo->conn);\n\t\t\tPollFds[nfds].events = ftsInfo->poll_events;\n\t\t\tPollFds[nfds].revents = 0;\n\t\t\tftsInfo->fd_index = nfds;\n\t\t\tnfds++;\n\t\t}\n\t\telse\n\t\t\tftsInfo->fd_index = -1; /*\n\t\t\t\t\t\t\t\t\t * This socket is not considered for\n\t\t\t\t\t\t\t\t\t * polling.\n\t\t\t\t\t\t\t\t\t */\n\t}\n\tif (nfds == 0)\n\t\treturn;\n\n\tnready = poll(PollFds, nfds, 50);\n\tif (nready < 0)\n\t{\n\t\tif (errno == EINTR)\n\t\t{\n\t\t\telogif(gp_log_fts == GPVARS_VERBOSITY_DEBUG, LOG,\n\t\t\t\t   \"FTS: ftsPoll() interrupted, nfds %d\", nfds);\n\t\t}\n\t\telse\n\t\t\telog(ERROR, \"FTS: ftsPoll() failed: nfds %d, %m\", nfds);\n\t}\n\telse if (nready == 0)\n\t{\n\t\telogif(gp_log_fts == GPVARS_VERBOSITY_DEBUG, LOG,\n\t\t\t   \"FTS: ftsPoll() timed out, nfds %d\", nfds);\n\t}\n\n\telogif(gp_log_fts == GPVARS_VERBOSITY_DEBUG, LOG,\n\t\t   \"FTS: ftsPoll() found %d out of %d sockets ready\",\n\t\t   nready, nfds);\n\n\tpg_time_t now = (pg_time_t) time(NULL);\n\n\t/* Record poll() response poll_revents for each \"per_segment\" object. */\n\tfor (i = 0; i < context->num_pairs; i++)\n\t{\n\t\tfts_segment_info *ftsInfo = &context->perSegInfos[i];\n\n\t\tif (ftsInfo->poll_events & (POLLIN|POLLOUT))\n\t\t{\n\t\t\tAssert(PollFds[ftsInfo->fd_index].fd == PQsocket(ftsInfo->conn));\n\t\t\tftsInfo->poll_revents = PollFds[ftsInfo->fd_index].revents;\n\t\t\t/*\n\t\t\t * Reset poll_events for fds that were found ready.  Assume\n\t\t\t * that at the most one bit is set in poll_events (POLLIN\n\t\t\t * or POLLOUT).\n\t\t\t */\n\t\t\tif (ftsInfo->poll_revents & ftsInfo->poll_events)\n\t\t\t{\n\t\t\t\tftsInfo->poll_events = 0;\n\t\t\t}\n\t\t\telse if (ftsInfo->poll_revents & (POLLHUP | POLLERR))\n\t\t\t{\n\t\t\t\tftsInfo->state = nextFailedState(ftsInfo->state);\n\t\t\t\telog(LOG,\n\t\t\t\t\t \"FTS poll failed (revents=%d, events=%d) for \"\n\t\t\t\t\t \"(content=%d, dbid=%d) state=%d, retry_count=%d, \"\n\t\t\t\t\t \"libpq status=%d, asyncStatus=%d\",\n\t\t\t\t\t ftsInfo->poll_revents, ftsInfo->poll_events,\n\t\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid, ftsInfo->state,\n\t\t\t\t\t ftsInfo->retry_count, ftsInfo->conn->status,\n\t\t\t\t\t ftsInfo->conn->asyncStatus);\n\t\t\t}\n\t\t\telse if (ftsInfo->poll_revents)\n\t\t\t{\n\t\t\t\tftsInfo->state = nextFailedState(ftsInfo->state);\n\t\t\t\telog(LOG,\n\t\t\t\t\t \"FTS unexpected events (revents=%d, events=%d) for \"\n\t\t\t\t\t \"(content=%d, dbid=%d) state=%d, retry_count=%d, \"\n\t\t\t\t\t \"libpq status=%d, asyncStatus=%d\",\n\t\t\t\t\t ftsInfo->poll_revents, ftsInfo->poll_events,\n\t\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid, ftsInfo->state,\n\t\t\t\t\t ftsInfo->retry_count, ftsInfo->conn->status,\n\t\t\t\t\t ftsInfo->conn->asyncStatus);\n\t\t\t}\n\t\t\t/* If poll timed-out above, check timeout */\n\t\t\tftsCheckTimeout(ftsInfo, now);\n\t\t}\n\t}\n}\n\n/*\n * Send FTS query\n */\nstatic void\nftsSend(fts_context *context)\n{\n\tfts_segment_info *ftsInfo;\n\tconst char *message_type;\n\tchar message[FTS_MSG_MAX_LEN];\n\tint i;\n\n\tfor (i = 0; i < context->num_pairs; i++)\n\t{\n\t\tftsInfo = &context->perSegInfos[i];\n\t\telogif(gp_log_fts == GPVARS_VERBOSITY_DEBUG, LOG,\n\t\t\t   \"FTS: ftsSend (content=%d, dbid=%d) state=%d, \"\n\t\t\t   \"retry_count=%d, conn->asyncStatus=%d\",\n\t\t\t   ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t   ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t   ftsInfo->state, ftsInfo->retry_count,\n\t\t\t   ftsInfo->conn ? ftsInfo->conn->asyncStatus : -1);\n\t\tswitch(ftsInfo->state)\n\t\t{\n\t\t\tcase FTS_PROBE_SEGMENT:\n\t\t\tcase FTS_SYNCREP_OFF_SEGMENT:\n\t\t\tcase FTS_PROMOTE_SEGMENT:\n\t\t\t\t/*\n\t\t\t\t * The libpq connection must be ready for accepting query and\n\t\t\t\t * the socket must be writable.\n\t\t\t\t */\n\t\t\t\tif (PQstatus(ftsInfo->conn) != CONNECTION_OK ||\n\t\t\t\t\tftsInfo->conn->asyncStatus != PGASYNC_IDLE ||\n\t\t\t\t    !(ftsInfo->poll_revents & POLLOUT))\n\t\t\t\t\tbreak;\n\t\t\t\tif (ftsInfo->state == FTS_PROBE_SEGMENT)\n\t\t\t\t\tmessage_type = FTS_MSG_PROBE;\n\t\t\t\telse if (ftsInfo->state == FTS_SYNCREP_OFF_SEGMENT)\n\t\t\t\t\tmessage_type = FTS_MSG_SYNCREP_OFF;\n\t\t\t\telse\n\t\t\t\t\tmessage_type = FTS_MSG_PROMOTE;\n\n\t\t\t\tsnprintf(message, FTS_MSG_MAX_LEN, FTS_MSG_FORMAT,\n\t\t\t\t\t\t message_type,\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex);\n\n\t\t\t\tif (PQsendQuery(ftsInfo->conn, message))\n\t\t\t\t{\n\t\t\t\t\t/*\n\t\t\t\t\t * Message sent successfully, mark the socket to be polled\n\t\t\t\t\t * for reading so we will be ready to read response when it\n\t\t\t\t\t * arrives.\n\t\t\t\t\t */\n\t\t\t\t\tftsInfo->poll_events = POLLIN;\n\t\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t\t   \"FTS sent %s to (content=%d, dbid=%d), retry_count=%d\",\n\t\t\t\t\t\t   message, ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t\t   ftsInfo->primary_cdbinfo->config->dbid, ftsInfo->retry_count);\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\telog(LOG,\n\t\t\t\t\t\t \"FTS: failed to send %s to segment (content=%d, \"\n\t\t\t\t\t\t \"dbid=%d) state=%d, retry_count=%d, \"\n\t\t\t\t\t\t \"conn->asyncStatus=%d %s\", message,\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t\t\t ftsInfo->state, ftsInfo->retry_count,\n\t\t\t\t\t\t ftsInfo->conn->asyncStatus,\n\t\t\t\t\t\t PQerrorMessage(ftsInfo->conn));\n\t\t\t\t\tftsInfo->state = nextFailedState(ftsInfo->state);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/* Cannot send messages in any other state. */\n\t\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * Record FTS handler's response from libpq result into fts_result\n */\nstatic void\nprobeRecordResponse(fts_segment_info *ftsInfo, PGresult *result)\n{\n\tftsInfo->result.isPrimaryAlive = true;\n\n\tftsInfo->result.isMirrorAlive = *PQgetvalue(result, 0,\n\t\t\tAnum_fts_message_response_is_mirror_up);\n\n\tftsInfo->result.isInSync = *PQgetvalue(result, 0,\n\t\t\tAnum_fts_message_response_is_in_sync);\n\n\tftsInfo->result.isSyncRepEnabled = *PQgetvalue(result, 0,\n\t\t\tAnum_fts_message_response_is_syncrep_enabled);\n\n\tftsInfo->result.isRoleMirror = *PQgetvalue(result, 0,\n\t\t\tAnum_fts_message_response_is_role_mirror);\n\n\tftsInfo->result.retryRequested = *PQgetvalue(result, 0,\n\t\t\tAnum_fts_message_response_request_retry);\n\n\telogif(gp_log_fts >= GPVARS_VERBOSITY_DEBUG, LOG,\n\t\t   \"FTS: segment (content=%d, dbid=%d, role=%c) reported \"\n\t\t   \"isMirrorUp %d, isInSync %d, isSyncRepEnabled %d, \"\n\t\t   \"isRoleMirror %d, and retryRequested %d to the prober.\",\n\t\t   ftsInfo->primary_cdbinfo->config->segindex,\n\t\t   ftsInfo->primary_cdbinfo->config->dbid,\n\t\t   ftsInfo->primary_cdbinfo->config->role,\n\t\t   ftsInfo->result.isMirrorAlive,\n\t\t   ftsInfo->result.isInSync,\n\t\t   ftsInfo->result.isSyncRepEnabled,\n\t\t   ftsInfo->result.isRoleMirror,\n\t\t   ftsInfo->result.retryRequested);\n}\n\n/*\n * Receive segment response\n */\nstatic void\nftsReceive(fts_context *context)\n{\n\tfts_segment_info *ftsInfo;\n\tPGresult *result = NULL;\n\tint ntuples;\n\tint nfields;\n\tint i;\n\n\tfor (i = 0; i < context->num_pairs; i++)\n\t{\n\t\tftsInfo = &context->perSegInfos[i];\n\t\telogif(gp_log_fts == GPVARS_VERBOSITY_DEBUG, LOG,\n\t\t\t   \"FTS: ftsReceive (content=%d, dbid=%d) state=%d, \"\n\t\t\t   \"retry_count=%d, conn->asyncStatus=%d\",\n\t\t\t   ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t   ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t   ftsInfo->state, ftsInfo->retry_count,\n\t\t\t   ftsInfo->conn ? ftsInfo->conn->asyncStatus : -1);\n\t\tswitch(ftsInfo->state)\n\t\t{\n\t\t\tcase FTS_PROBE_SEGMENT:\n\t\t\tcase FTS_SYNCREP_OFF_SEGMENT:\n\t\t\tcase FTS_PROMOTE_SEGMENT:\n\t\t\t\t/*\n\t\t\t\t * The libpq connection must be established and a message must\n\t\t\t\t * have arrived on the socket.\n\t\t\t\t */\n\t\t\t\tif (PQstatus(ftsInfo->conn) != CONNECTION_OK ||\n\t\t\t\t\t!(ftsInfo->poll_revents & POLLIN))\n\t\t\t\t\tbreak;\n\t\t\t\t/* Read the response that has arrived. */\n\t\t\t\tif (!PQconsumeInput(ftsInfo->conn))\n\t\t\t\t{\n\t\t\t\t\telog(LOG, \"FTS: failed to read from (content=%d, dbid=%d)\"\n\t\t\t\t\t\t \" state=%d, retry_count=%d, conn->asyncStatus=%d %s\",\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t\t\t ftsInfo->state, ftsInfo->retry_count,\n\t\t\t\t\t\t ftsInfo->conn->asyncStatus,\n\t\t\t\t\t\t PQerrorMessage(ftsInfo->conn));\n\t\t\t\t\tftsInfo->state = nextFailedState(ftsInfo->state);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/* Parse the response. */\n\t\t\t\tif (PQisBusy(ftsInfo->conn))\n\t\t\t\t{\n\t\t\t\t\t/*\n\t\t\t\t\t * There is not enough data in the buffer.\n\t\t\t\t\t */\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\t/*\n\t\t\t\t * Response parsed, PQgetResult() should not block for I/O now.\n\t\t\t\t */\n\t\t\t\tresult = PQgetResult(ftsInfo->conn);\n\n\t\t\t\tif (!result || PQstatus(ftsInfo->conn) == CONNECTION_BAD)\n\t\t\t\t{\n\t\t\t\t\telog(LOG, \"FTS: error getting results from (content=%d, \"\n\t\t\t\t\t\t \"dbid=%d) state=%d, retry_count=%d, \"\n\t\t\t\t\t\t \"conn->asyncStatus=%d conn->status=%d %s\",\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t\t\t ftsInfo->state, ftsInfo->retry_count,\n\t\t\t\t\t\t ftsInfo->conn->asyncStatus,\n\t\t\t\t\t\t ftsInfo->conn->status,\n\t\t\t\t\t\t PQerrorMessage(ftsInfo->conn));\n\t\t\t\t\tftsInfo->state = nextFailedState(ftsInfo->state);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (PQresultStatus(result) != PGRES_TUPLES_OK)\n\t\t\t\t{\n\t\t\t\t\telog(LOG, \"FTS: error response from (content=%d, dbid=%d)\"\n\t\t\t\t\t\t \" state=%d, retry_count=%d, conn->asyncStatus=%d %s\",\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t\t\t ftsInfo->state, ftsInfo->retry_count,\n\t\t\t\t\t\t ftsInfo->conn->asyncStatus,\n\t\t\t\t\t\t PQresultErrorMessage(result));\n\t\t\t\t\tftsInfo->state = nextFailedState(ftsInfo->state);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tntuples = PQntuples(result);\n\t\t\t\tnfields = PQnfields(result);\n\t\t\t\tif (nfields != Natts_fts_message_response ||\n\t\t\t\t\tntuples != FTS_MESSAGE_RESPONSE_NTUPLES)\n\t\t\t\t{\n\t\t\t\t\t/*\n\t\t\t\t\t * XXX: Investigate: including conn->asyncStatus generated\n\t\t\t\t\t * a format string warning at compile time.\n\t\t\t\t\t */\n\t\t\t\t\telog(LOG, \"FTS: invalid response from (content=%d, dbid=%d)\"\n\t\t\t\t\t\t \" state=%d, retry_count=%d, expected %d tuple with \"\n\t\t\t\t\t\t \"%d fields, got %d tuples with %d fields\",\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t\t ftsInfo->primary_cdbinfo->config->dbid,\n\t\t\t\t\t\t ftsInfo->state, ftsInfo->retry_count,\n\t\t\t\t\t\t FTS_MESSAGE_RESPONSE_NTUPLES,\n\t\t\t\t\t\t Natts_fts_message_response, ntuples, nfields);\n\t\t\t\t\tftsInfo->state = nextFailedState(ftsInfo->state);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t/*\n\t\t\t\t * Result received and parsed successfully.  Record it so that\n\t\t\t\t * subsequent step processes it and transitions to next state.\n\t\t\t\t */\n\t\t\t\tprobeRecordResponse(ftsInfo, result);\n\t\t\t\tftsInfo->state = nextSuccessState(ftsInfo->state);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\t/* Cannot receive response in any other state. */\n\t\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * Reference to the result should already be stored in\n\t\t * connection object. If it is not then free explicitly.\n\t\t */\n\t\tif (result && result != ftsInfo->conn->result)\n\t\t{\n\t\t\tPQclear(result);\n\t\t\tresult = NULL;\n\t\t}\n\t}\n}\n\nstatic void\nretryForFtsFailed(fts_segment_info *ftsInfo, pg_time_t now)\n{\n\tif (ftsInfo->retry_count == gp_fts_probe_retries)\n\t{\n\t\telog(LOG, \"FTS max (%d) retries exhausted \"\n\t\t\t\"(content=%d, dbid=%d) state=%d\",\n\t\t\tftsInfo->retry_count,\n\t\t\tftsInfo->primary_cdbinfo->config->segindex,\n\t\t\tftsInfo->primary_cdbinfo->config->dbid, ftsInfo->state);\n\t\treturn;\n\t}\n\n\tftsInfo->retry_count++;\n\tif (ftsInfo->state == FTS_PROBE_SUCCESS ||\n\t\tftsInfo->state == FTS_PROBE_FAILED)\n\t\tftsInfo->state = FTS_PROBE_RETRY_WAIT;\n\telse if (ftsInfo->state == FTS_SYNCREP_OFF_FAILED)\n\t\tftsInfo->state = FTS_SYNCREP_OFF_RETRY_WAIT;\n\telse\n\t\tftsInfo->state = FTS_PROMOTE_RETRY_WAIT;\n\tftsInfo->retryStartTime = now;\n\telogif(gp_log_fts == GPVARS_VERBOSITY_DEBUG, LOG,\n\t\t\"FTS initialized retry start time to now \"\n\t\t\"(content=%d, dbid=%d) state=%d\",\n\t\tftsInfo->primary_cdbinfo->config->segindex,\n\t\tftsInfo->primary_cdbinfo->config->dbid, ftsInfo->state);\n\n\tPQfinish(ftsInfo->conn);\n\tftsInfo->conn = NULL;\n\tftsInfo->poll_events = ftsInfo->poll_revents = 0;\n\t/* Reset result before next attempt. */\n\tmemset(&ftsInfo->result, 0, sizeof(fts_result));\n}\n\n/*\n * If retry attempts are available, transition the sgement to the start state\n * corresponding to their failure state.  If retries have exhausted, leave the\n * segment in the failure state.\n */\nstatic void\nprocessRetry(fts_context *context)\n{\n\tfts_segment_info *ftsInfo;\n\tint i;\n\tpg_time_t now = (pg_time_t) time(NULL);\n\n\tfor (i = 0; i < context->num_pairs; i++)\n\t{\n\t\tftsInfo = &context->perSegInfos[i];\n\t\tswitch(ftsInfo->state)\n\t\t{\n\t\t\tcase FTS_PROBE_SUCCESS:\n\t\t\t\t/*\n\t\t\t\t * Purpose of retryRequested flag is to avoid considering\n\t\t\t\t * mirror as down prematurely.  If mirror is already marked\n\t\t\t\t * down in configuration, there is no need to retry.\n\t\t\t\t */\n\t\t\t\tif (!(ftsInfo->result.retryRequested &&\n\t\t\t\t\t  SEGMENT_IS_ALIVE(ftsInfo->mirror_cdbinfo)))\n\t\t\t\t\tbreak;\n\t\t\t\t/* else, fallthrough */\n\t\t\tcase FTS_PROBE_FAILED:\n\t\t\tcase FTS_SYNCREP_OFF_FAILED:\n\t\t\tcase FTS_PROMOTE_FAILED:\n\t\t\t\tretryForFtsFailed(ftsInfo, now);\n\t\t\t\tbreak;\n\t\t\tcase FTS_PROBE_RETRY_WAIT:\n\t\t\tcase FTS_SYNCREP_OFF_RETRY_WAIT:\n\t\t\tcase FTS_PROMOTE_RETRY_WAIT:\n\t\t\t\t/* Wait for 1 second before making another attempt. */\n\t\t\t\tif ((int) (now - ftsInfo->retryStartTime) < 1)\n\t\t\t\t\tbreak;\n\t\t\t\t/*\n\t\t\t\t * We have remained in retry state for over a second, it's time\n\t\t\t\t * to make another attempt.\n\t\t\t\t */\n\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t   \"FTS retrying attempt %d (content=%d, dbid=%d) \"\n\t\t\t\t\t   \"state=%d\", ftsInfo->retry_count,\n\t\t\t\t\t   ftsInfo->primary_cdbinfo->config->segindex,\n\t\t\t\t\t   ftsInfo->primary_cdbinfo->config->dbid, ftsInfo->state);\n\t\t\t\tif (ftsInfo->state == FTS_PROBE_RETRY_WAIT)\n\t\t\t\t\tftsInfo->state = FTS_PROBE_SEGMENT;\n\t\t\t\telse if (ftsInfo->state == FTS_SYNCREP_OFF_RETRY_WAIT)\n\t\t\t\t\tftsInfo->state = FTS_SYNCREP_OFF_SEGMENT;\n\t\t\t\telse\n\t\t\t\t\tftsInfo->state = FTS_PROMOTE_SEGMENT;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t}\n\t}\n}\n\n/*\n * Return true for segments whose response is ready to be processed.  Segments\n * whose response is already processed should have response->conn set to NULL.\n */\nstatic bool\nftsResponseReady(fts_segment_info *ftsInfo)\n{\n\treturn (IsFtsMessageStateSuccess(ftsInfo->state) ||\n\t\t\tIsFtsMessageStateFailed(ftsInfo->state));\n}\n\nstatic bool\nupdateConfiguration(CdbComponentDatabaseInfo *primary,\n\t\t\t\t\tCdbComponentDatabaseInfo *mirror,\n\t\t\t\t\tchar newPrimaryRole, char newMirrorRole,\n\t\t\t\t\tbool IsInSync, bool IsPrimaryAlive, bool IsMirrorAlive)\n{\n\tbool UpdatePrimary = (IsPrimaryAlive != SEGMENT_IS_ALIVE(primary));\n\tbool UpdateMirror = (IsMirrorAlive != SEGMENT_IS_ALIVE(mirror));\n\n\t/*\n\t * If probe response state is different from current state in\n\t * configuration, update both primary and mirror.\n\t */\n\tif (IsInSync != SEGMENT_IS_IN_SYNC(primary))\n\t\tUpdatePrimary = UpdateMirror = true;\n\n\t/*\n\t * A mirror being promoted must be already in-sync in configuration.\n\t * Update to the configuration must include mode as not-in-sync and primary\n\t * status as down.\n\t */\n\tAssertImply(newPrimaryRole == GP_SEGMENT_CONFIGURATION_ROLE_MIRROR,\n\t\t\t\tSEGMENT_IS_IN_SYNC(mirror) && !IsInSync && !IsPrimaryAlive);\n\n\t/*\n\t * Primary and mirror should always have the same mode in configuration,\n\t * either both reflecting in-sync or not in-sync.\n\t */\n\tAssert(primary->config->mode == mirror->config->mode);\n\n\tbool UpdateNeeded = UpdatePrimary || UpdateMirror;\n\t/*\n\t * Commit/abort transaction below will destroy\n\t * CurrentResourceOwner.  We need it for catalog reads.\n\t */\n\tResourceOwner save = CurrentResourceOwner;\n\tif (UpdateNeeded)\n\t{\n\t\tStartTransactionCommand();\n\t\tGetTransactionSnapshot();\n\n\t\tif (UpdatePrimary)\n\t\t\tprobeWalRepUpdateConfig(primary->config->dbid, primary->config->segindex,\n\t\t\t\t\t\t\t\t\tnewPrimaryRole, IsPrimaryAlive,\n\t\t\t\t\t\t\t\t\tIsInSync);\n\n\t\tif (UpdateMirror)\n\t\t\tprobeWalRepUpdateConfig(mirror->config->dbid, mirror->config->segindex,\n\t\t\t\t\t\t\t\t\tnewMirrorRole, IsMirrorAlive,\n\t\t\t\t\t\t\t\t\tIsInSync);\n\n\t\tCommitTransactionCommand();\n\t\tCurrentResourceOwner = save;\n\n\t\t/*\n\t\t * Update the status to in-memory variable as well used by\n\t\t * dispatcher, now that changes has been persisted to catalog.\n\t\t */\n\t\tAssert(ftsProbeInfo);\n\t\tftsLock();\n\t\tif (IsPrimaryAlive)\n\t\t\tFTS_STATUS_SET_UP(ftsProbeInfo->status[primary->config->dbid]);\n\t\telse\n\t\t\tFTS_STATUS_SET_DOWN(ftsProbeInfo->status[primary->config->dbid]);\n\n\t\tif (IsMirrorAlive)\n\t\t\tFTS_STATUS_SET_UP(ftsProbeInfo->status[mirror->config->dbid]);\n\t\telse\n\t\t\tFTS_STATUS_SET_DOWN(ftsProbeInfo->status[mirror->config->dbid]);\n\t\tftsUnlock();\n\t}\n\n\treturn UpdateNeeded;\n}\n\n/*\n * Process resonses from primary segments:\n * (a) Transition internal state so that segments can be messaged subsequently\n * (e.g. promotion and turning off syncrep).\n * (b) Update gp_segment_configuration catalog table, if needed.\n */\nstatic bool\nprocessResponse(fts_context *context)\n{\n\tbool is_updated = false;\n\n\tfor (int response_index = 0;\n\t\t response_index < context->num_pairs && FtsIsActive();\n\t\t response_index ++)\n\t{\n\t\tfts_segment_info *ftsInfo = &(context->perSegInfos[response_index]);\n\n\t\t/*\n\t\t * Consider segments that are in final state (success / failure) and\n\t\t * that are not already processed.\n\t\t */\n\t\tif (!ftsResponseReady(ftsInfo))\n\t\t\tcontinue;\n\n\t\t/* All retries must have exhausted before a failure is processed. */\n\t\tAssertImply(IsFtsMessageStateFailed(ftsInfo->state),\n\t\t\t\t\tftsInfo->retry_count == gp_fts_probe_retries);\n\n\t\tCdbComponentDatabaseInfo *primary = ftsInfo->primary_cdbinfo;\n\n\t\tCdbComponentDatabaseInfo *mirror = ftsInfo->mirror_cdbinfo;\n\n\t\tbool IsPrimaryAlive = ftsInfo->result.isPrimaryAlive;\n\t\t/* Trust a response from primary only if it's alive. */\n\t\tbool IsMirrorAlive =  IsPrimaryAlive ?\n\t\t\tftsInfo->result.isMirrorAlive : SEGMENT_IS_ALIVE(mirror);\n\t\tbool IsInSync = IsPrimaryAlive ?\n\t\t\tftsInfo->result.isInSync : false;\n\n\t\t/* If primary and mirror are in sync, then both have to be ALIVE. */\n\t\tAssertImply(IsInSync, IsPrimaryAlive && IsMirrorAlive);\n\n\t\tswitch(ftsInfo->state)\n\t\t{\n\t\t\tcase FTS_PROBE_SUCCESS:\n\t\t\t\tAssert(IsPrimaryAlive);\n\t\t\t\tif (ftsInfo->result.isSyncRepEnabled && !IsMirrorAlive)\n\t\t\t\t{\n\t\t\t\t\tif (!ftsInfo->result.retryRequested)\n\t\t\t\t\t{\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * Primaries that have syncrep enabled continue to block\n\t\t\t\t\t\t * commits until FTS update the mirror status as down.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tis_updated |= updateConfiguration(\n\t\t\t\t\t\t\tprimary, mirror,\n\t\t\t\t\t\t\tGP_SEGMENT_CONFIGURATION_ROLE_PRIMARY,\n\t\t\t\t\t\t\tGP_SEGMENT_CONFIGURATION_ROLE_MIRROR,\n\t\t\t\t\t\t\tIsInSync, IsPrimaryAlive, IsMirrorAlive);\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * If mirror was marked up in configuration, it must have\n\t\t\t\t\t\t * been marked down by updateConfiguration().\n\t\t\t\t\t\t */\n\t\t\t\t\t\tAssertImply(SEGMENT_IS_ALIVE(mirror), is_updated);\n\t\t\t\t\t\t/*\n\t\t\t\t\t\t * Now that the configuration is updated, FTS must notify\n\t\t\t\t\t\t * the primaries to unblock commits by sending syncrep off\n\t\t\t\t\t\t * message.\n\t\t\t\t\t\t */\n\t\t\t\t\t\tftsInfo->state = FTS_SYNCREP_OFF_SEGMENT;\n\t\t\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t\t\t   \"FTS turning syncrep off on (content=%d, dbid=%d)\",\n\t\t\t\t\t\t\t   primary->config->segindex, primary->config->dbid);\n\t\t\t\t\t}\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t\t\t   \"FTS skipping mirror down update for (content=%d) as retryRequested\",\n\t\t\t\t\t\t\t   primary->config->segindex);\n\t\t\t\t\t\tftsInfo->state = FTS_RESPONSE_PROCESSED;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse if (ftsInfo->result.isRoleMirror)\n\t\t\t\t{\n\t\t\t\t\t/*\n\t\t\t\t\t * A promote message sent previously didn't make it to the\n\t\t\t\t\t * mirror.  Catalog must have been updated before sending\n\t\t\t\t\t * the previous promote message.\n\t\t\t\t\t */\n\t\t\t\t\tAssert(!IsMirrorAlive);\n\t\t\t\t\tAssert(!SEGMENT_IS_ALIVE(mirror));\n\t\t\t\t\tAssert(SEGMENT_IS_NOT_INSYNC(mirror));\n\t\t\t\t\tAssert(SEGMENT_IS_NOT_INSYNC(primary));\n\t\t\t\t\tAssert(!ftsInfo->result.isSyncRepEnabled);\n\t\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t\t   \"FTS resending promote request to (content=%d,\"\n\t\t\t\t\t\t   \" dbid=%d)\", primary->config->segindex, primary->config->dbid);\n\t\t\t\t\tftsInfo->state = FTS_PROMOTE_SEGMENT;\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\t/*\n\t\t\t\t\t * No subsequent state transition needed, update catalog if\n\t\t\t\t\t * necessary.  The cases are mirror status found to change\n\t\t\t\t\t * from down to up, mode found to change from not in-sync\n\t\t\t\t\t * to in-sync or syncrep found to change from off to on.\n\t\t\t\t\t */\n\t\t\t\t\tis_updated |= updateConfiguration(\n\t\t\t\t\t\tprimary, mirror,\n\t\t\t\t\t\tGP_SEGMENT_CONFIGURATION_ROLE_PRIMARY,\n\t\t\t\t\t\tGP_SEGMENT_CONFIGURATION_ROLE_MIRROR,\n\t\t\t\t\t\tIsInSync, IsPrimaryAlive, IsMirrorAlive);\n\t\t\t\t\tftsInfo->state = FTS_RESPONSE_PROCESSED;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase FTS_PROBE_FAILED:\n\t\t\t\t/* Primary is down */\n\n\t\t\t\t/* If primary is in resetting or making progress in recovery, do not mark it down and promote mirror */\n\t\t\t\tif (ftsInfo->restart_state == PM_IN_RESETTING)\n\t\t\t\t{\n\t\t\t\t\tAssert(strstr(PQerrorMessage(ftsInfo->conn), _(POSTMASTER_IN_RESET_MSG)));\n\t\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t\t \"FTS: detected segment is in resetting mode \"\n\t\t\t\t\t\t \"(content=%d) primary dbid=%d, mirror dbid=%d\",\n\t\t\t\t\t\t primary->config->segindex, primary->config->dbid, mirror->config->dbid);\n\n\t\t\t\t\tftsInfo->state = FTS_RESPONSE_PROCESSED;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\telse if (ftsInfo->restart_state == PM_IN_RECOVERY_MAKING_PROGRESS)\n\t\t\t\t{\n\t\t\t\t\tAssert(strstr(PQerrorMessage(ftsInfo->conn), _(POSTMASTER_IN_RECOVERY_MSG)) ||\n\t\t\t\t\t\t   strstr(PQerrorMessage(ftsInfo->conn), _(POSTMASTER_IN_STARTUP_MSG)));\n\t\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t\t \"FTS: detected segment is in recovery mode and making \"\n\t\t\t\t\t\t \"progress (content=%d) primary dbid=%d, mirror dbid=%d\",\n\t\t\t\t\t\t primary->config->segindex, primary->config->dbid, mirror->config->dbid);\n\n\t\t\t\t\tftsInfo->state = FTS_RESPONSE_PROCESSED;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tAssert(!IsPrimaryAlive);\n\t\t\t\t/* See if mirror can be promoted. */\n\t\t\t\tif (SEGMENT_IS_IN_SYNC(mirror))\n\t\t\t\t{\n\t\t\t\t\t/*\n\t\t\t\t\t * Primary and mirror must have been recorded as in-sync\n\t\t\t\t\t * before the probe.\n\t\t\t\t\t */\n\t\t\t\t\tAssert(SEGMENT_IS_IN_SYNC(primary));\n\n\t\t\t\t\t/*\n\t\t\t\t\t * Flip the roles and mark the failed primary as down in\n\t\t\t\t\t * FTS configuration before sending promote message.\n\t\t\t\t\t * Dispatcher should no longer consider the failed primary\n\t\t\t\t\t * for gang creation, FTS should no longer probe the failed\n\t\t\t\t\t * primary.\n\t\t\t\t\t */\n\t\t\t\t\tis_updated |= updateConfiguration(\n\t\t\t\t\t\tprimary, mirror,\n\t\t\t\t\t\tGP_SEGMENT_CONFIGURATION_ROLE_MIRROR, /* newPrimaryRole */\n\t\t\t\t\t\tGP_SEGMENT_CONFIGURATION_ROLE_PRIMARY, /* newMirrorRole */\n\t\t\t\t\t\tIsInSync, IsPrimaryAlive, IsMirrorAlive);\n\t\t\t\t\tAssert(is_updated);\n\n\t\t\t\t\t/*\n\t\t\t\t\t * Swap the primary and mirror references so that the\n\t\t\t\t\t * mirror will be promoted in subsequent connect, poll,\n\t\t\t\t\t * send, receive steps.\n\t\t\t\t\t */\n\t\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t\t   \"FTS promoting mirror (content=%d, dbid=%d) \"\n\t\t\t\t\t\t   \"to be the new primary\",\n\t\t\t\t\t\t   mirror->config->segindex, mirror->config->dbid);\n\t\t\t\t\tftsInfo->state = FTS_PROMOTE_SEGMENT;\n\t\t\t\t\tftsInfo->primary_cdbinfo = mirror;\n\t\t\t\t\tftsInfo->mirror_cdbinfo = primary;\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\t/*\n\t\t\t\t\t * Only log here, will handle it later, having an \"ERROR\"\n\t\t\t\t\t * keyword here for customer convenience\n\t\t\t\t\t */\n\t\t\t\t\telog(WARNING, \"ERROR: FTS double fault detected (content=%d) \"\n\t\t\t\t\t\t \"primary dbid=%d, mirror dbid=%d\",\n\t\t\t\t\t\t primary->config->segindex, primary->config->dbid, mirror->config->dbid);\n\t\t\t\t\tftsInfo->state = FTS_RESPONSE_PROCESSED;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase FTS_SYNCREP_OFF_FAILED:\n\t\t\t\t/*\n\t\t\t\t * Another attempt to turn off syncrep will be made in the next\n\t\t\t\t * probe cycle.  Until then, leave the transactions waiting for\n\t\t\t\t * syncrep.  A worse alternative is to PANIC.\n\t\t\t\t */\n\t\t\t\telog(WARNING, \"FTS failed to turn off syncrep on (content=%d,\"\n\t\t\t\t\t \" dbid=%d)\", primary->config->segindex, primary->config->dbid);\n\t\t\t\tftsInfo->state = FTS_RESPONSE_PROCESSED;\n\t\t\t\tbreak;\n\t\t\tcase FTS_PROMOTE_FAILED:\n\t\t\t\t/*\n\t\t\t\t * Only log here, will handle it later, having an \"ERROR\"\n\t\t\t\t * keyword here for customer convenience\n\t\t\t\t */\n\t\t\t\telog(WARNING, \"ERROR: FTS double fault detected (content=%d) \"\n\t\t\t\t\t \"primary dbid=%d, mirror dbid=%d\",\n\t\t\t\t\t primary->config->segindex, primary->config->dbid, mirror->config->dbid);\n\t\t\t\tftsInfo->state = FTS_RESPONSE_PROCESSED;\n\t\t\t\tbreak;\n\t\t\tcase FTS_PROMOTE_SUCCESS:\n\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t   \"FTS mirror (content=%d, dbid=%d) promotion \"\n\t\t\t\t\t   \"triggered successfully\",\n\t\t\t\t\t   primary->config->segindex, primary->config->dbid);\n\t\t\t\tftsInfo->state = FTS_RESPONSE_PROCESSED;\n\t\t\t\tbreak;\n\t\t\tcase FTS_SYNCREP_OFF_SUCCESS:\n\t\t\t\telogif(gp_log_fts >= GPVARS_VERBOSITY_VERBOSE, LOG,\n\t\t\t\t\t   \"FTS primary (content=%d, dbid=%d) notified to turn \"\n\t\t\t\t\t   \"syncrep off\", primary->config->segindex, primary->config->dbid);\n\t\t\t\tftsInfo->state = FTS_RESPONSE_PROCESSED;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\telog(ERROR, \"FTS invalid internal state %d for (content=%d)\"\n\t\t\t\t\t \"primary dbid=%d, mirror dbid=%d\", ftsInfo->state,\n\t\t\t\t\t primary->config->segindex, primary->config->dbid, mirror->config->dbid);\n\t\t\t\tbreak;\n\t\t}\n\t\t/* Close connection and reset result for next message, if any. */\n\t\tmemset(&ftsInfo->result, 0, sizeof(fts_result));\n\t\tPQfinish(ftsInfo->conn);\n\t\tftsInfo->conn = NULL;\n\t\tftsInfo->poll_events = ftsInfo->poll_revents = 0;\n\t\tftsInfo->retry_count = 0;\n\t}\n\n\treturn is_updated;\n}\n\n#ifdef USE_ASSERT_CHECKING\nstatic bool\nFtsIsSegmentAlive(CdbComponentDatabaseInfo *segInfo)\n{\n\tif (SEGMENT_IS_ACTIVE_MIRROR(segInfo) && SEGMENT_IS_ALIVE(segInfo))\n\t\treturn true;\n\n\tif (SEGMENT_IS_ACTIVE_PRIMARY(segInfo))\n\t\treturn true;\n\n\treturn false;\n}\n#endif\n\n/*\n * Initialize context before a probe cycle based on cluster configuration in\n * cdbs.\n */\nstatic void\nFtsWalRepInitProbeContext(CdbComponentDatabases *cdbs, fts_context *context)\n{\n\tcontext->num_pairs = cdbs->total_segments;\n\tcontext->perSegInfos = (fts_segment_info *) palloc0(\n\t\tcontext->num_pairs * sizeof(fts_segment_info));\n\n\tint fts_index = 0;\n\tint cdb_index = 0;\n\tfor(; cdb_index < cdbs->total_segment_dbs; cdb_index++)\n\t{\n\t\tCdbComponentDatabaseInfo *primary = &(cdbs->segment_db_info[cdb_index]);\n\t\tif (!SEGMENT_IS_ACTIVE_PRIMARY(primary))\n\t\t\tcontinue;\n\t\tCdbComponentDatabaseInfo *mirror = FtsGetPeerSegment(cdbs,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t primary->config->segindex,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t primary->config->dbid);\n\t\t/*\n\t\t * If there is no mirror under this primary, no need to probe.\n\t\t */\n\t\tif (!mirror)\n\t\t{\n\t\t\tcontext->num_pairs--;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* primary in catalog will NEVER be marked down. */\n\t\tAssert(FtsIsSegmentAlive(primary));\n\n\t\tfts_segment_info *ftsInfo = &(context->perSegInfos[fts_index]);\n\t\t/*\n\t\t * Initialize the response object.  Response from a segment will be\n\t\t * processed only if ftsInfo->state is one of SUCCESS states.  If a\n\t\t * failure is encountered in messaging a segment, its response will not\n\t\t * be processed.\n\t\t */\n\t\tftsInfo->result.isPrimaryAlive = false;\n\t\tftsInfo->result.isMirrorAlive = false;\n\t\tftsInfo->result.isInSync = false;\n\t\tftsInfo->result.isSyncRepEnabled = false;\n\t\tftsInfo->result.retryRequested = false;\n\t\tftsInfo->result.isRoleMirror = false;\n\t\tftsInfo->result.dbid = primary->config->dbid;\n\t\tftsInfo->state = FTS_PROBE_SEGMENT;\n\t\tftsInfo->restart_state = PM_NOT_IN_RESTART;\n\t\tftsInfo->xlogrecptr = InvalidXLogRecPtr;\n\n\t\tftsInfo->primary_cdbinfo = primary;\n\t\tftsInfo->mirror_cdbinfo = mirror;\n\n\t\tAssert(fts_index < context->num_pairs);\n\t\tfts_index ++;\n\t}\n}\n\nstatic void\nInitPollFds(size_t size)\n{\n\tPollFds = (struct pollfd *) palloc0(size * sizeof(struct pollfd));\n}\n\nbool\nFtsWalRepMessageSegments(CdbComponentDatabases *cdbs)\n{\n\tbool is_updated = false;\n\tfts_context context;\n\n\tFtsWalRepInitProbeContext(cdbs, &context);\n\tInitPollFds(cdbs->total_segments);\n\n\twhile (!allDone(&context) && FtsIsActive())\n\t{\n\t\tftsConnect(&context);\n\t\tftsPoll(&context);\n\t\tftsSend(&context);\n\t\tftsReceive(&context);\n\t\tprocessRetry(&context);\n\t\tis_updated |= processResponse(&context);\n\t}\n\tint i;\n\tif (!FtsIsActive())\n\t{\n\t\tfor (i = 0; i < context.num_pairs; i++)\n\t\t{\n\t\t\tif (context.perSegInfos[i].conn)\n\t\t\t{\n\t\t\t\tPQfinish(context.perSegInfos[i].conn);\n\t\t\t\tcontext.perSegInfos[i].conn = NULL;\n\t\t\t}\n\t\t}\n\t}\n#ifdef USE_ASSERT_CHECKING\n\t/*\n\t * At the end of probe cycle, there shouldn't be any active libpq\n\t * connections.\n\t */\n\tfor (i = 0; i < context.num_pairs; i++)\n\t{\n\t\tif (context.perSegInfos[i].conn != NULL)\n\t\t\telog(ERROR,\n\t\t\t\t \"FTS libpq connection left open (content=%d, dbid=%d)\"\n\t\t\t\t \" state=%d, retry_count=%d, conn->status=%d\",\n\t\t\t\t context.perSegInfos[i].primary_cdbinfo->config->segindex,\n\t\t\t\t context.perSegInfos[i].primary_cdbinfo->config->dbid,\n\t\t\t\t context.perSegInfos[i].state,\n\t\t\t\t context.perSegInfos[i].retry_count,\n\t\t\t\t context.perSegInfos[i].conn->status);\n\t}\n#endif\n\tpfree(context.perSegInfos);\n\tpfree(PollFds);\n\treturn is_updated;\n}\n\n/* EOF */\n",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/fts/ftsprobe.c:1334"
    },
    {
        "log": "ereport(...)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/gpopt/gpdbwrappers.cpp:1624"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/gpopt/gpdbwrappers.cpp:1633"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/gpopt/gpdbwrappers.cpp:1635"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/gpopt/gpdbwrappers.cpp:2622"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/libpq/auth-scram.c:429"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/libpq/auth-scram.c:847"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/libpq/be-fsstubs.c:560"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/libpq/pqcomm.c:13"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/libpq/pqcomm.c:23"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/libpq/pqcomm.c:197"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/libpq/pqcomm.c:934"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/libpq/pqcomm.c:1089"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/libpq/pqcomm.c:1339"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/libpq/pqcomm.c:1572"
    },
    {
        "log": "ereport(COMERROR, ...)",
        "severity_level": "COMERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/libpq/test/pqcomm_test.c:118"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/executor/functions.c:1167"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/executor/spi.c:1314"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/executor/spi.c:1762"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/executor/spi.c:1914"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/executor/spi.c:2033"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/executor/spi.c:2098"
    },
    {
        "log": "ereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INTERNAL_ERROR),\n\t\t\t\t\t\t\t\t ERRMSG_GP_INSUFFICIENT_STATEMENT_MEMORY))",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": "ERRCODE_INTERNAL_ERROR",
        "errcode_numeric": "XX000",
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/executor/execHHashagg.c:989"
    },
    {
        "log": "ereport(ERROR,\n\t\t\t\t\t\t(errcode(ERRCODE_INTERNAL_ERROR),\n\t\t\t\t\t\t\t\t ERRMSG_GP_INSUFFICIENT_STATEMENT_MEMORY))",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": "ERRCODE_INTERNAL_ERROR",
        "errcode_numeric": "XX000",
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/executor/execHHashagg.c:1933"
    },
    {
        "log": "ereport(ERROR, (errcode(ERRCODE_INTERNAL_ERROR),\n\t\t\t\t ERRMSG_GP_INSUFFICIENT_STATEMENT_MEMORY))",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": "ERRCODE_INTERNAL_ERROR",
        "errcode_numeric": "XX000",
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/executor/execHHashagg.c:2094"
    },
    {
        "log": "elog(LOG)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/executor/nodeHash.c:600"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/replication/repl_scanner.l:225"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/replication/walsender.c:545"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/replication/walreceiver.c:321"
    },
    {
        "log": "ereport(ERROR,...)",
        "severity_level": "ERROR",
        "errmsg_template": null,
        "errmsg_variables": [],
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": null,
        "file_path": "src/backend/commands/schemacmds.c:163"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/commands/copy.c:2718"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/commands/copy.c:2720"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/commands/copy.c:3850"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/commands/copy.c:3851"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/commands/matview.c:795"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/commands/prepare.c:501"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/commands/tablecmds.c:10418"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/commands/tablecmds.c:13249"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/commands/variable.c:285"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/port/win32_latch.c:314"
    },
    {
        "log": "elog(FATAL)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/port/win32_shmem.c:440"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/backend/port/unix_latch.c:646"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/port/win32/security.c:25"
    },
    {
        "log": "ereport (we're called too early)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/port/win32/security.c:108"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/tsearch/ts_locale.c:121"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/backend/tsearch/ts_locale.c:135"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/common/psprintf.c:45"
    },
    {
        "log": "elog(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/common/psprintf.c:93"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/common/unicode_norm.c:304"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/common/saslprep.c:1068"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/port/chklocale.c:315"
    },
    {
        "log": "ereport(ERROR)",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/port/path.c:602"
    },
    {
        "log": "ereport()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse ereport log",
        "file_path": "src/interfaces/ecpg/preproc/pgc.l:1496"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/interfaces/libpq/pqexpbuffer.h:12"
    },
    {
        "log": "elog()",
        "severity_level": null,
        "errmsg_template": null,
        "errmsg_variables": null,
        "errcode": null,
        "errcode_numeric": null,
        "errmsg_clean": null,
        "script_parse_error": "Failed to parse elog log",
        "file_path": "src/interfaces/libpq/pqexpbuffer.c:11"
    }
]